Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Document Type,Publication Stage,Open Access,Source,EID
"Garouani M., Ahmad A., Bouneffa M., Hamlich M., Bourguin G., Lewandowski A.","57226345482;57192646034;19337242800;55611586400;55914522900;23397053000;","Using meta-learning for automated algorithms selection and configuration: an experimental framework for industrial big data",2022,"Journal of Big Data","9","1","57","","",,1,"10.1186/s40537-022-00612-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129057794&doi=10.1186%2fs40537-022-00612-4&partnerID=40&md5=35693574bd6ffa8f22c5a71f3226dbe5","Advanced analytics are fundamental to transform large manufacturing data into resourceful knowledge for various purposes. In its very nature, such “industrial big data” can relay its usefulness to reach further utilitarian applications. In this context, Machine Learning (ML) is among the major predictive modeling approaches that can enable manufacturing researchers and practitioners to improve the product quality and achieve resource efficiency by exploiting large amounts of data (which is collected during manufacturing process). However, disposing ML algorithms is a challenging task for manufacturing industrial actors due to the prior specification of one or more algorithms hyperparameters (HPs) and their values. Moreover, manufacturing industrial actors often lack the technical expertise to apply advanced analytics. Consequently, it necessitates frequent consultations with data scientists; but such collaborations tends to cost the delays, which can generate the risks such as human-resource bottlenecks. As the complexity of these tasks increases, so does the demand for support solutions. In response, the field of automated ML (AutoML) is a data mining-based formalism that aims to reduce human effort and speedup the development cycle through automation. In this regard, existing approaches include evolutionary algorithms, Bayesian optimization, and reinforcement learning. These approaches mainly focus on providing the user assistance by automating the partial or entire data analysis process, but they provide very limited details concerning their impact on the analysis. The major goal of these conventional approaches has been generally focused on the performance factors, while the other important and even crucial aspects such as computational complexity are rather omitted. Therefore, in this paper, we present a novel meta-learning based approach to automate ML predictive models built over the industrial big data. The approach is leveraged with development of, AMLBID, an Automated ML tool for Big Industrial Data analyses. It attempts to support the manufacturing engineers and researchers who presumably have meager skills to carry out the advanced analytics. The empirical results show that AMLBID surpasses the state-of-the-art approaches and could retrieve the usefulness of large manufacturing data to prosper the research in manufacturing domain and improve the use of predictive models instead of precluding their outcomes. © 2022, The Author(s).",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85129057794
"Müller D., Müller M.G., Kress D., Pesch E.","57204680556;57207400805;44661350000;6601957889;","An algorithm selection approach for the flexible job shop scheduling problem: Choosing constraint programming solvers through machine learning",2022,"European Journal of Operational Research","302","3",,"874","891",,2,"10.1016/j.ejor.2022.01.034","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124380510&doi=10.1016%2fj.ejor.2022.01.034&partnerID=40&md5=5cfa40a52cc37b8c70cfa7a3cc823dfa","Constraint programming solvers are known to perform remarkably well for most scheduling problems. However, when comparing the performance of different available solvers, there is usually no clear winner over all relevant problem instances. This gives rise to the question of how to select a promising solver when knowing the concrete instance to be solved. In this article, we aim to provide first insights into this question for the flexible job shop scheduling problem. We investigate relative performance differences among five constraint programming solvers on problem instances taken from the literature as well as randomly generated problem instances. These solvers include commercial and non-commercial software and represent the state-of-the-art as identified in the relevant literature. We find that two solvers, the IBM ILOG CPLEX CP Optimizer and Google's OR-Tools, outperform alternative solvers. These two solvers show complementary strengths regarding their ability to determine provably optimal solutions within practically reasonable time limits and their ability to quickly determine high quality feasible solutions across different test instances. Hence, we leverage the resulting performance complementarity by proposing algorithm selection approaches that predict the best solver for a given problem instance based on instance features or parameters. The approaches are based on two machine learning techniques, decision trees and deep neural networks, in various variants. In a computational study, we analyze the performance of the resulting algorithm selection models and show that our approaches outperform the use of a single solver and should thus be considered as a relevant tool by decision makers in practice. © 2022 Elsevier B.V.",Article,"Final","",Scopus,2-s2.0-85124380510
"Zhou Y.","57191995034;","A regression learner-based approach for battery cycling ageing prediction―advances in energy management strategy and techno-economic analysis",2022,"Energy","256",,"124668","","",,2,"10.1016/j.energy.2022.124668","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133734943&doi=10.1016%2fj.energy.2022.124668&partnerID=40&md5=131ab01a3d61875f50b175ac46dd13d3","Renewable energy planning, electrochemical battery storages and advanced energy management strategies are flexible solutions for transformation towards smart grids, whereas the complex battery cycling ageing is nonlinearly dependent on intermittent renewable supply, stochastic load profiles and dynamic charging/discharging behaviors. In this study, a nonlinear mathematical model is developed to explore effective strategies for smart grids. A general regression learner-based battery cycling ageing prediction method is proposed for quantifications of lifetime battery cycling ageing and battery replacement times, including the database preparation, surrogate model training with typical feature extraction and classification, cross-validation, and performance prediction in various battery groups. A machine learning (ML) algorithm selection approach is proposed through the statistical analysis, to guide the accurate surrogate model development, considering the diversity in dynamic charging/discharging behaviours and intrinsic cycling ageing performances of each battery. Afterwards, a novel battery discharging control strategy is proposed, to address the contradiction between the economic cost-saving and the associated battery replacement cost. Last but not the least, the machine learning-based models are thereafter integrated in the district energy community for technical performance analysis. This study can provide a regression learner-based battery cycling ageing modelling method, a machine learning algorithm selection approach, and a holistic framework for systematic integration with avoidance on techno-economic performance overestimation, which is critical to guide renewable energy planning, electrochemical battery storages, and advanced energy management strategies. © 2022 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85133734943
"Xie Z., Yu Y., Zhang J., Chen M.","57218550216;57786595300;57221658393;55733270200;","The searching artificial intelligence: Consumers show less aversion to algorithm-recommended search product",2022,"Psychology and Marketing","39","10",,"1902","1919",,,"10.1002/mar.21706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133599045&doi=10.1002%2fmar.21706&partnerID=40&md5=fd91a707d7dd985b554fc238bcc82a69","Though artificial intelligence (AI) recommendation is a hot topic in recent marketing research, previous research has shown a convergent tendency for aversion to AI recommendation. It is imperative to find ways to promote AI usage and reduce consumers’ AI aversion. This study fills this void by exploring the effect of AI (vs. human) recommenders on consumers’ preferences for search versus experience products in the context of e-commerce. Two studies provide convergent evidence that consumers show less avoidance of algorithms when recommending search products compared to experience products. A behavioral experiment (Study 1, N = 112) validates that consumers are less likely to purchase experience products recommended by AI, while there are no significant differences between AI versus human recommenders when recommending search products. Using event-related potential (ERP), a further consumer neuroscience study (Study 2, N = 26) shows that consumers have a higher level of cognitive conflict (i.e., a larger magnitude of N2) when AI (vs. human) recommends experience products, while the effect disappears for search products. This paper shows that for search products, marketers can obtain similar evaluations using AI recommenders, which is relatively cheaper and more time-saving compared with human recommenders. Therefore, our work provides important implications for theory and practice on e-commerce and marketing communication. © 2022 Wiley Periodicals LLC.",Article,"Final","",Scopus,2-s2.0-85133599045
"Chen W., Zhang L.","57221304249;55271615200;","An automated machine learning approach for earthquake casualty rate and economic loss prediction",2022,"Reliability Engineering and System Safety","225",,"108645","","",,,"10.1016/j.ress.2022.108645","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131694928&doi=10.1016%2fj.ress.2022.108645&partnerID=40&md5=6a5a7b848f834dc3347a6f00fe972d8a","This study presents an automated machine learning (AutoML) framework to predict the casualty rate and direct economic loss induced by earthquakes. The AutoML framework enables automated combined algorithm selection and hyperparameter tuning (CASH), reducing the manual works in the model development. The proposed AutoML framework includes 5 modules: data collection, data preprocessing, CASH, loss prediction, and model analysis. The AutoML models are learned from the dataset that is composed of earthquake information and social indicators. The optimal algorithm and hyperparameter setting of models are determined by the CASH module. A two-step model including a classifier and a regression model is designed for the casualty rate to address zero-casualty cases and also minimize their impacts on data distribution. The proposed AutoML framework is implemented on the seismic loss dataset of mainland China to demonstrate its practicability. A comparison study is conducted to show the high predictive abilities of the AutoML model compared with the traditional seismic risk model and other AutoML models. Models learned from the complete dataset achieve the ultimate performance compared with subsets that are composed of partial features. The model interpretation results indicate that earthquake magnitude, position, and population density are leading indicators for loss prediction. © 2022",Article,"Final","",Scopus,2-s2.0-85131694928
"Stauder M., Kühl N.","57247254200;57196220660;","AI for in-line vehicle sequence controlling: development and evaluation of an adaptive machine learning artifact to predict sequence deviations in a mixed-model production line",2022,"Flexible Services and Manufacturing Journal","34","3",,"709","747",,,"10.1007/s10696-021-09430-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114367971&doi=10.1007%2fs10696-021-09430-x&partnerID=40&md5=4f244202425ce7c94f3785f173a612c7","Customers in the manufacturing sector, especially in the automotive industry, have a high demand for individualized products at price levels comparable to traditional mass-production. The contrary objectives of providing a variety of products and operating at minimum costs have introduced a high degree of production planning and control mechanisms based on a stable order sequence for mixed-model assembly lines. A major threat to this development is sequence scrambling, triggered by both operational and product-related root causes. Despite the introduction of Just-in-time and fixed production times, the problem of sequence scrambling remains partially unresolved in the automotive industry. Negative downstream effects range from disruptions in the Just-in-sequence supply chain, to a discontinuation of the production process. A precise prediction of sequence deviations at an early stage allows the introduction of counteractions to stabilize the sequence before disorder emerges. While procedural causes are widely addressed in research, the work at hand requires a different perspective involving a product-related view. Built on unique data from a real-world global automotive manufacturer, a supervised classification model is trained and evaluated. This includes all the necessary steps to design, implement, and assess an AI-artifact, as well as data gathering, preprocessing, algorithm selection, and evaluation. To ensure long-term prediction stability, we include a continuous learning module to counter data drifts. We show that up to 50% of the major deviations can be predicted in advance. However, we do not consider any process-related information, such as machine conditions and shift plans, but solely focus on the exploitation of product features like body type, power train, color, and special equipment. © 2021, The Author(s).",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85114367971
"Brown N., South K., Wiese E.S.","57869871700;57869871800;56335976300;","The Shortest Path to Ethics in AI: An Integrated Assignment Where Human Concerns Guide Technical Decisions",2022,"ICER 2022 - Proceedings of the 2022 ACM Conference on International Computing Education Research","1",,,"344","355",,,"10.1145/3501385.3543978","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137101813&doi=10.1145%2f3501385.3543978&partnerID=40&md5=a9a8496acfa84380cbe800148578c303","How can we teach AI students to use human concerns to guide their technical decisions? We created an AI assignment with a human context, asking students to find the safest path rather than the shortest path. This integrated assignment evaluated 120 students' understanding of the limitations and assumptions of standard graph search algorithms, and required students to consider human impacts to propose appropriate modifications. Since the assignment focused on algorithm selection and modification, it provided the instructor with a different perspective on student understanding (compared with questions on algorithm execution). Specifically, many students: tried to solve a bottleneck problem with algorithms designed for accumulation problems, did not distinguish between calculations that could be done during the incremental construction of a path versus ones that required knowledge of the full path, and, when proposing modifications to a standard algorithm, did not present the full technical details necessary to implement their ideas. We created rubrics to analyze students' responses. Our rubrics cover three dimensions: technical AI knowledge, consideration of human factors, and the integration of technical decisions as they align with the human context. These rubrics demonstrate how students' skills can vary along each dimension, and also provide a template for scoring integrated assignments for other CS topics. Overall, this work demonstrates how to integrate human concerns with technical content in a way that deepens technical rigor and supports instructor pedagogical content knowledge. © 2022 ACM.",Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85137101813
"Liu H., Xu J., Chen S., Guo T.","57191739222;37053056800;57863422200;57863689200;","Compiler Optimization Parameter Selection Method Based on Ensemble Learning",2022,"Electronics (Switzerland)","11","15","2452","","",,,"10.3390/electronics11152452","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136788162&doi=10.3390%2felectronics11152452&partnerID=40&md5=0ce5dd3a13cc030f5b9e58a51d96a02f","Iterative compilation based on machine learning can effectively predict a program’s compiler optimization parameters. Although having some limits, such as the low efficiency of optimization parameter search and prediction accuracy, machine learning-based solutions have been a frontier research field in the field of iterative compilation and have gained increasing attention. The research challenges are focused on learning algorithm selection, optimal parameter search, and program feature representation. For the existing problems, we propose an ensemble learning-based optimization parameter selection (ELOPS) method for the compiler. First, in order to further improve the optimization parameter search efficiency and accuracy, we proposed a multi-objective particle swarm optimization (PSO) algorithm to determine the optimal compiler parameters of the program. Second, we extracted the mixed features of the program through the feature-class relevance method, rather than using static or dynamic features alone. Finally, as the existing research usually uses a separate machine learning algorithm to build prediction models, an ensemble learning model using program features and optimization parameters was constructed to effectively predict compiler optimization parameters of the new program. Using standard performance evaluation corporation 2006 (SPEC2006) and NAS parallel benchmark (NPB) benchmarks as well as some typical scientific computing programs, we compared ELOPS with the existing methods. The experimental results showed that we can respectively achieve 1.29× and 1.26× speedup when using our method on two platforms, which are better results than those of existing methods. © 2022 by the authors.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85136788162
"Liu L., Liu J., Zhou Q., Huang D.","57225131025;56467579100;57222736460;57204391236;","Machine learning algorithm selection for windage alteration fault diagnosis of mine ventilation system",2022,"Advanced Engineering Informatics","53",,"101666","","",,,"10.1016/j.aei.2022.101666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132412251&doi=10.1016%2fj.aei.2022.101666&partnerID=40&md5=e19903c807719c38a02c9013ecb41035","Machine learning algorithms have been widely used in mine fault diagnosis. The correct selection of the suitable algorithms is the key factor that affects the fault diagnosis. However, the impact of machine learning algorithms on the prediction performance of mine fault diagnosis models has not been fully evaluated. In this study, the windage alteration faults (WAFs) diagnosis models, which are based on K-nearest neighbor algorithm (KNN), multi-layer perceptron (MLP), support vector machine (SVM), and decision tree (DT), are constructed. Furthermore, the applicability of these four algorithms in the WAFs diagnosis is explored by a T-type ventilation network simulation experiment and the field empirical application research of Jinchuan No. 2 mine. The accuracy of the fault location diagnosis for the four models in both networks was 100%. In the simulation experiment, the mean absolute percentage error (MAPE) between the predicted values and the real values of the fault volume of the four models was 0.59%, 97.26%, 123.61%, and 8.78%, respectively. The MAPE for the field empirical application was 3.94%, 52.40%, 25.25%, and 7.15%, respectively. The results of the comprehensive evaluation of the fault location and fault volume diagnosis tests showed that the KNN model is the most suitable algorithm for the WAFs diagnosis, whereas the prediction performance of the DT model was the second-best. This study realizes the intelligent diagnosis of WAFs, and provides technical support for the realization of intelligent ventilation. © 2022 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85132412251
"Adil S.M., Elahi C., Patel D.N., Seas A., Warman P.I., Fuller A.T., Haglund M.M., Dunn T.W.","56465590800;57203882417;57700299600;57041030000;57329546700;56521038300;7006603985;23988344800;","Deep Learning to Predict Traumatic Brain Injury Outcomes in the Low-Resource Setting",2022,"World Neurosurgery","164",,,"e8","e16",,1,"10.1016/j.wneu.2022.02.097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130326931&doi=10.1016%2fj.wneu.2022.02.097&partnerID=40&md5=d3b1de2d4e4ed95e929e4bde6ab8db5d","Objective: Traumatic brain injury (TBI) disproportionately affects low- and middle-income countries (LMICs). In these settings, accurate patient prognostication is both difficult and essential for high-quality patient care. With the ultimate goal of enhancing TBI triage in LMICs, we aim to develop the first deep learning model to predict outcomes after TBI and compare its performance with that of less complex algorithms. Methods: TBI patients’ data were prospectively collected in Kampala, Uganda, from 2016 to 2020. To predict good versus poor outcome at hospital discharge, we created deep neural network, shallow neural network, and elastic-net regularized logistic regression models. Predictors included 13 easily acquirable clinical variables. We assessed model performance with 5-fold cross-validation to calculate areas under both the receiver operating characteristic curve and precision-recall curve (AUPRC), in addition to standardized partial AUPRC to focus on comparisons at clinically relevant operating points. Results: We included 2164 patients for model training, of which 12% had poor outcomes. The deep neural network performed best as measured by the area under the receiver operating characteristic curve (0.941) and standardized partial AUPRC in region maximizing recall (0.291), whereas the shallow neural network was best by the area under the precision-recall curve (0.770). In several other comparisons, the elastic-net regularized logistic regression was noninferior to the neural networks. Conclusions: We present the first use of deep learning for TBI prognostication, with an emphasis on LMICs, where there is great need for decision support to allocate limited resources. Optimal algorithm selection depends on the specific clinical setting; deep learning is not a panacea, though it may have a role in these efforts. © 2022 Elsevier Inc.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85130326931
"Gan M., Hou H., Wu X., Liu B., Yang Y., Xie C.","57536445700;14630489000;12238907900;57537674100;57538905100;55751635700;","Machine learning algorithm selection for real-time energy management of hybrid energy ship",2022,"Energy Reports","8",,,"1096","1102",,,"10.1016/j.egyr.2022.02.200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126689217&doi=10.1016%2fj.egyr.2022.02.200&partnerID=40&md5=5538412bd95f904248b753ce3be1b745","The advantages of hybrid energy ship with photovoltaic in energy conservation and emission reduction are becoming more and more prominent with increasing tension of global fossil energy. However, how to deal with photovoltaic uncertainty in real time and make photovoltaic efficiently connected to ship microgrid has become a key technical problem. Therefore, we propose a real-time energy management strategy for hybrid energy ship based on approximate model predictive control. Firstly, aiming at minimizing the operating cost and deviation from the reference state of charge, an energy management framework based on model predictive control is established. Secondly, the machine learning algorithm is trained to approximate the optimal control action of model predictive control offline, and the performance of different machine learning algorithms is analyzed quantitatively. Finally, taking a ferry equipped with photovoltaic as an example, the appropriate machine learning algorithm and sample number are selected. The results show that the proposed strategy can not only ensure the optimization performance, but also effectively reduce the amount of calculation and realize the real-time operation of energy management in hybrid energy ship. © 2022 The Author(s)",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85126689217
"Liu Z., Han J., Meng F., Liao H.","57191280689;56333240200;57213962702;55580964700;","A cloud-based and web-based group decision support system in multilingual environment with hesitant fuzzy linguistic preference relations",2022,"International Journal of Intelligent Systems","37","8",,"5186","5216",,2,"10.1002/int.22789","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121996373&doi=10.1002%2fint.22789&partnerID=40&md5=e1cf7d20ed1d94ab6b969e53d230390a","Due to the growing needs in decision-making under uncertainty, existing studies introduced consistency and consensus-driven algorithms for group decision-making (GDM) problems with hesitant fuzzy linguistic preference relations (HFLPRs). A decision support system (DSS) that can host these GDM algorithms to provide decision-support services or tools for practical use is urgently needed. However, the state-of-the-art architectures cannot organize these algorithms and related data to run within one framework. This is mainly due to the running environments for these GDM algorithms are different since these algorithms were not originally designed to be compatible. Given the multilingual consistency and consensus-based decision support algorithms, how to design and implement a cloud-based DSS in a multilingual environment is still an open question. To fill this gap, this paper provides a web-based and cloud-based DSS with a novel architecture that utilizes the advantages of microservices. The proposed system implements a multilingual support framework to dynamically upload, manage and run multilingual consistency and consensus-based decision support algorithms. An algorithm recommendation module is developed to help users choose suitable decision support algorithms. Tokenization is applied to deal with regulatory issues of knowledge protection, data privacy, and security while storing, analyzing, and transforming data into different algorithms for effective decision-making. An expert feedback study verified that our web and cloud-based DSS is a right artifact to fulfill the objective claimed in this paper. © 2021 Wiley Periodicals LLC.",Article,"Final","",Scopus,2-s2.0-85121996373
"Bertsimas D., Borenstein A.R.A., Dauvin A., Orfanoudaki A.","7005282772;57221447450;57222565266;57216889389;","Ensemble machine learning for personalized antihypertensive treatment",2022,"Naval Research Logistics","69","5",,"669","688",,,"10.1002/nav.22040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120880479&doi=10.1002%2fnav.22040&partnerID=40&md5=423fe6379acaa09401ffb9a15bdd230d","Due to its prevalence and association with cardiovascular diseases and premature death, hypertension is a major public health challenge. Proper prevention and management measures are needed to effectively reduce the pervasiveness of the condition. Current clinical guidelines for hypertension provide physicians with general suggestions for first-line pharmacologic treatment, but do not consider patient-specific characteristics. In this study, longitudinal electronic health record data are utilized to develop personalized predictions and prescription recommendations for hypertensive patients. We demonstrate that both binary classification and regression algorithms can be used to accurately predict a patient's future hypertensive status. We then present a prescriptive framework to determine the optimal antihypertensive treatment for a patient using their individual characteristics and clinical condition. Given the observational nature of the data, we address potential confounding through generalized propensity score evaluation and optimal matching. For patients for whom the algorithm recommendation differs from the standard of care, we demonstrate an approximate 15.87% decrease in next blood pressure score based on the predicted outcome under the recommended treatment. An interactive dashboard has been developed to be used by physicians as a clinical support tool. © 2021 Wiley Periodicals LLC.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85120880479
"Tessari M., Iacca G.","57786740000;36682414000;","Reinforcement learning based adaptive metaheuristics",2022,"GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",,,,"1854","1861",,,"10.1145/3520304.3533983","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136331076&doi=10.1145%2f3520304.3533983&partnerID=40&md5=2762061e4dea8c2678f27ce215d1c362","Parameter adaptation, that is the capability to automatically adjust an algorithm's hyperparameters depending on the problem being faced, is one of the main trends in evolutionary computation applied to numerical optimization. While several handcrafted adaptation policies have been proposed over the years to address this problem, only few attempts have been done so far at apply machine learning to learn such policies. Here, we introduce a general-purpose framework for performing parameter adaptation in continuous-domain metaheuristics based on state-of-the-art reinforcement learning algorithms. We demonstrate the applicability of this framework on two algorithms, namely Covariance Matrix Adaptation Evolution Strategies (CMA-ES) and Differential Evolution (DE), for which we learn, respectively, adaptation policies for the step-size (for CMA-ES), and the scale factor and crossover rate (for DE). We train these policies on a set of 46 benchmark functions at different dimensionalities, with various inputs to the policies, in two settings: one policy per function, and one global policy for all functions. Compared, respectively, to the Cumulative Step-size Adaptation (CSA) policy and to two well-known adaptive DE variants (iDE and jDE), our policies are able to produce competitive results in the majority of cases, especially in the case of DE. © 2022 ACM.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85136331076
"Jurado I.C., Vanschoren J.","57221330963;23394289300;","Multi-fidelity optimization method with asynchronous generalized island model for AutoML",2022,"GECCO 2022 Companion - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",,,,"220","223",,1,"10.1145/3520304.3528917","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133708541&doi=10.1145%2f3520304.3528917&partnerID=40&md5=1d56b5819f02af6b8a3601180238f4e9","AutoML frameworks seek to find the best configurations of machine learning techniques. A research field that has gained a great deal of popularity in recent years is the combined algorithm selection and hyperparameter optimisation (CASH) problem. Several bio-inspired optimization techniques have been applied in AutoML, each with their drawbacks and benefits. For instance, methods may get stuck evaluating computationally expensive models, or certain solutions may dominate early on and inhibit the discovery of better ones. We propose to use multiple bio-inspired techniques in parallel in a generalized island model & combine this with multi-fidelity optimization to speed up the search. We analyze 3 different island topologies, including fully connected, unconnected, and ring topologies, to understand the trade-offs between information sharing and maintaining diversity. With respect to convergence time, the proposed method outperforms Asynchronous Evolutionary Algorithms and Asynchronous Successive Halving techniques. In an objective comparison based on the OpenML AutoML Benchmark, we also find that the proposed method is competitive with current state-of-the-art AutoML frameworks such as TPOT, AutoWEKA, AutoSklearn, H2O AutoML, GAMA, Asynchronous Successive Halving, and random search. © 2022 Owner/Author.",Conference Paper,"Final","",Scopus,2-s2.0-85133708541
"Seiler M.V., Prager R.P., Kerschke P., Trautmann H.","57219109157;57211237852;56336853600;23974957500;","A collection of deep learning-based feature-free approaches for characterizing single-objective continuous fitness landscapes",2022,"GECCO 2022 - Proceedings of the 2022 Genetic and Evolutionary Computation Conference",,,,"657","665",,,"10.1145/3512290.3528834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135223765&doi=10.1145%2f3512290.3528834&partnerID=40&md5=2dab9a87ee5895cf67f50f8f2ceb15f5","Exploratory Landscape Analysis is a powerful technique for numerically characterizing landscapes of single-objective continuous optimization problems. Landscape insights are crucial both for problem understanding as well as for assessing benchmark set diversity and composition. Despite the irrefutable usefulness of these features, they suffer from their own ailments and downsides. Hence, in this work we provide a collection of different approaches to characterize optimization landscapes. Similar to conventional landscape features, we require a small initial sample. However, instead of computing features based on that sample, we develop alternative representations of the original sample. These range from point clouds to 2D images and, therefore, are entirely feature-free. We demonstrate and validate our devised methods on the BBOB testbed and predict, with the help of Deep Learning, the high-level, expert-based landscape properties such as the degree of multimodality and the existence of funnel structures. The quality of our approaches is on par with methods relying on the traditional landscape features. Thereby, we provide an exciting new perspective on every research area which utilizes problem information such as problem understanding and algorithm design as well as automated algorithm configuration and selection. © 2022 ACM.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85135223765
"Lacroux A., Martin-Lacroux C.","56530639200;57195288666;","Should I Trust the Artificial Intelligence to Recruit? Recruiters’ Perceptions and Behavior When Faced With Algorithm-Based Recommendation Systems During Resume Screening",2022,"Frontiers in Psychology","13",,"895997","","",,,"10.3389/fpsyg.2022.895997","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134627665&doi=10.3389%2ffpsyg.2022.895997&partnerID=40&md5=cca1204179efad3ac83301102d0f682e","Resume screening assisted by decision support systems that incorporate artificial intelligence is currently undergoing a strong development in many organizations, raising technical, managerial, legal, and ethical issues. The purpose of the present paper is to better understand the reactions of recruiters when they are offered algorithm-based recommendations during resume screening. Two polarized attitudes have been identified in the literature on users’ reactions to algorithm-based recommendations: algorithm aversion, which reflects a general distrust and preference for human recommendations; and automation bias, which corresponds to an overconfidence in the decisions or recommendations made by algorithmic decision support systems (ADSS). Drawing on results obtained in the field of automated decision support areas, we make the general hypothesis that recruiters trust human experts more than ADSS, because they distrust algorithms for subjective decisions such as recruitment. An experiment on resume screening was conducted on a sample of professionals (N = 694) involved in the screening of job applications. They were asked to study a job offer, then evaluate two fictitious resumes in a 2 × 2 factorial design with manipulation of the type of recommendation (no recommendation/algorithmic recommendation/human expert recommendation) and of the consistency of the recommendations (consistent vs. inconsistent recommendation). Our results support the general hypothesis of preference for human recommendations: recruiters exhibit a higher level of trust toward human expert recommendations compared with algorithmic recommendations. However, we also found that recommendation’s consistence has a differential and unexpected impact on decisions: in the presence of an inconsistent algorithmic recommendation, recruiters favored the unsuitable over the suitable resume. Our results also show that specific personality traits (extraversion, neuroticism, and self-confidence) are associated with a differential use of algorithmic recommendations. Implications for research and HR policies are finally discussed. Copyright © 2022 Lacroux and Martin-Lacroux.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85134627665
"Balcan M.-F., Prasad S., Sandholm T., Vitercik E.","8954993900;57214917671;57203083791;56829527400;","Improved Sample Complexity Bounds for Branch-And-Cut",2022,"Leibniz International Proceedings in Informatics, LIPIcs","235",,"3","","",,,"10.4230/LIPIcs.CP.2022.3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135698899&doi=10.4230%2fLIPIcs.CP.2022.3&partnerID=40&md5=3ae223a099886efa9d7d35866cb69a28","The branch-and-cut algorithm for integer programming has a wide variety of tunable parameters that have a huge impact on its performance, but which are challenging to tune by hand. An increasingly popular approach is to use machine learning to configure these parameters based on a training set of integer programs from the application domain. We bound how large the training set should be to ensure that for any configuration, its average performance over the training set is close to its expected future performance. Our guarantees apply to parameters that control the most important aspects of branch-and-cut: node selection, branching constraint selection, and cut selection, and are sharper and more general than those from prior research. © Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik.",Conference Paper,"Final","",Scopus,2-s2.0-85135698899
"Bonidia R.P., Santos A.P.A., De Almeida B.L.S., Stadler P.F., Da Rocha U.N., Sanches D.S., De Carvalho A.C.P.L.F.","57200702454;56786509800;57385839400;7102702401;24781191100;55481490300;57217223446;","BioAutoML: automated feature engineering and metalearning to predict noncoding RNAs in bacteria",2022,"Briefings in Bioinformatics","23","4","bbac218","","",,,"10.1093/bib/bbac218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134635088&doi=10.1093%2fbib%2fbbac218&partnerID=40&md5=8c977e9163c99fcc6fe7eb5a085cf023","Recent technological advances have led to an exponential expansion of biological sequence data and extraction of meaningful information through Machine Learning (ML) algorithms. This knowledge has improved the understanding of mechanisms related to several fatal diseases, e.g. Cancer and coronavirus disease 2019, helping to develop innovative solutions, such as CRISPR-based gene editing, coronavirus vaccine and precision medicine. These advances benefit our society and economy, directly impacting people's lives in various areas, such as health care, drug discovery, forensic analysis and food processing. Nevertheless, ML-based approaches to biological data require representative, quantitative and informative features. Many ML algorithms can handle only numerical data, and therefore sequences need to be translated into a numerical feature vector. This process, known as feature extraction, is a fundamental step for developing high-quality ML-based models in bioinformatics, by allowing the feature engineering stage, with design and selection of suitable features. Feature engineering, ML algorithm selection and hyperparameter tuning are often manual and time-consuming processes, requiring extensive domain knowledge. To deal with this problem, we present a new package: BioAutoML. BioAutoML automatically runs an end-to-end ML pipeline, extracting numerical and informative features from biological sequence databases, using the MathFeature package, and automating the feature selection, ML algorithm(s) recommendation and tuning of the selected algorithm(s) hyperparameters, using Automated ML (AutoML). BioAutoML has two components, divided into four modules: (1) automated feature engineering (feature extraction and selection modules) and (2) Metalearning (algorithm recommendation and hyper-parameter tuning modules). We experimentally evaluate BioAutoML in two different scenarios: (i) prediction of the three main classes of noncoding RNAs (ncRNAs) and (ii) prediction of the eight categories of ncRNAs in bacteria, including housekeeping and regulatory types. To assess BioAutoML predictive performance, it is experimentally compared with two other AutoML tools (RECIPE and TPOT). According to the experimental results, BioAutoML can accelerate new studies, reducing the cost of feature engineering processing and either keeping or improving predictive performance. © 2022 The Author(s). Published by Oxford University Press.",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85134635088
"Bai Y., Li Y., Shen Y., Yang M., Zhang W., Cui B.","57794768600;57221715937;57793092300;57199913288;57793367300;57211734542;","AutoDC: An automatic machine learning framework for disease classification",2022,"Bioinformatics","38","13",,"3415","3421",,,"10.1093/bioinformatics/btac334","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133679072&doi=10.1093%2fbioinformatics%2fbtac334&partnerID=40&md5=9138d9ef75b91e475b21aee61a150f45","Motivation: The emergence of next-generation sequencing techniques opens up tremendous opportunities for researchers to uncover the basic mechanisms of disease at the molecular level. Recently, automatic machine learning (AutoML) frameworks have been employed for genomic and epigenomic data analysis. However, to analyze those high-dimensional data, existing AutoML frameworks suffer from the following issues: (i) they could not effectively filter out the redundant features from the original data, and (ii) they usually obey the rule of feature engineering first and algorithm hyper-parameter tuning later to build the machine learning pipeline, which could lead to sub-optimal outcomes. Thus, it is an urgent need to design a new AutoML framework for high-dimensional omics data analysis. Results: We introduce a new method: AutoDC, a tailored AutoML framework, for different disease classification based on gene expression data. AutoDC designs two novel optimization strategies to improve the performance. One is that AutoDC designs a novel two-stage feature selection method to select the features with high gene contribution scores. The other is that AutoDC proposes a novel optimization method, based on a two-layer Multi-Armed Bandit framework, to jointly optimize the feature engineering, algorithm selection and algorithm hyper-parameter tuning. We apply our framework to two public gene expression datasets. Compared with three state-of-The-Art AutoML frameworks, AutoDC could effectively classify diseases with higher predictive accuracy. © 2022 The Author(s) 2022. Published by Oxford University Press. All rights reserved.",Article,"Final","",Scopus,2-s2.0-85133679072
"Sun Y., Que H., Cai Q., Zhao J., Li J., Kong Z., Wang S.","57781580700;36022723200;57781314700;57338204200;57781843000;23008629200;57406964000;","Borderline SMOTE Algorithm and Feature Selection‐Based Network Anomalies Detection Strategy",2022,"Energies","15","13","4751","","",,,"10.3390/en15134751","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133448044&doi=10.3390%2fen15134751&partnerID=40&md5=0b8f56a9757fb2ded6fa1944417b6ff8","This paper proposes a novel network anomaly detection framework based on data balance and feature selection. Different from the previous binary classification of network intrusion, the network anomaly detection strategy proposed in this paper solves the problem of multiple classification of network intrusion. Regarding the common data imbalance of a network intrusion detection set, a resampling strategy generated by random sampling and Borderline SMOTE data is developed for data balance. According to the features of the intrusion detection dataset, feature selection is carried out based on information gain rate. Experiments are carried out on three basic machine learning algorithms (K‐nearest neighbor algorithm (KNN), decision tree (DT), random forest (RF)), and the optimal feature selection scheme is obtained. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85133448044
"Kuang G., Li B., Mo S., Hu X., Li L.","57427062600;25631051300;57796352200;57226546145;57681458800;","Review on Machine Learning-based Defect Detection of Shield Tunnel Lining",2022,"Periodica Polytechnica Civil Engineering","66","3",,"943","957",,,"10.3311/PPci.19859","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133954845&doi=10.3311%2fPPci.19859&partnerID=40&md5=1e36629142059fa3ddf3a00f029c0574","At present, machine learning methods are widely used in various industries for their high adaptability, optimization function, and self-learning reserve function. Besides, the world-famous cities have almost built and formed subway networks that promote economic development. This paper presents the art states of Defect detection of Shield Tunnel lining based on Machine learning (DSTM). In addition, the processing method of image data from the shield tunnel is being explored to adapt to its complex environment. Comparison and analysis are used to show the performance of the algorithms in terms of the effects of data set establishment, algorithm selection, and detection devices. Based on the analysis results, Convolutional Neural Network methods show high recognition accuracy and better adaptability to the complexity of the environment in the shield tunnel compared to traditional machine learning methods. The Support Vector Machine algorithms show high recognition performance only for small data sets. To improve detection models and increase detection accuracy, measures such as optimizing features, fusing algorithms, creating a high-quality data set, increasing the sample size, and using devices with high detection accuracy can be recommended. Finally, we analyze the challenges in the field of coupling DSTM, meanwhile, the possible development direction of DSTM is prospected. © 2022, Budapest University of Technology and Economics. All rights reserved.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85133954845
"Wen L., Wang Y., Li X.","57201552705;57608594600;56021323400;","A new automatic convolutional neural network based on deep reinforcement learning for fault diagnosis",2022,"Frontiers of Mechanical Engineering","17","2","17","","",,1,"10.1007/s11465-022-0673-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133240152&doi=10.1007%2fs11465-022-0673-7&partnerID=40&md5=5b380792443011b645028bcbd50ad38a","Convolutional neural network (CNN) has achieved remarkable applications in fault diagnosis. However, the tuning aiming at obtaining the well-trained CNN model is mainly manual search. Tuning requires considerable experiences on the knowledge on CNN training and fault diagnosis, and is always time consuming and labor intensive, making the automatic hyper parameter optimization (HPO) of CNN models essential. To solve this problem, this paper proposes a novel automatic CNN (ACNN) for fault diagnosis, which can automatically tune its three key hyper parameters, namely, learning rate, batch size, and L2-regulation. First, a new deep reinforcement learning (DRL) is developed, and it constructs an agent aiming at controlling these three hyper parameters along with the training of CNN models online. Second, a new structure of DRL is designed by combining deep deterministic policy gradient and long short-term memory, which takes the training loss of CNN models as its input and can output the adjustment on these three hyper parameters. Third, a new training method for ACNN is designed to enhance its stability. Two famous bearing datasets are selected to evaluate the performance of ACNN. It is compared with four commonly used HPO methods, namely, random search, Bayesian optimization, tree Parzen estimator, and sequential model-based algorithm configuration. ACNN is also compared with other published machine learning (ML) and deep learning (DL) methods. The results show that ACNN outperforms these HPO and ML/DL methods, validating its potential in fault diagnosis. [Figure not available: see fulltext.]. © 2022, Higher Education Press.",Article,"Final","",Scopus,2-s2.0-85133240152
"Lee H.J., Kim Y.W., Kim J.H., Lee Y.-J., Moon J., Jeong P., Jeong J., Kim J.-S., Lee J.S.","57660571000;57816812200;57244867100;57659319600;57659625900;57659626000;57659943800;47661436500;8047245500;","Optimization of FFR prediction algorithm for gray zone by hemodynamic features with synthetic model and biometric data",2022,"Computer Methods and Programs in Biomedicine","220",,"106827","","",,2,"10.1016/j.cmpb.2022.106827","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129302843&doi=10.1016%2fj.cmpb.2022.106827&partnerID=40&md5=9fa01f147366290f57db4d0dea805438","Background: Recent attempts on adopting artificial intelligence algorithm on coronary diagnosis had limitations on data quantity and quality. While most of previous studies only used vessel image as input data, flow features and biometric features should be also considered. Moreover, the accuracy should be optimized within gray zone as the purpose is to decide stent insertion with estimated fractional flow reserve. Objectives: The main purpose of this study is to develop an artificial intelligence-based coronary vascular diagnosis system focused on performance in the gray zone, from CT image extraction to FFR estimation. Three main issues should be considered for an algorithm to be used for pre-screening: algorithm optimization in the gray zone, minimization of labor during image processing, and consideration of flow and biometric features. This paper introduces a full FFR pre-screening system from automatic image extraction to an algorithm for estimating the FFR value. Method: The main techniques used in this study are an automatic image extraction algorithm, lattice Boltzmann method based computational fluid dynamics analysis of a synthetic model and patient data, and an AI algorithm optimization. For feature extraction, this study focused on an automatic process to reduce manual labor. The algorithm consisted of two steps: the first algorithm calculates flow features from geometrical features, and the second algorithm estimates the FFR value from flow features and patient biometric features. Algorithm selection, outlier elimination, and k-fold selection were included to optimize the algorithm. Conclusion: Eight types of algorithms including two neural network models and six machine learning models were optimized and tested. The random forest model shows the highest performance before optimization, whereas the multilayer perceptron regressor shows the highest gray zone accuracy after optimization. © 2022",Article,"Final","",Scopus,2-s2.0-85129302843
"Zhang X., Li Y., Li Z.","22936443200;57191907378;55879860200;","Comparative Research of Hyper-Parameters Mathematical Optimization Algorithms for Automatic Machine Learning in New Generation Mobile Network",2022,"Mobile Networks and Applications","27","3",,"928","935",,1,"10.1007/s11036-022-01913-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124544509&doi=10.1007%2fs11036-022-01913-x&partnerID=40&md5=4339b78af25d00fa3894de7e7bff3137","Under the configuration of the new generation communication network, the algorithm based on machine learning has been widely used in network optimization and mobile user behavior prediction. Therefore, the optimization method with hyper-parameters will have a huge development space in the field of mobile communication network. However, for non-professionals, the bottleneck that restricts the further development and application of the whole machine learning is the selection of suitable machine learning algorithm and the determination of suitable algorithm hyper-parameters. Researchers have proposed to use automatic machine learning algorithm to solve this remarkable problem. This article forms a technical manual that can be easily searched by researchers with summarizing related hyper-parameter optimization methods and proposing the corresponding algorithm framework. Moreover, through the comparison of related optimization methods, we highlight the characteristics and deficiencies of related algorithms in the new generation of mobile networks, and put forward suggestions for future improvement. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85124544509
"Huang C., Bai H., Yao X.","55377952500;57338434500;57222996723;","Online algorithm configuration for differential evolution algorithm",2022,"Applied Intelligence","52","8",,"9193","9211",,1,"10.1007/s10489-021-02752-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122162318&doi=10.1007%2fs10489-021-02752-1&partnerID=40&md5=b56759f9d57c0497144c4c71e05ba4ae","The performance of evolutionary algorithms (EAs) is strongly affected by their configurations. Thus, algorithm configuration (AC) problem, that is, to properly set algorithm’s configuration, including the operators and parameter values for maximizing the algorithm’s performance on given problem(s) is an essential and challenging task in the design and application of EAs. In this paper, an online algorithm configuration (OAC) approach is proposed for differential evolution (DE) algorithm to adapt its configuration in a data-driven way. In our proposed OAC, the multi-armed bandit algorithm is adopted to select trial vector generation strategies for DE, and the kernel density estimation method is used to adapt the associated control parameters during the evolutionary search process. The performance of DE algorithm using the proposed OAC (OAC-DE) is evaluated on a benchmark set of 30 bound-constrained numerical optimization problems and compared with several adaptive DE variants. Besides, the influence of OAC’s hyper-parameter on its performance is analyzed. The comparison results show OAC-DE achieves better average performance than the compared algorithms, which validates the effectiveness of the proposed OAC. The sensitivity analysis indicates that the hyper-parameter of OAC has little impact on OAC-DE’s performance. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85122162318
"Kolbeinsson A., Shukla N., Gupta A., Marla L., Yellepeddi K.","57210639548;57210637197;57219526347;55445193400;57210636024;","Galactic Air Improves Ancillary Revenues with Dynamic Personalized Pricing",2022,"Interfaces","52","3",,"233","249",,,"10.1287/inte.2021.1105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134247307&doi=10.1287%2finte.2021.1105&partnerID=40&md5=ccdb6e3e03f6283cc7821fca9736142d","Ancillaries are a rapidly growing source of revenue for airlines, yet their prices are currently statically determined using rules of thumb and are matched only to the average customer or to customer groups. Offering ancillaries at dynamic and personalized prices based on flight characteristics and customer needs could greatly improve airline revenue and customer satisfaction. Through a start-up (Deepair) that builds and deploys novel machine learning techniques to introduce such dynamically priced ancillaries to airlines, we partnered with a major European airline, Galactic Air (pseudonym), to build models and algorithms for improved pricing. These algorithms recommend dynamic personalized ancillary prices for a stream of features (called context) relating to each shopping session. Our recommended prices are restricted to be lower than the human-curated prices for each customer group. We designed and compared multiple machine learning models and deployed the best-performing ones live on the airline’s booking system in an online A/B testing framework. Over a six-month live implementation period, our dynamic pricing system increased the ancillary revenue per offer by 25% and conversion rate by 15% compared with the industry standard of human-curated rule-based prices. Copyright: © 2022 INFORMS",Article,"Final","",Scopus,2-s2.0-85134247307
"Liu G., Li X., Han Z.","57110966400;57224315613;57224312966;","Selection of building energy consumption prediction machine learning algorithms and parameter setting based on quality of samples [基于样本集质量的建筑能耗预测机器学习算法选择及参数设置]",2022,"Chongqing Daxue Xuebao/Journal of Chongqing University","45","5",,"79","95",,,"10.11835/j.issn.1000-582X.2020.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131748622&doi=10.11835%2fj.issn.1000-582X.2020.058&partnerID=40&md5=6722c947840c19284b0969d1156c6028","Machine learning algorithms are playing a more important role in building energy consumption prediction during the conceptual design. The selection of the machine learning algorithms and parameter setting have become a focus in the field of building performance design. However, the algorithms and their parameters are usually determined by the principle of algorithms rather than the features of the training samples which also have an effect on the performance of algorithms. Therefore, a classification method based on the quality of training samples which is evaluated by sample size and sample distribution characteristics is proposed. The performance of different machine learning algorithms for different quality sample sets is tested, and algorithm selection and parameter setting strategies for different quality sample sets are formulated. The relationship between sample quality and algorithm performance is investigated to provide effective guidance for architects. © 2022, Editorial Board of Journal of Chongqing University, Chongqing University Journals Department. All right reserved.",Article,"Final","",Scopus,2-s2.0-85131748622
"Kuk E., Stopa J., Kuk M., Janiga D., Wojnarowski P.","57695962700;6701583271;57218769050;56416451900;12244215900;","Optimal Well Control Based on Auto-Adaptive Decision Tree—Maximizing Energy Efficiency in High-Nitrogen Underground Gas Storage",2022,"Energies","15","9","3413","","",,,"10.3390/en15093413","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130168457&doi=10.3390%2fen15093413&partnerID=40&md5=554b14eed81512858d6ac52c1ab8aa08","To move the world toward a more sustainable energy future, it is crucial to use the limited hydrocarbon geological resources efficiently and to develop technologies that facilitate this. More rational management of petroleum reservoirs and underground gas storage can be obtained by optimizing well control. This paper presents a novel approach to optimal well control based on the combination of optimal control theory, innovative artificial intelligence methods, and numerical reservoir simulations. In the developed algorithm, well control is based on an auto-adaptive parameterized decision tree. Its parameters are optimized by state-of-the-art machine learning, which uses previous results to determine favorable parameters. During optimization, a numerical reservoir simulator is applied to compute the objective function. The developed solution enables full automation of the wells for optimal control. An exemplary application of the developed solution to optimize underground storage of gas with high nitrogen content confirmed its effectiveness. The total nitrogen content in the gas decreased by 2.4%, increasing energy efficiency without increasing expense, as only well control was modified. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85130168457
"Camacho-Urriolagoitia F.J., Villuendas-Rey Y., López-Yáñez I., Camacho-Nieto O., Yáñez-Márquez C.","57672436700;15123278300;56013671100;57200993848;57197542054;","Correlation Assessment of the Performance of Associative Classifiers on Credit Datasets Based on Data Complexity Measures",2022,"Mathematics","10","9","1460","","",,,"10.3390/math10091460","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129770812&doi=10.3390%2fmath10091460&partnerID=40&md5=ff8a5e0f111cae64d65bb8a51ed07eea","One of the four basic machine learning tasks is pattern classification. The selection of the proper learning algorithm for a given problem is a challenging task, formally known as the algorithm selection problem (ASP). In particular, we are interested in the behavior of the associative classifiers derived from Alpha-Beta models applied to the financial field. In this paper, the behavior of four associative classifiers was studied: the One-Hot version of the Hybrid Associative Classifier with Translation (CHAT-OHM), the Extended Gamma (EG), the Naïve Associative Classifier (NAC), and the Assisted Classification for Imbalanced Datasets (ACID). To establish the performance, we used the area under the curve (AUC), F-score, and geometric mean measures. The four classifiers were applied over 11 datasets from the financial area. Then, the performance of each one was analyzed, considering their correlation with the measures of data complexity, corresponding to six categories based on specific aspects of the datasets: feature, linearity, neighborhood, network, dimensionality, and class imbalance. The correlations that arise between the measures of complexity of the datasets and the measures of performance of the associative classifiers are established; these results are expressed with Spearman’s Rho coefficient. The experimental results correctly indicated correlations between data complexity measures and the performance of the associative classifiers. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85129770812
"Strassl S., Musliu N.","57407412100;7801571378;","Instance space analysis and algorithm selection for the job shop scheduling problem",2022,"Computers and Operations Research","141",,"105661","","",,1,"10.1016/j.cor.2021.105661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122633971&doi=10.1016%2fj.cor.2021.105661&partnerID=40&md5=9bd4fb38fdc22a85ede6c414e69872ac","This paper is concerned with the job shop scheduling problem, a well-known, NP-hard problem that has been extensively studied in the literature, but for which, despite its age and popularity, there has been relatively little work done towards understanding the landscape of instances. We provide a systematic analysis of the instance space for this problem. For this purpose, the benchmark instances commonly used in the literature were analyzed and extended by a set of newly generated instances of various sizes with processing times drawn from different probability distributions. A number of different state-of-the-art algorithms were evaluated on the extended instance set to analyze their performance patterns and highlight the differences to the current set of benchmark instances. It was found that the existing instances cover a significantly smaller area than the generated ones and did in fact result in different conclusions regarding the algorithms’ performances. Furthermore, different algorithms have been shown to excel on distinct subsets of the extended instance set. This has been utilized to train machine learning models to predict the best algorithm for a given instance, the best of which was able to obtain the best solution for 90% of the instances, whereas the best individual algorithm only obtained the best solution for 64%. © 2021",Article,"Final","",Scopus,2-s2.0-85122633971
"Netto R., Fabre S., Fontana T.A., Livramento V., Pilla L.L., Behjat L., Guntzel J.L.","55316142200;57205200687;57194761384;44661435700;57205336200;6507825639;7801476765;","Algorithm Selection Framework for Legalization Using Deep Convolutional Neural Networks and Transfer Learning",2022,"IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems","41","5",,"1481","1494",,,"10.1109/TCAD.2021.3079126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105865843&doi=10.1109%2fTCAD.2021.3079126&partnerID=40&md5=fa5e58ccf8d31d632e046f78e4b075ac","Machine learning (ML) models have been used to improve the quality of different physical design steps, such as timing analysis, clock tree synthesis, and routing. However, so far very few works have addressed the problem of algorithm selection during physical design, which can drastically reduce the computational effort of some steps. This work proposes a legalization algorithm selection framework using deep convolutional neural networks (CNNs). To extract features, we used snapshots of circuit placements and used transfer learning to train the models using pretrained weights of the Squeezenet architecture. By doing so, we can greatly reduce the training time and required data even though the pretrained weights come from a different problem. We performed extensive experimental analysis of ML models, providing details on how we chose the parameters of our model, such as CNN architecture, learning rate, and number of epochs. We evaluated the proposed framework by training a model to select between different legalization algorithms according to cell displacement and wirelength variation. The trained models achieved an average F -score of 0.98 when predicting cell displacement and 0.83 when predicting wirelength variation. When integrated into the physical design flow, the cell displacement model achieved the best results on 15 out of 16 designs, while the wirelength variation model achieved that for 10 out of 16 designs, being better than any individual legalization algorithm. Finally, using the proposed ML model for algorithm selection resulted in a speedup of up to 10times compared to running all the algorithms separately. © 1982-2012 IEEE.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85105865843
"Li H., Liang Q., Chen M., Dai Z., Li H., Zhu M.","57060870800;57216974510;7406348123;57020636800;57218313197;17347557200;","Pruning SMAC search space based on key hyperparameters",2022,"Concurrency and Computation: Practice and Experience","34","9","e5805","","",,,"10.1002/cpe.5805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085597975&doi=10.1002%2fcpe.5805&partnerID=40&md5=67bdfcb7a8e944df71d1a0e994291285","Machine learning (ML) has been widely applied in many areas in recent decades. However, because of its inherent complexity and characteristics, the efficiency and effectiveness of ML algorithm often to be heavily relies on the technical experts' experience and expertise which play a crucial role to optimize hyperparameters of algorithms. Generally, the procedure tuning the exposed hyperparameters of ML algorithm to achieve better performance is called Hyperparameters Optimization. Traditional hyperparameters optimization methods are manually exhaustive search, which is unavailable for high dimensional search spaces and large datasets. Recent automated sequential model-based optimization led to substantial improvements for this problem, whose core idea is fitting a regression model to describe the importance and dependence of algorithm's performance on certain given hyperparameter setting. Sequential model-based algorithm configuration (SMAC) is a the-state-of-art approach, which is specified by four components, Initialize, FitModel, SelectConfigurations, and Intensify. In this article, we propose to add a pruning procedure into SMAC approach, it quantifies the importance of hyperparameters by analyzing the performance of a list of promising configurations and reduces search space by discarding noncritical and bad key hyperparameters. To investigate the impact of pruning for model's performance, we conducted experiments on the configuration space constructed by Auto-Sklearn and compared the effect of run time and pruning ratio with our algorithm. The experiments results verified that, our method made the configuration selected by SMAC more stable and achieved better performance. © 2020 John Wiley & Sons, Ltd.",Conference Paper,"Final","",Scopus,2-s2.0-85085597975
"Oliver J.R., Karadaghy O.A., Fassas S.N., Arambula Z., Bur A.M.","57203606017;57193901551;57151380700;57449042100;57218000877;","Machine learning directed sentinel lymph node biopsy in cutaneous head and neck melanoma",2022,"Head and Neck","44","4",,"975","988",,,"10.1002/hed.26993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124477241&doi=10.1002%2fhed.26993&partnerID=40&md5=4ca477242b701be6a338f9b64fd4229c","Background: The specificity of sentinel lymph node biopsy (SLNB) for detecting lymph node metastasis in head and neck melanoma (HNM) is low under current National Comprehensive Cancer Network (NCCN) treatment guidelines. Methods: Multiple machine learning (ML) algorithms were developed to identify HNM patients at very low risk of occult nodal metastasis using National Cancer Database (NCDB) data from 8466 clinically node negative HNM patients who underwent SLNB. SLNB performance under NCCN guidelines and ML algorithm recommendations was compared on independent test data from the NCDB (n = 2117) and an academic medical center (n = 96). Results: The top-performing ML algorithm (AUC = 0.734) recommendations obtained significantly higher specificity compared to the NCCN guidelines in both internal (25.8% vs. 11.3%, p < 0.001) and external test populations (30.1% vs. 7.1%, p < 0.001), while achieving sensitivity >97%. Conclusion: Machine learning can identify clinically node negative HNM patients at very low risk of nodal metastasis, who may not benefit from SLNB. © 2022 Wiley Periodicals LLC",Article,"Final","",Scopus,2-s2.0-85124477241
"Mu T., Wang H., Wang C., Liang Z., Shao X.","57217029103;14061534000;57209508178;57219762184;57435115900;","Auto-CASH: A meta-learning embedding approach for autonomous classification algorithm selection",2022,"Information Sciences","591",,,"344","364",,2,"10.1016/j.ins.2022.01.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123843336&doi=10.1016%2fj.ins.2022.01.040&partnerID=40&md5=520855686d9825d2550a992cb99abaa3","With years of development, machine learning algorithms have excellent performance in some tasks of data analysis and data mining. To apply machine learning to new tasks, suitable algorithm and hyperparameters selection techniques, which is known as Combined Algorithm Selection and Hyperparameter optimization problem, are in demand. In the field of data analysis, how to automate the algorithm selection process has become a hot research topic in recent years. Most of the existing approaches are developed under the background of Automated Machine Learning with high time or space complexity. To alleviate the issue, an approach extracts and learns from prior experience based on meta-learning theory named Auto-CASH is proposed in this paper. One of the major drawbacks of existing meta-learning methods is that they rely too much on human expertise to extract and filter knowledge that guides subsequent training. Auto-CASH can automatically select features of tasks by introducing a reinforcement learning strategy. Thus Auto-CASH becomes less dependent on human expertise. Besides, two pruning strategies when processing Hyperparameter Optimization to improve efficiency are firstly proposed. Extensive experiments on classification tasks are conducted and results demonstrate that Auto-CASH outperforms state-of-the-art CASH approaches and popular AutoML systems with less time cost. © 2022 Elsevier Inc.",Article,"Final","",Scopus,2-s2.0-85123843336
"Ouadah A., Zemmouchi-Ghomari L., Salhi N.","57216953256;36999099300;57409905000;","Selecting an appropriate supervised machine learning algorithm for predictive maintenance",2022,"International Journal of Advanced Manufacturing Technology","119","7-8",,"4277","4301",,2,"10.1007/s00170-021-08551-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122749987&doi=10.1007%2fs00170-021-08551-9&partnerID=40&md5=dcdad8be17cc7f78a974b66a6d2e69cb","Predictive maintenance refers to predicting malfunctions using data from monitoring equipment and process performance measurements. Machine learning algorithms and techniques are often used to analyze equipment monitoring data. Machine learning is the process in which a computer can work more precisely by collecting and analyzing data. It is often the case that machine learning algorithms use supervised learning, in which labelled data is used to feed the algorithm. However, there are many supervised machine learning algorithms available. Therefore, choosing the best-supervised machine learning algorithm to resolve predictive maintenance issues is not trivial. This paper aims to increase the performance of predictive maintenance and achieve its goals by selecting the most suitable supervised machine learning algorithm. Based on the most commonly used criteria in research articles, we selected three supervised machine learning algorithms from a comparative study: Random forest, Decision tree and KNN. We then tested selected algorithms on data from real-world and simulation scenarios. Finally, we conducted the experiment based on vibration analysis and reliability evaluation. We noticed that Random forests and Decision trees obtained slightly the same performance. KNN is a better classification algorithm for extensive volumes of data; on the contrary, Random forest performs better in the case of small datasets. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85122749987
"Crase S., Thennadil S.N.","57217143965;9036564300;","An analysis framework for clustering algorithm selection with applications to spectroscopy",2022,"PLoS ONE","17","3 March","e0266369","","",,,"10.1371/journal.pone.0266369","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127398745&doi=10.1371%2fjournal.pone.0266369&partnerID=40&md5=c950c05fa94a8e8c91aa59dc11b4e372","Cluster analysis is a valuable unsupervised machine learning technique that is applied in a multitude of domains to identify similarities or clusters in unlabelled data. However, its performance is dependent of the characteristics of the data it is being applied to. There is no universally best clustering algorithm, and hence, there are numerous clustering algorithms available with different performance characteristics. This raises the problem of how to select an appropriate clustering algorithm for the given analytical purposes. We present and validate an analysis framework to address this problem. Unlike most current literature which focuses on characterizing the clustering algorithm itself, we present a wider holistic approach, with a focus on the user's needs, the data's characteristics and the characteristics of the clusters it may contain. In our analysis framework, we utilize a softer qualitative approach to identify appropriate characteristics for consideration when matching clustering algorithms to the intended application. These are used to generate a small subset of suitable clustering algorithms whose performance are then evaluated utilizing quantitative cluster validity indices. To validate our analysis framework for selecting clustering algorithms, we applied it to four different types of datasets: three datasets of homemade explosives spectroscopy, eight datasets of publicly available spectroscopy data covering food and biomedical applications, a gene expression cancer dataset, and three classic machine learning datasets. Each data type has discernible differences in the composition of the data and the context within which they are used. Our analysis framework, when applied to each of these challenges, recommended differing subsets of clustering algorithms for final quantitative performance evaluation. For each application, the recommended clustering algorithms were confirmed to contain the top performing algorithms through quantitative performance indices. Copyright: © 2022 Crase, Thennadil. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85127398745
"Piccolo S.R., Mecham A., Golightly N.P., Johnson J.L., Miller D.B.","35300280800;57545934000;57201675547;57546325800;57201334076;","The ability to classify patients based on gene-expression data varies by algorithm and performance metric",2022,"PLoS Computational Biology","18","3","e1009926","","",,2,"10.1371/journal.pcbi.1009926","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126976793&doi=10.1371%2fjournal.pcbi.1009926&partnerID=40&md5=013fbb174b243fa5947782ebe473b3ed","By classifying patients into subgroups, clinicians can provide more effective care than using a uniform approach for all patients. Such subgroups might include patients with a particular disease subtype, patients with a good (or poor) prognosis, or patients most (or least) likely to respond to a particular therapy. Transcriptomic measurements reflect the downstream effects of genomic and epigenomic variations. However, high-throughput technologies generate thousands of measurements per patient, and complex dependencies exist among genes, so it may be infeasible to classify patients using traditional statistical models. Machine-learning classification algorithms can help with this problem. However, hundreds of classification algorithms exist—and most support diverse hyperparameters—so it is difficult for researchers to know which are optimal for gene-expression biomarkers. We performed a benchmark comparison, applying 52 classification algorithms to 50 gene-expression datasets (143 class variables). We evaluated algorithms that represent diverse machine-learning methodologies and have been implemented in general-purpose, open-source, machine-learning libraries. When available, we combined clinical predictors with gene-expression data. Additionally, we evaluated the effects of performing hyperparameter optimization and feature selection using nested cross validation. Kernel-@@@@@and ensemble-based algorithms consistently outperformed other types of classification algorithms; however, even the top-performing algorithms performed poorly in some cases. Hyperparameter optimization and feature selection typically improved predictive performance, and univariate feature-selection algorithms typically outperformed more sophisticated methods. Together, our findings illustrate that algorithm performance varies considerably when other factors are held constant and thus that algorithm selection is a critical step in biomarker studies. Copyright: © 2022 Piccolo et al.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85126976793
"Ip W., Prahalad P., Palma J., Chen J.H.","57226644608;57193967488;57532751000;57112911000;","A Data-Driven Algorithm to Recommend Initial Clinical Workup for Outpatient Specialty Referral: Algorithm Development and Validation Using Electronic Health Record Data and Expert Surveys",2022,"JMIR Medical Informatics","10","3","e30104","","",,,"10.2196/30104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126443985&doi=10.2196%2f30104&partnerID=40&md5=a9eee5199aefbce00c232721414e9e64","Background: Millions of people have limited access to specialty care. The problem is exacerbated by ineffective specialty visits due to incomplete prereferral workup, leading to delays in diagnosis and treatment. Existing processes to guide prereferral diagnostic workup are labor-intensive (ie, building a consensus guideline between primary care doctors and specialists) and require the availability of the specialists (ie, electronic consultation). Objective: Using pediatric endocrinology as an example, we develop a recommender algorithm to anticipate patients' initial workup needs at the time of specialty referral and compare it to a reference benchmark using the most common workup orders. We also evaluate the clinical appropriateness of the algorithm recommendations. Methods: Electronic health record data were extracted from 3424 pediatric patients with new outpatient endocrinology referrals at an academic institution from 2015 to 2020. Using item co-occurrence statistics, we predicted the initial workup orders that would be entered by specialists and assessed the recommender's performance in a holdout data set based on what the specialists actually ordered. We surveyed endocrinologists to assess the clinical appropriateness of the predicted orders and to understand the initial workup process. Results: Specialists (n=12) indicated that <50% of new patient referrals arrive with complete initial workup for common referral reasons. The algorithm achieved an area under the receiver operating characteristic curve of 0.95 (95% CI 0.95-0.96). Compared to a reference benchmark using the most common orders, precision and recall improved from 37% to 48% (P<.001) and from 27% to 39% (P<.001) for the top 4 recommendations, respectively. The top 4 recommendations generated for common referral conditions (abnormal thyroid studies, obesity, amenorrhea) were considered clinically appropriate the majority of the time by specialists surveyed and practice guidelines reviewed. Conclusions: An item association-based recommender algorithm can predict appropriate specialists'workup orders with high discriminatory accuracy. This could support future clinical decision support tools to increase effectiveness and access to specialty referrals. Our study demonstrates important first steps toward a data-driven paradigm for outpatient specialty consultation with a tier of automated recommendations that proactively enable initial workup that would otherwise be delayed by awaiting an in-person visit. © 2022 JMIR Publications Inc.. All right reserved.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85126443985
"Škvorc U., Eftimov T., Korošec P.","57210410040;56178012800;35734181400;","Transfer Learning Analysis of Multi-Class Classification for Landscape-Aware Algorithm Selection",2022,"Mathematics","10","3","432","","",,2,"10.3390/math10030432","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124029592&doi=10.3390%2fmath10030432&partnerID=40&md5=97ee59f0412c4d66349f74f65e402815","In optimization, algorithm selection, which is the selection of the most suitable algorithm for a specific problem, is of great importance, as algorithm performance is heavily dependent on the problem being solved. However, when using machine learning for algorithm selection, the performance of the algorithm selection model depends on the data used to train and test the model, and existing optimization benchmarks only provide a limited amount of data. To help with this problem, artificial problem generation has been shown to be a useful tool for augmenting existing benchmark problems. In this paper, we are interested in the problem of knowledge transfer between the artificially generated and existing handmade benchmark problems in the domain of continuous numerical optimization. That is, can an algorithm selection model trained purely on artificially generated problems correctly provide algorithm recommendations for existing handmade problems. We show that such a model produces low-quality results, and we also provide explanations about how the algorithm selection model works and show the differences between the problem data sets in order to explain the model’s performance. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85124029592
"Ghambari S., Rakhshani H., Lepagnot J., Jourdan L., Idoumghar L.","57198358640;57189489701;35773578800;6603180053;6507715145;","Unbalanced budget distribution for automatic algorithm configuration",2022,"Soft Computing","26","3",,"1315","1330",,,"10.1007/s00500-021-06403-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118464590&doi=10.1007%2fs00500-021-06403-y&partnerID=40&md5=310ce1aaa6b465e19d9a708949592f3a","Optimization algorithms often have several critical setting parameters and the improvement of the empirical performance of these algorithms depends on tuning them. Manually configuration of such parameters is a tedious task that results in unsatisfactory outputs. Therefore, several automatic algorithm configuration frameworks have been proposed to regulate the parameters of a given algorithm for a series of problem instances. Although the developed frameworks perform very well to deal with various problems, however, there is still a trade-off between the accuracy and budget requirements that need to be addressed. This work investigates the performance of unbalanced distribution of budget for different configurations to deal with the automatic algorithm configuration problem. Inspired by the bandit-based approaches, the main goal is to find a better configuration that substantially improves the performance of the target algorithm while using a smaller run time budget. In this work, non-dominated sorting genetic algorithm II is employed as a target algorithm using jMetalPy software platform and the multimodal multi-objective optimization (MMO) test suite of CEC’2020 is used as a set of test problems. We did a comprehensive comparison with other known methods including random search, Bayesian optimization, sequential model-based algorithm configuration (SMAC), iterated local search in parameter configuration space (ParamILS), iterated racing for automatic algorithm configuration (irace), and many-objective automatic algorithm configuration (MAC) methods. In order to characterize, validate and evaluate the performance of these methods, hypervolume (HV), generational distance, and epsilon indicator (Iϵ+) are used as performance indicators. The experimental results interestingly proved the efficiency of the proposed approach for automatic algorithm configuration with a minimum time budget in comparison with other competitors. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85118464590
"De Coster A., Musliu N., Schaerf A., Schoisswohl J., Smith-Miles K.","57253919600;7801571378;6701629145;57504276900;55369980900;","Algorithm selection and instance space analysis for curriculum-based course timetabling",2022,"Journal of Scheduling","25","1",,"35","58",,2,"10.1007/s10951-021-00701-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114668116&doi=10.1007%2fs10951-021-00701-x&partnerID=40&md5=3c7b2f89ce7856e662abb4d13c371a1d","We propose an algorithm selection approach and an instance space analysis for the well-known curriculum-based course timetabling problem (CB-CTT), which is an important problem for its application in higher education. Several state of the art algorithms exist, including both exact and metaheuristic methods. Results of these algorithms on existing instances in the literature show that there is no single algorithm outperforming the others. Therefore, a deep analysis of the strengths and weaknesses of these algorithms, depending on the instance, is an important research question. In this work, a detailed analysis of the instance space for CB-CTT is performed, charting the regions where these algorithms perform best. We further investigate the application of machine learning methods to automated algorithm selection for CB-CTT, strengthening the insights gained through the instance space analysis. For our research, we contribute new real-life instances and extend the generation of synthetic instances to better correspond to these new instances. Finally, this work shows how instance space analysis and the application of algorithm selection complement each other, underlining the value of both approaches in understanding algorithm performance. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85114668116
"Karimi-Mamaghan M., Mohammadi M., Meyer P., Karimi-Mamaghan A.M., Talbi E.-G.","57203638620;55613229146;35237543500;56421538900;6701668267;","Machine learning at the service of meta-heuristics for solving combinatorial optimization problems: A state-of-the-art",2022,"European Journal of Operational Research","296","2",,"393","422",,30,"10.1016/j.ejor.2021.04.032","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106881984&doi=10.1016%2fj.ejor.2021.04.032&partnerID=40&md5=5da65b8d463515e3b3fecc39207d3dc6","In recent years, there has been a growing research interest in integrating machine learning techniques into meta-heuristics for solving combinatorial optimization problems. This integration aims to lead meta-heuristics toward an efficient, effective, and robust search and improve their performance in terms of solution quality, convergence rate, and robustness. Since various integration methods with different purposes have been developed, there is a need to review the recent advances in using machine learning techniques to improve meta-heuristics. To the best of our knowledge, the literature is deprived of having a comprehensive yet technical review. To fill this gap, this paper provides such a review on the use of machine learning techniques in the design of different elements of meta-heuristics for different purposes including algorithm selection, fitness evaluation, initialization, evolution, parameter setting, and cooperation. First, we describe the key concepts and preliminaries of each of these ways of integration. Then, the recent advances in each way of integration are reviewed and classified based on a proposed unified taxonomy. Finally, we provide a technical discussion on the advantages, limitations, requirements, and challenges of implementing each of these integration ways, followed by promising future research directions. © 2021 The Authors",Review,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85106881984
"Patil P.S., Kappuram K., Rumao R., Bari P.","57879053800;57878901300;57878740200;57747328700;","Development of AMES: Automated ML Expert System",2022,"2022 International Conference on Machine Learning, Big Data, Cloud and Parallel Computing, COM-IT-CON 2022",,,,"208","213",,,"10.1109/COM-IT-CON54601.2022.9850737","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137461606&doi=10.1109%2fCOM-IT-CON54601.2022.9850737&partnerID=40&md5=a14573409a7b078d34e1d477ab549448","There has been an exponential rise in the quantity of data in the last few decades and as a consequence of this, the need for Machine Learning based applications has increased in every domain. In recent years, Machine Learning has been used in many fields to achieve significant breakthroughs. These fields include financial services, transportation, healthcare, e-commerce, retail, etc. wherein Machine Learning has been used for innovation, transformation, and optimization to get highly promising results. In today's world, Machine Learning is not used only for research and development applications but also in the enterprise domain. However, traditional Machine Learning methods are dependent on humans and that is not a feasible option for businesses having limited resources and those which cannot invest in a highly qualified data science team. Even in the case of Machine Learning engineers who are in high demand across various industries, improving the efficiency of tasks related to Machine Learning has become a challenge. This calls for the creation of an application that can automate the end-to-end process of applying machine learning solutions to real-world problems. 'AMES: Automated ML Expert System' will make Machine Learning truly available, even to people with minimal expertise in this field. Such a system will increase productivity by automating repetitive tasks, help to avoid errors that might creep in due to manual processes, and democratize Machine Learning by making the power of ML accessible to everybody. Tasks such as Hyperparameter Optimization, feature engineering, data preprocessing, visualization, and even model selection, if automated, will prove to be of great benefit to ML engineers and novice users alike. This project aims to automate all these tasks and make the process of building a model simple, quick and efficient. © 2022 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85137461606
"Huo J., Al-Neshmi H.M.M.","22634265300;57218499383;","Hyperparameters optimisation of ensemble classifiers and its application for landslide hazards classification",2022,"International Journal of Modelling, Identification and Control","40","2",,"158","175",,,"10.1504/IJMIC.2022.10047439","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136998694&doi=10.1504%2fIJMIC.2022.10047439&partnerID=40&md5=640a1974e3c4f697dc01662304fd0306","Along with assessing the hazards of landslides taking into consideration the faced difficulties and the consumed time when determining the algorithms configurations and parameters manually, the primary aspiration of this study is to optimise the parameters of two ensemble-based machine learning algorithms using particle swarm optimisation, genetic algorithm, and Bayesian optimisation so that the optimised algorithms can identify and classify landslides more efficiently and accurately. Random forest classifier and XGBoost models were used and the ADASYN was implemented to overcome the shortage of imbalanced data. In the experiments, it was clearly shown that the hypered ensemble-based models along with the PSO and GA successfully surpassed the single models on classifying the landslides' triggers, sizes, and types. The experimental results demonstrated that the hyperparameters optimisation can greatly improve the accuracy of the ensemble classifiers, thus it can provide accurate classification results and decision support for the disaster prevention and mitigation management departments. © 2022 Inderscience Enterprises Ltd.",Article,"Final","",Scopus,2-s2.0-85136998694
"Prager R.P., Seiler M.V., Trautmann H., Kerschke P.","57211237852;57219109157;23974957500;56336853600;","Automated Algorithm Selection in Single-Objective Continuous Optimization: A Comparative Study of Deep Learning and Landscape Analysis Methods",2022,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13398 LNCS",,,"3","17",,,"10.1007/978-3-031-14714-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136937461&doi=10.1007%2f978-3-031-14714-2_1&partnerID=40&md5=d1d4aa415873dae170b0c51b12c1f52a","In recent years, feature-based automated algorithm selection using exploratory landscape analysis has demonstrated its great potential in single-objective continuous black-box optimization. However, feature computation is problem-specific and can be costly in terms of computational resources. This paper investigates feature-free approaches that rely on state-of-the-art deep learning techniques operating on either images or point clouds. We show that point-cloud-based strategies, in particular, are highly competitive and also substantially reduce the size of the required solver portfolio. Moreover, we highlight the effect and importance of cost-sensitive learning in automated algorithm selection models. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85136937461
"Cohen-Shapira N., Rokach L.","57211939999;9276243500;","Learning dataset representation for automatic machine learning algorithm selection",2022,"Knowledge and Information Systems",,,,"","",,,"10.1007/s10115-022-01716-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135401097&doi=10.1007%2fs10115-022-01716-2&partnerID=40&md5=98b5a3204efbfb6950b71560cd373e08","The algorithm selection problem is defined as identifying the best-performing machine learning (ML) algorithm for a given combination of dataset, task, and evaluation measure. The human expertise required to evaluate the increasing number of ML algorithms available has resulted in the need to automate the algorithm selection task. Various approaches have emerged to handle the automatic algorithm selection challenge, including meta-learning. Meta-learning is a popular approach that leverages accumulated experience for future learning and typically involves dataset characterization. Existing meta-learning methods often represent a dataset using predefined features and thus cannot be generalized across different ML tasks, or alternatively, learn a dataset’s representation in a supervised manner and therefore are unable to deal with unsupervised tasks. In this study, we propose a novel learning-based task-agnostic method for producing dataset representations. Then, we introduce TRIO, a meta-learning approach, that utilizes the proposed dataset representations to accurately recommend top-performing algorithms for previously unseen datasets. TRIO first learns graphical representations for the datasets, using four tools to learn the latent interactions among dataset instances and then utilizes a graph convolutional neural network technique to extract embedding representations from the graphs obtained. We extensively evaluate the effectiveness of our approach on 337 datasets and 195 ML algorithms, demonstrating that TRIO significantly outperforms state-of-the-art methods for algorithm selection for both supervised (classification and regression) and unsupervised (clustering) tasks. © 2022, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Article,"Article in Press","",Scopus,2-s2.0-85135401097
"Nayak A., Božić B., Longo L.","57641685800;57519074600;24314788200;","An Ontological Approach for Recommending a Feature Selection Algorithm",2022,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13362 LNCS",,,"300","314",,,"10.1007/978-3-031-09917-5_20","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135046964&doi=10.1007%2f978-3-031-09917-5_20&partnerID=40&md5=08639c7114697834df5192ad7a3bcb8f","Feature selection plays an important role in machine learning or data mining problems. Removing irrelevant features increases model accuracy and reduces the computational cost. However, selecting important features is not a simple task as one feature selection algorithm does not perform well on all the datasets that are of interest. This paper tries to address the recommendation of a feature selection algorithm based on dataset characteristics and quality. The research uses three types of dataset characteristics along with data quality metrics. The main contribution of the work is the utilization of Semantic Web techniques to develop a novel system that can aid in robust feature selection algorithm recommendations. The system’s strength lies in assisting users of machine learning algorithms by providing more relevant feature selection algorithms for the dataset using an ontology called Feature Selection algorithm recommendation based on Data Characteristics and Quality (FSDCQ). Results are generated using six different feature selection algorithms and four types of classifiers on ten datasets from UCI repository. Recommendations take the form of “Feature selection algorithm X is recommended for dataset i, as it performed better on dataset j, similar to dataset i in terms of class overlap 0.3, label noise 0.2, completeness 0.9, conciseness 0.8 units"". While the domain-specific ontology FSDCQ was created to aid in the task of algorithm recommendation for feature selection, it is easily applicable to other meta-learning scenarios. © 2022, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85135046964
"Qiao K., Yu K., Qu B., Liang J., Yue C., Ban X.","57214235942;56182208900;35178088100;57218094690;57192820832;57323366800;","Feature Extraction for Recommendation of Constrained Multi-Objective Evolutionary Algorithms",2022,"IEEE Transactions on Evolutionary Computation",,,,"1","1",,,"10.1109/TEVC.2022.3186667","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133787354&doi=10.1109%2fTEVC.2022.3186667&partnerID=40&md5=3108e7f44bad73e84649f78f1b573acd","The evolutionary algorithm recommendation is catching increasing attention when solving practical application problems since different algorithms often perform differently on different problems. To achieve the algorithm recommendation, extracting effective features to accurately characterize the problems is necessary, which is related to the feature extraction problem. So far, most feature extraction methods focus on single-objective optimization problems, and only a few studies are conducted on multi-objective optimization problems and constrained optimization problems, let alone constrained multi-objective optimization problems that are widely encountered in the real world. To fill the gap, this paper proposes an evolution-based constrained multi-objective feature extraction method (ECMOFE), in which the information generated in the evolutionary process is leveraged to form the feature matrix. To be specific, we create two populations to respectively optimize constraints and objectives for some generations. Furthermore, two complementary evolutionary operators are used to generate offspring for each population. In the environmental selection, the successful rate of offspring individuals generated by each operator of each population is recorded to form the feature matrix. Then, a dimension reduction method is designed to compress the size of the feature matrix. By the above process, the feature vector that can reflect the global relationship between constraints and objectives and the difficulty of the CMOP is formed. Based on the formed features, several algorithm recommendation methods are built on the basis of classifiers. The results based on multiple metrics show the effectiveness of the proposed ECMOFE. IEEE",Article,"Article in Press","",Scopus,2-s2.0-85133787354
"Afshar R.R., Rhuggenaath J., Zhang Y., Kaymak U.","56767837100;57205540467;57189904037;7004385524;","An Automated Deep Reinforcement Learning Pipeline for Dynamic Pricing",2022,"IEEE Transactions on Artificial Intelligence",,,,"1","10",,,"10.1109/TAI.2022.3186292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133739199&doi=10.1109%2fTAI.2022.3186292&partnerID=40&md5=392ae75d4830509e2c6ed2d8715b1b96","Dynamic pricing problem is difficult due to the highly dynamic environment and unknown demand distributions. In this paper, we propose a Deep Reinforcement Learning (DRL) framework, which is a pipeline that automatically defines the DRL components for solving a Dynamic Pricing problem. The automated DRL pipeline is necessary because the DRL framework can be designed in numerous ways, and manually finding optimal configurations is tedious. The levels of automation make non-experts capable of using DRL for dynamic pricing. Our DRL pipeline contains three steps of DRL design, including MDP modeling, algorithm selection, and hyper-parameter optimization. It starts with transforming available information to state representation and defining reward function using a reward shaping approach. Then, the hyper-parameters are tuned using a novel hyper-parameters optimization method that integrates Bayesian Optimization and the selection operator of the Genetic algorithm. We employ our DRL pipeline on reserve price optimization problems in online advertising as a case study. We show that using the DRL configuration obtained by our DRL pipeline, a pricing policy is obtained whose revenue is significantly higher than the benchmark methods. The evaluation is performed by developing a simulation for the RTB environment that makes exploration possible for the RL agent. IEEE",Article,"Article in Press","",Scopus,2-s2.0-85133739199
"Reilly D., Taylor M., Fergus P., Chalmers C., Thompson S.","7103164701;55346872200;55905767600;57014950600;57219497111;","The Categorical Data Conundrum: Heuristics for classification problems &#x2013; a case study on domestic fire injuries",2022,"IEEE Access",,,,"1","1",,,"10.1109/ACCESS.2022.3187287","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133738304&doi=10.1109%2fACCESS.2022.3187287&partnerID=40&md5=d6cf082f2b2caa0fb671dfcba86acb68","Machine learning is well developed amongst the scientific community in terms of theoretical foundations (statistics and algorithms) and frameworks (Tensorflow, PyTorch, H2O). However, machine learning is heavily focused on numerical data, or numerical data mixed with some categorical data. For numerical datasets, scientists and engineers can enjoy reasonable success with only a limited knowledge of theoretical foundations and the inner workings of machine learning frameworks. However, it is a different story when dealing with purely categorical datasets, which require a deeper understanding of machine learning frameworks and associated encodings and algorithms in order to achieve success. This paper addresses the issues in handling purely categorical datasets for multi-classification problems and provides a set of heuristics for dealing with purely categorical data. In particular, issues such as pre-processing, feature encoding and algorithm selection are considered. The heuristics are then demonstrated through a case study, based on a categorical data set of domestic fire injuries, covering a 10-year period. Novel contributions are made through the heuristics and the performance analysis of different encoding techniques. The case study itself also makes a novel contribution through the classification of different types of injuries, based on related features. Author",Article,"Article in Press","All Open Access, Gold",Scopus,2-s2.0-85133738304
"Sun Y., Esler S., Thiruvady D., Ernst A.T., Li X., Morgan K.","56661412300;57196116505;6506160827;7103004297;12241812800;55422158900;","Instance space analysis for the car sequencing problem",2022,"Annals of Operations Research",,,,"","",,,"10.1007/s10479-022-04860-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133637753&doi=10.1007%2fs10479-022-04860-8&partnerID=40&md5=e79fc57ec2f47be674a0610c16ff317b","We investigate an important research question for solving the car sequencing problem, that is, which characteristics make an instance hard to solve? To do so, we carry out an instance space analysis for the car sequencing problem, by extracting a vector of problem features to characterize an instance. In order to visualize the instance space, the feature vectors are projected onto a 2-D space using dimensionality reduction techniques. The resulting 2-D visualizations provide new insights into the characteristics of the instances used for testing and how these characteristics influence the behaviours of an optimization algorithm. This analysis guides us in constructing a new set of benchmark instances with a range of instance properties. We demonstrate that these new instances are more diverse than the previous benchmarks, including some instances that are significantly more difficult to solve. We introduce two new algorithms for solving the car sequencing problem and compare them with four existing methods from the literature. Our new algorithms are shown to perform competitively for this problem but no single algorithm can outperform all others over all instances. This observation motivates us to build an algorithm selection model based on machine learning, to identify the niche in the instance space that an algorithm is expected to perform well on. Our analysis helps to understand problem hardness and select an appropriate algorithm for solving a given car sequencing problem instance. © 2022, The Author(s).",Article,"Article in Press","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85133637753
"Su Y., Sun W.","57788100400;57786307700;","Classification and interaction of new media instant music video based on deep learning under the background of artificial intelligence",2022,"Journal of Supercomputing",,,,"","",,,"10.1007/s11227-022-04672-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133592040&doi=10.1007%2fs11227-022-04672-4&partnerID=40&md5=aa632a1ff8193ebff5b699c1045be77f","With the continuous upgrading and improvement in the Internet and terminal equipment, many instant music videos share information with users through social platforms. This study explores the impact of new media technology on the content of instant music videos on the Internet under Artificial Intelligence (AI) technology to effectively distinguish the elegant and vulgar short videos and improve the quality of short videos on the Internet. Obscene and harmful instant music videos in the massive data are the bottleneck for its development. An improved deep learning model is proposed based on OPEN_NSFW using the AI image detection system technology of the Internet of Things with a powerful processing ability to image information. Experiments demonstrate that this model significantly reduces the false positive rate and improves the recall compared with the traditional machine learning computing model. Besides, it improves the accuracy when discriminating whether the publisher’s head image involves eroticism. In addition, this model can identify and classify the main content of instant music videos to optimize the content. This work provides the characteristic basis for the algorithm to judge and protect the original content. Combining algorithm recommendations and strengthening manual intervention promotes online instant music videos' sustainable and healthy development. These findings can provide an excellent technical guarantee and experimental references for the standardized development of the instant music video industry in the future. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Article in Press","",Scopus,2-s2.0-85133592040
"Liu C., Zhang S.","57780471100;57781530700;","Design of simulation teaching platform for traffic signal control optimization based on AI algorithm",2022,"Proceedings of SPIE - The International Society for Optical Engineering","12173",,"121731D","","",,,"10.1117/12.2634500","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133408112&doi=10.1117%2f12.2634500&partnerID=40&md5=ea7e762c34a9e491007fabeb17f83bbc","With the rapid development of artificial intelligence, reinforcement learning techniques are emerging, and in the field of transportation, the control of traffic signals can be optimized using reinforcement learning algorithm to improve traffic efficiency. In teaching, the reinforcement learning algorithm is more difficult than the traditional timing algorithm to understand, and there is a lack of teaching platform - which can not only give students an in-depth understanding of the algorithm and advantages of reinforcement learning, but also facilitate teachers to evaluate students' mastery. In view of the above problems, this project designs and develops a traffic signal control optimization simulation teaching platform using AI algorithm, which realizes a number of reinforcement learning algorithms, and combines the basic characteristics of teaching, adopts a distributed system structure, which can meet the needs of online teaching and evaluation. The platform is divided into teacher side and student side, the main functions of the student side are: SUMO road network simulation model selection, action mechanism selection (and input related parameters), reinforcement learning algorithm selection (Q-learning and Sarsa), model input parameters and evaluation parameters (such as fuel consumption, queue number) selection, algorithm operation, statistical output and so on. During the operation, the platform displays the statistical results in real time in the form of dynamic graphs, and after the operation, outputs static charts of evaluation data of different algorithms. The platform has the function of evaluating and scoring according to the weighted average of the optimization of output results. According to the requirements set by teachers, it can score the aspects of pass efficiency and environmental impact, and generate experimental reports as the basis for teaching evaluation. The teacher can view the completion of the experiment and the evaluation results of the experiment, so as to check the students' learning of the reinforcement learning algorithm. The work done in this subject, on the one hand, helps students to study and experiment with advanced AI algorithms, on the other hand, it can also help teachers to master teaching situation, improve teaching quality, and it is of great significance to the application of AI algorithms in the field of traffic signal control optimization and personnel training. © 2022 SPIE",Conference Paper,"Final","",Scopus,2-s2.0-85133408112
"Avinash M., Nithya M., Aravind S.","57192373813;57200376770;57214204602;","Automated Machine Learning-Algorithm Selection with Fine-Tuned Parameters",2022,"Proceedings - 2022 6th International Conference on Intelligent Computing and Control Systems, ICICCS 2022",,,,"1175","1180",,,"10.1109/ICICCS53718.2022.9788236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85133154001&doi=10.1109%2fICICCS53718.2022.9788236&partnerID=40&md5=bf311a96bb168e5b4028a7241f8c1185","In recent years, the impact and usage of Machine Learning have increased multi-fold. To get the best outcome, it is imperative to choose the right algorithm. Hence, Algorithm Selection is one of the most important stages in the Machine Learning Lifecycle. Selecting the optimal algorithm out of many possible algorithms for a given problem statement is always a tedious and time-consuming process. This project assists the researchers and Machine Learning enthusiasts in selecting the appropriate algorithm by providing a comparative selection among the different machine learning algorithms available for a pre-processed dataset. Besides the best Algorithm Selection, this project also aims to provide the best hyper-parameters for the opted algorithm using a novel technique. The Algorithm Selection functionality is furnished by comparing the various accuracy metrics like Accuracy, Recall, Precision, F1 Score, R2 Score, Mean Squared Error, etc. The objective of this project is to compare numerous algorithms and suggest the best algorithm. Thus, the user can instead concentrate on Data Collection and Data Pre-processing to achieve success in building the best model. © 2022 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85133154001
"Atheaux I., Makarychev-Mikhailov S.M., Faria D.C., Farrell J.W., MacKay B.A.","57743059000;6602459947;57742562000;52863320300;56785460600;","Produced Water, X-Ray Fluorescence Spectrometry and Machine Learning: How Can They be Connected?",2022,"Society of Petroleum Engineers - SPE International Oilfield Scale Conference and Exhibition, OSS 2022",,,,"","",,,"10.2118/209505-MS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132040294&doi=10.2118%2f209505-MS&partnerID=40&md5=dbb98c12b3c732b41b794e37d26dd376","Fast and reliable field-portable chemical analysis of produced waters remains one of the main challenges of scale-risk mitigation, as it enables timely control over scale inhibitor type and dosage. While many analytical methods are potentially applicable to produced waters, most of them lack reliability under field conditions or do not meet increasingly tight cost requirements. X-ray fluorescence (XRF) spectrometry is routinely used in the oil field for analysis of cores, drilling cuttings, and muds, as it provides quick noninvasive detection of many elements simultaneously. Inexpensive portable handheld analyzers typically feature limited element range, sensitivity, and resolution, and are therefore expected to be inferior to benchtop analyzers. The question is whether handheld devices can offer suitable detection limits and accuracy for produced water analysis, despite these limitations. At the same time, one of the major challenges of XRF, the so-called matrix effect, is strong in produced waters and also negatively affects the analysis accuracy. This study demonstrates how multivariate machine-learning (ML) techniques can be applied to the full XRF spectra recorded with a handheld analyzer. ML spectra processing is shown to successfully mitigate matrix effects and enable simultaneous quantification of all ions of interest. Interestingly, key physical (density) and chemical (total dissolved solids and hardness) properties of produced water can also be quantified using ML techniques. In the paper, the experimental protocols are described first, followed by a detailed discussion of the data workflows, which covers the XRF spectra preprocessing, algorithm selection and tuning, and independent validation procedures. Over 50 different ML algorithms are trained on different spectra ranges of a multicomponent calibration dataset, and the three best models are applied to several real-life produced water sample sets for validation. A rigorous error analysis is performed for all ML models. In field samples, the resulting analysis errors (RMSE) are less than 100 mg/L for barium and strontium, less than 150 mg/L for sulfate, and remarkably small/accurate for other ions and properties considering measurement with a handheld device. Copyright 2022, Society of Petroleum Engineers DOI 10.2118/209505-MS",Conference Paper,"Final","",Scopus,2-s2.0-85132040294
"Zhou Y., Wang C., Zhou R., Wang X., Wang H., Yu Y.","57190180590;55904612000;57737345700;57828942200;57738353100;57422634500;","A Specific Emitter Identification Method Based on RF-DNA and XGBoost",2022,"2022 7th International Conference on Intelligent Computing and Signal Processing, ICSP 2022",,,,"1530","1533",,,"10.1109/ICSP54964.2022.9778627","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131805659&doi=10.1109%2fICSP54964.2022.9778627&partnerID=40&md5=435e809c50dffbd74e45675f1d660416","Specific emitter identification (SEI) is a promising research direction in artificial intelligence and Internet of Things. Aiming at the fingerprint features extraction and the identification algorithm selection for SEI, a novel method based on RF-DNA feature set and extreme gradient boosting (XGBoost) algorithm is proposed in this paper. Firstly, considering the advantages of RF-DNA in characterizing the fluctuation degree of instantaneous sequences, a RF-DNA feature set is constructed based on statistical features extracted from instantaneous frequency, instantaneous phase and instantaneous amplitude of signals. Then, the XGBoost algorithm is used to perform feature learning on the structured RF-DNA data set. Finally, three civil communication emitters of the same model are used as the identification objects to verify the performance of the identification method. Experimental results show that the RFDNA feature set exhibits satisfactory feature expression performance, and the XGBoost algorithm shows favorable feature learning properties. © 2022 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85131805659
"Gaur V., Kumar R.","57210728479;35785765700;","DDoSLSTM: Detection of Distributed Denial of Service Attacks on IoT Devices using LSTM Model",2022,"2022 International Conference on Communication, Computing and Internet of Things, IC3IoT 2022 - Proceedings",,,,"","",,,"10.1109/IC3IOT53935.2022.9767889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130789243&doi=10.1109%2fIC3IOT53935.2022.9767889&partnerID=40&md5=c4cd674533ff73e3cc105fd90efe356d","Distributed Denial of Service (DDoS) attack is a persistent complication in the network's security. These attacks have been detected by many machine learning algorithms and feature selection methods. This paper chose the Recurrent Neural Network based long short-term memory model that works on time series data and handles long time-dependent inputs, thereby detecting DDoS attacks. In our paper, we focused primarily on increasing the classification performance of the LSTM model. Multi-layer LSTM model has been used for binary and multiclass data and maximum accuracy attained is 99.46% (1- Layer LSTM with Binary data) followed by 99.16% for 2-Layer LSTM with Multiclass Grouped data. The proposed DDoSLSTM model outperforms other state-of-the-art techniques, including deep neural network (DNN), RNN, CNN, Transformers. © 2022 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85130789243
"Garouani M., Zaysa K.","57226345482;57699736400;","Leveraging the Automated Machine Learning for Arabic Opinion Mining: A Preliminary Study on AutoML Tools and Comparison to Human Performance",2022,"Lecture Notes in Networks and Systems","455 LNNS",,,"163","171",,,"10.1007/978-3-031-02447-4_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130362929&doi=10.1007%2f978-3-031-02447-4_17&partnerID=40&md5=497b22532ae63f1e65a5e3ff71c93287","Despite the broad range of Machine Learning (ML) algorithms, there are no clear guidelines on how to identify the optimal algorithm and corresponding hyperparameters configurations given an Opinion Mining (OM) problem. In ML, this is known as the Algorithm Selection Problem (ASP). Although Automatic Algorithm Selection or AutoML has proven to be successful in many areas of ASP, it has hardly been explored in OM. This paper explores the benefits of using AutoML in this field. To this end, this work examines to what extent AutoML can be competitive against ad hoc methods (manually select and tune ML pipelines) on Arabic opinion mining modeled from a supervised learning perspective. We compare four state-of-the-art AutoML tools on 10 different popular datasets to human performance. Experimental results show that the AutoML technology can be considered as a powerful approach to support the ML algorithm selection problem in opinion mining. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85130362929
"Veeramakali T., Shobanadevi A., Nayak N.R., Kumar S., Singhal S., Subramanian M.","57194048821;57202971882;57207953064;57711773400;57194063002;57708907700;","Preserving the Privacy of Healthcare Data over Social Networks Using Machine Learning",2022,"Computational Intelligence and Neuroscience","2022",,"4690936","","",,,"10.1155/2022/4690936","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130050853&doi=10.1155%2f2022%2f4690936&partnerID=40&md5=8fad84a8bb5cb8ceb85e015bc393fbe4","A key challenge in clinical recommendation systems is the problem of aberrant patient profiles in social networks. As a result of a person's abnormal profile, numerous vests might be used to make fake remarks about them, cyber bullying, or cyber-attacks. Many clinical researchers have done extensive study on this topic. The most recent studies on this topic are summarized, and an overarching framework is provided. When it comes to the methods and datasets that make up the data collection, the feature presentation and algorithm selection layers provide an overview of the various types of algorithm selections available. The categorization and evaluation of diseases and disorders has been one of the major advantages of machine learning in medical. Because it was harder to predict, it rendered it more controllable. It might range from difficult-to-find cancers in the early stages to certain other illnesses spread through the bloodstream. In healthcare, we may pick methods in machine learning depending on reliable outcomes. To do so, we must run the findings through each method. The major issue arises during information training and validation. Because the dataset is so large, eliminating mistakes might be difficult. The providers, other characteristics, various algorithms, data labelling techniques, and assessment criteria are all presented and contrasted in depth. Detecting anomalous users in medical social networks, on the other hand, is a work in progress. The result evaluation layer provides an explanation of how to evaluate and mark up the results of the various algorithm selection layers. Finally, it looks forward to more study in this area. © 2022 T. Veeramakali et al.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85130050853
"Shaikh S.G., Suresh Kumar B., Narang G.","57671794000;57210948184;14829403500;","Recommender system for health care analysis using machine learning technique: a review",2022,"Theoretical Issues in Ergonomics Science","23","5",,"613","642",,,"10.1080/1463922X.2022.2061078","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129682371&doi=10.1080%2f1463922X.2022.2061078&partnerID=40&md5=3f1670d2cc3065bb9d47c764003f1b01","Recommender systems use different techniques of machine learning (ML) to suggest users and recommend service or entity in various field of application such as in health care recommender system (HRS). Due to the vast count of algorithms shown in the literature, HRS and various application sectors are now utilizing ML algorithms from the area of artificial intelligence. However, selecting an appropriate ML algorithm in the case of a health recommender system seems to be a time-consuming task. However the development of recommender system in different service domain faces problems of algorithms selection for better accuracy. This article examined the usage of ML techniques in recommender systems for health applications through a survey of the literature. The objectives of this article are (i) recognize the literature review finding of recommender system in health applications using ML and deep learning algorithms. (ii) Assist new researchers with the help of gap in previous research. The results of this study is to proposed new recommender system in health application of mosquito borne disease by using hybrid approach of ML technique. © 2022 Informa UK Limited, trading as Taylor & Francis Group.",Review,"Final","",Scopus,2-s2.0-85129682371
"Ruiz De Arcaute G.M., Hernandez J.A., Reviriego P.","57670565600;35479093500;19639262200;","Assessing the Impact of Membership Inference Attacks on Classical Machine Learning Algorithms",2022,"2022 18th International Conference on the Design of Reliable Communication Networks, DRCN 2022",,,,"","",,,"10.1109/DRCN53993.2022.9758025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129596421&doi=10.1109%2fDRCN53993.2022.9758025&partnerID=40&md5=204662428dcce76b9354f4c28dc1b4a8","In the last decade, machine learning has been widely adopted in many areas and the trend not only continues but accelerates. This has raised many issues ranging from ethics and security to privacy. In particular, it has been shown that in some settings an attacker can infer if a given element has been used to train a machine learning model. This is not acceptable in many applications that deal with sensitive data and thus designers need to take privacy into account when implementing machine learning based systems. In this paper, we study the impact of such membership inference attacks on traditional machine learning algorithms. Our goals are 1) to identify if there are some algorithms that are more vulnerable than others, and 2) if privacy can be guaranteed by selecting the algorithm parameters and if so at what cost on performance. Our results show that ensemble averaging models, especially Extra-Trees, are vulnerable to this attack. This information can be used by designers as an additional input to guide the machine learning algorithm selection process so that privacy is incorporated as a design goal from the beginning. © 2022 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85129596421
"Tanabe R.","57665377500;","Benchmarking Feature-based Algorithm Selection Systems for Black-box Numerical Optimization",2022,"IEEE Transactions on Evolutionary Computation",,,,"","",,,"10.1109/TEVC.2022.3169770","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129442603&doi=10.1109%2fTEVC.2022.3169770&partnerID=40&md5=fff45327419582cb34c019ac316d36a6","Feature-based algorithm selection aims to automatically find the best one from a portfolio of optimization algorithms on an unseen problem based on its landscape features. Feature-based algorithm selection has recently received attention in the research field of black-box numerical optimization. However, there is still room for analysis of algorithm selection for black-box optimization. Most previous studies have focused only on whether an algorithm selection system can outperform the single-best solver in a portfolio. In addition, a benchmarking methodology for algorithm selection systems has not been well investigated in the literature. In this context, this paper analyzes algorithm selection systems on the 24 noiseless black-box optimization benchmarking functions. First, we demonstrate that the first successful performance measure is more reliable than the expected runtime measure for benchmarking algorithm selection systems. Then, we examine the influence of randomness on the performance of algorithm selection systems. We also show that the performance of algorithm selection systems can be significantly improved by using sequential least squares programming as a pre-solver. We point out that the difficulty of outperforming the single-best solver depends on algorithm portfolios, cross-validation methods, and dimensions. Finally, we demonstrate that the effectiveness of algorithm portfolios depends on various factors. These findings provide fundamental insights for algorithm selection for black-box optimization. IEEE",Article,"Article in Press","All Open Access, Green",Scopus,2-s2.0-85129442603
"Gokiladevi M., Santhoshkumar S., Varadarajan V.","57659903800;57224523941;57314747500;","MACHINE LEARNING ALGORITHM SELECTION FOR CHRONIC KIDNEY DISEASE DIAGNOSIS AND CLASSIFICATION",2022,"Malaysian Journal of Computer Science","2022","Special Issue 1",,"102","115",,,"10.22452/mjcs.sp2022no1.8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85129250817&doi=10.22452%2fmjcs.sp2022no1.8&partnerID=40&md5=9c772955c361dc42eda80d67a71566ab","In last decades, chronic kidney disease (CKD) becomes a global health problem that is steadily developing worldwide. It is a chronic illness highly related to increased morbidity and mortality, cardiovascular diseases, and high healthcare cost. Earlier identification and classification of CKD is treated as a major factor in controlling the mortality rate. Data mining (DM) techniques are used for the extraction of hidden details from the clinical and laboratory patient data that is used to aid doctors in enhancing diagnostic accuracy. Recently, machine learning (ML) techniques are commonly employed for the prediction and classification of diseases in healthcare sector. With this motivation, this study examines the performance of different ML algorithms to diagnose CKD at the earlier stages. The proposed model involves data pre-processing in two stages such as missing value replacement and data transformation. Besides, a set of five ML based classification models are involved such as support vector machine (SVM), random forest (RF), logistic regression (LR), K-nearest neighbor (KNN), and decision tree (DT). For investigating the performance of the different ML models, a benchmark CKD dataset from UCI repository is employed and the results are examined under different aspects. Among the different classifiers, the RF model has accomplished superior results with the maximum precision of 0.99, recall of 0.99, and F-score of 0.99 with a minimal error rate of 0.012. © 2022. All Rights Reserved.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85129250817
"Tornede A., Gehring L., Tornede T., Wever M., Hüllermeier E.","57216673585;57226400635;57216341230;57195224973;6701552637;","Algorithm selection on a meta level",2022,"Machine Learning",,,,"","",,1,"10.1007/s10994-022-06161-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128259652&doi=10.1007%2fs10994-022-06161-4&partnerID=40&md5=5942c529b3ffeb0400450e0f0babde89","The problem of selecting an algorithm that appears most suitable for a specific instance of an algorithmic problem class, such as the Boolean satisfiability problem, is called instance-specific algorithm selection. Over the past decade, the problem has received considerable attention, resulting in a number of different methods for algorithm selection. Although most of these methods are based on machine learning, surprisingly little work has been done on meta learning, that is, on taking advantage of the complementarity of existing algorithm selection methods in order to combine them into a single superior algorithm selector. In this paper, we introduce the problem of meta algorithm selection, which essentially asks for the best way to combine a given set of algorithm selectors. We present a general methodological framework for meta algorithm selection as well as several concrete learning methods as instantiations of this framework, essentially combining ideas of meta learning and ensemble learning. In an extensive experimental evaluation, we demonstrate that ensembles of algorithm selectors can significantly outperform single algorithm selectors and have the potential to form the new state of the art in algorithm selection. © 2022, The Author(s).",Article,"Article in Press","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85128259652
"Dabbas H., Friedrich B.","57221633894;57146848800;","Benchmarking machine learning algorithms by inferring transportation modes from unlabeled GPS data",2022,"Transportation Research Procedia","62",,,"383","392",,,"10.1016/j.trpro.2022.02.048","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127504346&doi=10.1016%2fj.trpro.2022.02.048&partnerID=40&md5=3a923ae6bcab13e9aa5be8eb63feb1cc","Many traffic-related applications, e.g. traffic demand modeling, rely on conventional data collection methods such as travel surveys. These methods can be demanding in terms of cost and time, which results in low network coverage and limited representativeness. On-motion sensors, e.g. smartphones, offer the opportunity to replace such methods and compensate for the aforementioned drawbacks by collecting positioning data automatically. GPS data consist of positioning records each of which has geographical coordinates and a timestamp associated. These data, however, require cleansing and processing before being put into use. Information about the used transportation mode is missing from this kind of data unless travelers were specifically asked to report it. In the literature, supervised machine learning (ML) algorithms were successful in inferring transportation modes from GPS data. However, these algorithms, unlike unsupervised ML algorithms, require training data that are not always available. This paper aims to investigate the capability of unsupervised ML algorithms to infer transportation modes from real GPS data extracted from smartphones. Therefore, we used two datasets to benchmark different unsupervised ML algorithms with different input attributes. The paper also investigates the feasibility of using a pre-trained model for unlabeled real data. Finally, we compared the best performing unsupervised setup to the supervised ML algorithms recommended in the literature. The results suggest that the recommended unsupervised setups can reach an overall inferring accuracy of 93%. © 2022 Elsevier B.V.. All rights reserved.",Conference Paper,"Final","All Open Access, Gold",Scopus,2-s2.0-85127504346
"Liu P., Pan F., Zhou X., Li S., Jin L.","57555546900;7201487284;57210941499;56605879200;57217848930;","CF-DAML: Distributed automated machine learning based on collaborative filtering",2022,"Applied Intelligence",,,,"","",,,"10.1007/s10489-021-03049-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127409871&doi=10.1007%2fs10489-021-03049-z&partnerID=40&md5=22133190e4ccb9ebbf401b33764e3f4b","The search for a good machine learning (ML) model takes a long time and requires the considerations of many alternatives, including data preprocessing, algorithm selection, and hyperparameter tuning methods. Thus, tedious searches face a combinatorial explosion problem. In this work, we build a new automated machine learning (AutoML) system called CF-DAML, a distributed automated system based on collaborative filtering (CF), to address these challenges by recommending and training suitable models for supervised learning tasks. CF-DAML first computes some informative meta-features for a new dataset, then uses a weighted l1-norm (W1-norm) to accurately calculate the k nearest neighbors (kNN) of the new dataset, and finally recommends the top N models with good performances on each of its neighbors to the new dataset. We also design a distributed system (DSTM) for training the models to reduce the time complexity substantially. In addition, we develop a multilayer selective stacked ensemble system (MSSE), whose base models are selected from among suitable candidate models based on their runtimes, classification accuracies, and diversities, to enhance the stability of CF-DAML. To our knowledge, this is the first work to combine memory-based CF and the selective stacked ensemble to solve the AutoML problem. Extensive experiments are conducted on many UCI datasets and the comparative results demonstrate that our approach outperforms the current state-of-the-art methods. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Article in Press","",Scopus,2-s2.0-85127409871
"Ju X., Lu J., Luo X., Zhou G., Wang S., Li S., Yang Y.","57556577800;35731595000;8976166200;57222222786;57555823500;57556771100;57826106300;","Interest points analysis for internet forum based on long-short windows similarity",2022,"Computers, Materials and Continua","72","2",,"3247","3267",,,"10.32604/cmc.2022.026698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127307527&doi=10.32604%2fcmc.2022.026698&partnerID=40&md5=b93e96a68533c473b3f39f08aacbc0b2","For Internet forum Points of Interest (PoI), existing analysis methods are usually lack of usability analysis under different conditions and ignore the long-Term variation, which lead to blindness in method selection. To address this problem, this paper proposed a PoI variation prediction framework based on similarity analysis between long and short windows. Based on the framework, this paper presented 5 PoI analysis algorithms which can be categorized into 2 types, i.e., the traditional sequence analysis methods such as autoregressive integrated moving average model (ARIMA), support vector regressor (SVR), and the deep learning methods such as convolutional neural network (CNN), long-short term memory network (LSTM), Transformer (TRM). Specifically, this paper firstly divides observed data into long and short windows, and extracts key words as PoI of each window. Then, the PoI similarities between long and short windows are calculated for training and prediction. Finally, series of experiments is conducted based on real Internet forum datasets. The results show that, all the 5 algorithms could predict PoI variations well, which indicate effectiveness of the proposed framework.When the length of long window is small, traditional methods perform better, and SVR is the best. On the contrary, the deep learning methods show superiority, and LSTM performs best. The results could provide beneficial references for PoI variation analysis and prediction algorithms selection under different parameter configurations. © 2022 Tech Science Press. All rights reserved.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85127307527
"Tan Y., Chen J., Benson C.H.","57198777236;55881827900;57202681173;","Predicting Hydraulic Conductivity of Geosynthetic Clay Liners Using a Neural Network Algorithm",2022,"Geotechnical Special Publication","2022-March","GSP 335",,"21","28",,1,"10.1061/9780784484050.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126990232&doi=10.1061%2f9780784484050.003&partnerID=40&md5=2c1ba28c74c399a1da61031f72982806","Hydraulic conductivity tests on geosynthetic clay liners (GCLs) to evaluate chemical compatibility can require months to years to reach equilibrium. There is a need for alternative methods to screen GCLs for chemical compatibility that are more expedient. In this study, a neural network machine learning (ML) algorithm was used to predict the hydraulic conductivity of Na-Bentonite (NaB) GCLs to leachate chemistry. Development of the ML predictive model (MLPM) included five steps: data collection, data cleaning and normalization, algorithm selection, parameters optimization, and model validation and evaluation. The MLPM is based on data collected from two decades of tests conducted on NaB GCLs with a broad range of leachates. Bentonite characteristics, permeant chemistry, and stress conditions are incorporated into the MLPM. Validation showed that the MLPM predicts hydraulic conductivity within one order of magnitude of the measured hydraulic conductivity in 85% of the cases. © ASCE",Conference Paper,"Final","",Scopus,2-s2.0-85126990232
"Brazdil P., van Rijn J.N., Soares C., Vanschoren J.","56117723500;56347016200;13908004500;23394289300;","Introduction",2022,"Cognitive Technologies",,,,"3","17",,,"10.1007/978-3-030-67024-5_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126981296&doi=10.1007%2f978-3-030-67024-5_1&partnerID=40&md5=500560d5ac6299d9f5f202081aba125e","This chapter starts by describing the organization of the book, which consists of three parts. Part I discusses some basic concepts, including, for instance, what metalearning is and how it is related to automatic machine learning (AutoML). This continues with a presentation of the basic architecture of metalearning/AutoML systems, discussion of systems that exploit algorithm selection using prior metadata, methodology used in their evaluation, and different types of meta-level models, while mentioning the respective chapters where more details can be found. This part also includes discussion of methods used for hyperparameter optimization and workflow design. Part II includes the discussion of more advanced techniques and methods. The first chapter discusses the problem of setting up configuration spaces and conducting experiments. Subsequent chapters discuss different types of ensembles, metalearning in ensemble methods, algorithms used for data streams and transfer of meta-models across tasks. One chapter is dedicated to metalearning for deep neural networks. The last two chapters discuss the problem of automating various data science tasks and trying to design systems that are more complex. Part III is relatively short. It discusses repositories of metadata (including experimental results) and exemplifies what can be learned from this metadata by giving illustrative examples. The final chapter presents concluding remarks. © 2022, The Author(s).",Book Chapter,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85126981296
"Huijben I.A., Kool W., Paulus M.B., Van Sloun R.J.","57218405320;57210765351;57219754052;56052857600;","A Review of the Gumbel-max Trick and its Extensions for Discrete Stochasticity in Machine Learning",2022,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,,"10.1109/TPAMI.2022.3157042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126274595&doi=10.1109%2fTPAMI.2022.3157042&partnerID=40&md5=ad75734ff759cef9d3b6b19ee7577032","The Gumbel-max trick is a method to draw a sample from a categorical distribution, given by its unnormalized (log-)probabilities. Over the past years, the machine learning community has proposed several extensions of this trick to facilitate, e.g., drawing multiple samples, sampling from structured domains, or gradient estimation for error backpropagation in neural network optimization. The goal of this survey article is to present background about the Gumbel-max trick, and to provide a structured overview of its extensions to ease algorithm selection. Moreover, it presents a comprehensive outline of (machine learning) literature in which Gumbel-based algorithms have been leveraged, reviews commonly-made design choices, and sketches a future perspective. IEEE",Article,"Article in Press","All Open Access, Green",Scopus,2-s2.0-85126274595
"Lindauer M., Eggensperger K., Feurer M., Biedenkapp A., Deng D., Benjamins C., Ruhkopf T., Sass R., Hutter F.","56032621900;56397110000;57100170600;57195955636;57221840417;57275115300;57443610600;57275046900;55931808800;","SMAC3: A Versatile Bayesian Optimization Package for Hyperparameter Optimization",2022,"Journal of Machine Learning Research","23",,,"","",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124235402&partnerID=40&md5=658d38b5392529f0dfffcc185edc4958","Algorithm parameters, in particular hyperparameters of machine learning algorithms, can substantially impact their performance. To support users in determining well-performing hyperparameter configurations for their algorithms, datasets and applications at hand, SMAC3 offers a robust and flexible framework for Bayesian Optimization, which can improve performance within a few evaluations. It offers several facades and pre-sets for typical use cases, such as optimizing hyperparameters, solving low dimensional continuous (artificial) global optimization problems and configuring algorithms to perform well across multiple problem instances. The SMAC3 package is available under a permissive BSD-license at https://github.com/automl/SMAC3. © 2022 Marius Lindauer, Katharina Eggensperger, Matthias Feurer, André Biedenkapp, Carolin Benjamins, Difan Deng, René Sass and Frank Hutter.",Article,"Final","",Scopus,2-s2.0-85124235402
"Garouani M., Ahmad A., Bouneffa M., Hamlich M.","57226345482;57192646034;19337242800;55611586400;","AMLBID: An auto-explained Automated Machine Learning tool for Big Industrial Data",2022,"SoftwareX","17",,"100919","","",,5,"10.1016/j.softx.2021.100919","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121265920&doi=10.1016%2fj.softx.2021.100919&partnerID=40&md5=7f168872e6185c17a3079221d35a7d14","The Machine Learning(ML) based solutions in manufacturing industrial contexts often require skilled resources. More practical non-expert software solutions are then desired to enhance the usability of ML algorithms. The algorithm selection and configuration is one of the most difficult tasks for users like manufacturing specialists. The identification of the most appropriate algorithm in an automatic manner is among the major research challenges to achieve optimal performance of ML tools. In this paper, we present an auto-explained Automated Machine Learning tool for Big Industrial Data(AMLBID) to better cope with the prominent challenges posed by the evolution of Big Industrial Data. It is a meta-learning based decision support system for the automated selection and tuning of implied hyperparameters for ML algorithms. Moreover, the framework is equipped with an explainer module that makes the outcomes transparent and interpretable for well-performing ML systems. © 2021 The Authors",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85121265920
"Ruan X., Chen H.","57325920200;57326495400;","Network Governance Prediction Based on Artificial Intelligence and Algorithm Recommendation",2022,"Lecture Notes on Data Engineering and Communications Technologies","97",,,"273","279",,,"10.1007/978-3-030-89508-2_35","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118635926&doi=10.1007%2f978-3-030-89508-2_35&partnerID=40&md5=419ac2249274d67973d750a5e6ddcd2c","In recent years, China has entered the era of rapid development of network information, network communication mode is more diversified, professional and intelligent, leading to the governance of its communication order needs to face great challenges and development opportunities. As one of the ways of network communication, algorithmic recommendation can achieve accurate delivery to users and solve the problem of information flooding, but it also brings some problems. For example, some enterprises spread vulgar and harmful content in order to attract attention and search hot, which is a very severe challenge to the controllers of network transmission order. In this regard, this paper first introduces the algorithm recommendation, analyzes the problems faced by the network propagation of algorithm recommendation, and puts forward the corresponding countermeasures. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85118635926
"Huerta I.I., Neira D.A., Ortega D.A., Varas V., Godoy J., Asín-Achá R.","57217176963;57216312125;57217175411;57217175691;55567072800;25928259300;","Improving the state-of-the-art in the Traveling Salesman Problem: An Anytime Automatic Algorithm Selection",2022,"Expert Systems with Applications","187",,"115948","","",,2,"10.1016/j.eswa.2021.115948","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116552514&doi=10.1016%2fj.eswa.2021.115948&partnerID=40&md5=f3489041643fdeef69b2056271d755a8","This work presents a new metaheuristic for the euclidean Traveling Salesman Problem (TSP) based on an Anytime Automatic Algorithm Selection model using a portfolio of five state-of-the-art solvers. We introduce a new spatial representation of nodes, in the form of a matrix grid, avoiding costly calculation of features. Furthermore, we use a new compact staggered representation for the ranking of algorithms at each time step. Then, we feed inputs (matrix grid) and outputs (staggered representation) into a classifying convolutional neural network to predict the ranking of the solvers at a given time. We use the available datasets for TSP and generate new instances to augment their number, reaching 6,689 instances, distributed into training and test sets. Results show that the time required to predict the best solver is drastically reduced in comparison to previous traditional feature selection and machine learning methods. Furthermore, the prediction can be obtained at any time and, on average, the metasolver is better than running all the solvers separately on all the datasets, obtaining 79.8% accuracy. © 2021",Article,"Final","",Scopus,2-s2.0-85116552514
"Landivar G.E.V., Arambulo J.A.E., Martinez M.A.Q., Vazquez M.Y.L.","57205367027;57270371500;57221771864;55907610800;","Machine Learning Algorithm Selection for a Clinical Decision Support System Based on a Multicriteria Method",2022,"Lecture Notes in Networks and Systems","319",,,"1002","1010",,,"10.1007/978-3-030-85540-6_128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115658965&doi=10.1007%2f978-3-030-85540-6_128&partnerID=40&md5=67f784600d7607de63dd79c5246e4240","On the current information in the medical area related to cancer analysis, the selection of an optimal Machine Learning algorithm, based on a multicriteria method, for a system that supports clinical decisions is sought. As a methodology, exploratory research and the deductive method were applied to analyze the information from existing articles and ML algorithms' behavior applied in the area of medicine. This research and based on a use case of training and testing of the GLM, SVM, and ANN algorithms for selecting an algorithm. Addition-ally, for clinical decisions, and architecture prototype for medical data collection is presented resulted. Based on AHP and TOPSIS methods Support Vector Machine (SVM) is the best alternative. © 2022, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85115658965
"Li Y., Xue H.","57485584800;57486197300;","Construction of University Network Governance System Based on Artificial Intelligence and Network Algorithm Recommendation",2021,"ACM International Conference Proceeding Series",,,,"438","442",,,"10.1145/3510858.3510984","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126243753&doi=10.1145%2f3510858.3510984&partnerID=40&md5=0286ebe477703253ccfd08a1929299ff","With the introduction of the concept of computer education, the Internet began to pay attention to education and learning, and a large number of Internet learning webpages came into being, covering all levels of education and science. Driven by this wave of education, the school actively encourages teachers to develop various online courses that are not restricted by time and space, and promote the innovation of learning models. Through these online platforms, some colleges and universities have developed online courses for college students, hoping to stimulate learners' intrinsic motivation and cultivate students' personality through online courses and other teaching methods. However, online courses have constructed a complex engineering system, and there are currently some problems, such as the unclear layout of online courses, and similar professional content design and evaluation methods. How to build a university network governance system has become an important research direction for experts and scholars. This article introduces artificial intelligence, calculates the construction data of the university network governance system based on the fitness function of artificial intelligence and network algorithms, and constructs a model diagram of the university network governance system. It is hoped that this research can play a positive role in the governance of university networks. © 2021 ACM.",Conference Paper,"Final","",Scopus,2-s2.0-85126243753
"Nguyen T.-D., Musial K., Gabrys B.","57217776034;14626743000;7006540228;","AutoWeka4MCPS-AVATAR: Accelerating automated machine learning pipeline composition and optimisation",2021,"Expert Systems with Applications","185",,"115643","","",,1,"10.1016/j.eswa.2021.115643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111714806&doi=10.1016%2fj.eswa.2021.115643&partnerID=40&md5=b998ed57b42737bdb6fa302412fab7eb","Automated machine learning pipeline (ML) composition and optimisation aim at automating the process of finding the most promising ML pipelines within allocated resources (i.e., time, CPU and memory). Existing methods, such as Bayesian-based and genetic-based optimisation, which are implemented in Auto-Weka, Auto-sklearn and TPOT, evaluate pipelines by executing them. Therefore, the pipeline composition and optimisation of these methods frequently require a tremendous amount of time that prevents them from exploring complex pipelines to find better predictive models. To further explore this research challenge, we have conducted experiments showing that many of the generated pipelines are invalid in the first place, and attempting to execute them is a waste of time and resources. To address this issue, we propose a novel method to evaluate the validity of ML pipelines, without their execution, using a surrogate model (AVATAR). The AVATAR generates a knowledge base by automatically learning the capabilities and effects of ML algorithms on datasets’ characteristics. This knowledge base is used for a simplified mapping from an original ML pipeline to a surrogate model which is a Petri net based pipeline. Instead of executing the original ML pipeline to evaluate its validity, the AVATAR evaluates its surrogate model constructed by capabilities and effects of the ML pipeline components and input/output simplified mappings. Evaluating this surrogate model is less resource-intensive than the execution of the original pipeline. As a result, the AVATAR enables the pipeline composition and optimisation methods to evaluate more pipelines by quickly rejecting invalid pipelines. We integrate the AVATAR into the sequential model-based algorithm configuration (SMAC). Our experiments show that when SMAC employs AVATAR, it finds better solutions than on its own. This is down to the fact that the AVATAR can evaluate more pipelines within the same time budget and allocated resources. © 2021 Elsevier Ltd",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85111714806
"Parmezan A.R.S., Lee H.D., Spolaôr N., Wu F.C.","57189351806;35088365000;35088722200;57237785000;","Automatic recommendation of feature selection algorithms based on dataset characteristics",2021,"Expert Systems with Applications","185",,"115589","","",,5,"10.1016/j.eswa.2021.115589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110602572&doi=10.1016%2fj.eswa.2021.115589&partnerID=40&md5=97c56d67ec13ae4b9fe9fd914c41303c","Feature selection in real-world data mining problems is essential to make the learning task efficient and more accurate. Identifying the best feature selection algorithm, among the many available, is a complex activity that still relies heavily on human experts or some random trial-and-error procedure. Thus, the automated machine learning community has taken some steps towards the automation of this process. In this paper, we address the metalearning challenge of recommending feature selection algorithms by proposing a novel meta-feature engineering model. Our model considers a broad collection of meta-features that enable the study of the relationship between the dataset properties and the feature selection algorithm performance in terms of several criteria. We arrange the input meta-features into eight categories: (i) simple, (ii) statistical, (iii) information-theoretical, (iv) complexity, (v) landmarking, (vi) based on symbolic models, (vii) based on images, and (viii) based on complex networks (graphs). The target meta-features emerge from a multi-criteria performance measure, based on five individual performance indexes, that assesses feature selection methods grounded in information, distance, dependence, consistency, and precision measures. We evaluate our proposal using a recently developed framework that extracts the input meta-features from 213 benchmark datasets, and ranks the assessed feature selection algorithms, to fill in the target meta-features in meta-bases. This evaluation uses five state-of-the-art classification methods to induce recommendation models from meta-bases: C4.5, Random Forest, XGBoost, ANN, and SVM. The results showed that it is possible to reach an average accuracy of up to 90% applying our meta-feature engineering model. This work is the first to use an extensive empirical evaluation to provide a careful discussion of the strengths and limitations of more than 160 meta-features. These meta-features, while designed to aid the task of feature selection algorithm recommendation, can readily be employed in other metalearning scenarios. Therefore, we believe our findings are a valuable contribution to the fields of automated machine learning and data mining, as well as to the feature extraction and pattern recognition communities. © 2021 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85110602572
"Olier I., Orhobor O.I., Dash T., Davis A.M., Soldatova L.N., Vanschoren J., King R.D.","8914762100;57203851201;56464021900;55778145354;8942311800;23394289300;7404501046;","Transformational machine learning: Learning how to learn from many related scientific problems",2021,"Proceedings of the National Academy of Sciences of the United States of America","118","49","e2108013118","","",,2,"10.1073/pnas.2108013118","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120867317&doi=10.1073%2fpnas.2108013118&partnerID=40&md5=7393836691363b0c2f78fab9bd382017","Almost all machine learning (ML) is based on representing examples using intrinsic features. When there are multiple related ML problems (tasks), it is possible to transform these features into extrinsic features by first training ML models on other tasks and letting them each make predictions for each example of the new task, yielding a novel representation. We call this transformational ML (TML). TML is very closely related to, and synergistic with, transfer learning, multitask learning, and stacking. TML is applicable to improving any nonlinear ML method. We tested TML using the most important classes of nonlinear ML: random forests, gradient boosting machines, support vector machines, k-nearest neighbors, and neural networks. To ensure the generality and robustness of the evaluation, we utilized thousands of ML problems from three scientific domains: drug design, predicting gene expression, and ML algorithm selection. We found that TML significantly improved the predictive performance of all the ML methods in all the domains (4 to 50% average improvements) and that TML features generally outperformed intrinsic features. Use of TML also enhances scientific understanding through explainable ML. In drug design, we found that TML provided insight into drug target specificity, the relationships between drugs, and the relationships between target proteins. TML leads to an ecosystem-based approach to ML, where new tasks, examples, predictions, and so on synergistically interact to improve performance. To contribute to this ecosystem, all our data, code, and our ∼50,000 ML models have been fully annotated with metadata, linked, and openly published using Findability, Accessibility, Interoperability, and Reusability principles (∼100 Gbytes). © 2021 National Academy of Sciences. All rights reserved.",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85120867317
"Estiri H., Strasser Z.H., Murphy S.N.","56073265200;57219792597;7402778470;","Individualized prediction of COVID-19 adverse outcomes with MLHO",2021,"Scientific Reports","11","1","5322","","",,11,"10.1038/s41598-021-84781-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102186114&doi=10.1038%2fs41598-021-84781-x&partnerID=40&md5=99b4f33582384f7339e586c575879f7a","The COVID-19 pandemic has devastated the world with health and economic wreckage. Precise estimates of adverse outcomes from COVID-19 could have led to better allocation of healthcare resources and more efficient targeted preventive measures, including insight into prioritizing how to best distribute a vaccination. We developed MLHO (pronounced as melo), an end-to-end Machine Learning framework that leverages iterative feature and algorithm selection to predict Health Outcomes. MLHO implements iterative sequential representation mining, and feature and model selection, for predicting patient-level risk of hospitalization, ICU admission, need for mechanical ventilation, and death. It bases this prediction on data from patients’ past medical records (before their COVID-19 infection). MLHO’s architecture enables a parallel and outcome-oriented model calibration, in which different statistical learning algorithms and vectors of features are simultaneously tested to improve prediction of health outcomes. Using clinical and demographic data from a large cohort of over 13,000 COVID-19-positive patients, we modeled the four adverse outcomes utilizing about 600 features representing patients’ pre-COVID health records and demographics. The mean AUC ROC for mortality prediction was 0.91, while the prediction performance ranged between 0.80 and 0.81 for the ICU, hospitalization, and ventilation. We broadly describe the clusters of features that were utilized in modeling and their relative influence for predicting each outcome. Our results demonstrated that while demographic variables (namely age) are important predictors of adverse outcomes after a COVID-19 infection, the incorporation of the past clinical records are vital for a reliable prediction model. As the COVID-19 pandemic unfolds around the world, adaptable and interpretable machine learning frameworks (like MLHO) are crucial to improve our readiness for confronting the potential future waves of COVID-19, as well as other novel infectious diseases that may emerge. © 2021, The Author(s).",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85102186114
"Basgalupp M.P., Barros R.C., de Sá A.G.C., Pappa G.L., Mantovani R.G., de Carvalho A.C.P.L.F., Freitas A.A.","35247748900;35247802600;36991825900;8884637200;56902359300;35581825800;55490514300;","An extensive experimental evaluation of automated machine learning methods for recommending classification algorithms",2021,"Evolutionary Intelligence","14","4",,"1895","1914",,,"10.1007/s12065-020-00463-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089682130&doi=10.1007%2fs12065-020-00463-z&partnerID=40&md5=32280eb369c63525679bc913ba31d7a1","This paper presents an experimental comparison among four automated machine learning (AutoML) methods for recommending the best classification algorithm for a given input dataset. Three of these methods are based on evolutionary algorithms (EAs), and the other is Auto-WEKA, a well-known AutoML method based on the combined algorithm selection and hyper-parameter optimisation (CASH) approach. The EA-based methods build classification algorithms from a single machine learning paradigm: either decision-tree induction, rule induction, or Bayesian network classification. Auto-WEKA combines algorithm selection and hyper-parameter optimisation to recommend classification algorithms from multiple paradigms. We performed controlled experiments where these four AutoML methods were given the same runtime limit for different values of this limit. In general, the difference in predictive accuracy of the three best AutoML methods was not statistically significant. However, the EA evolving decision-tree induction algorithms has the advantage of producing algorithms that generate interpretable classification models and that are more scalable to large datasets, by comparison with many algorithms from other learning paradigms that can be recommended by Auto-WEKA. We also observed that Auto-WEKA has shown meta-overfitting, a form of overfitting at the meta-learning level, rather than at the base-learning level. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85089682130
"Kazakovtsev V., Muravyov S.","57205118603;57194035005;","Application of the automatic selection and configuration of clustering algorithms method for the Apache Spark framework",2021,"ACM International Conference Proceeding Series",,,,"","",,,"10.1145/3503047.3503104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123791298&doi=10.1145%2f3503047.3503104&partnerID=40&md5=fc5f2c351bcfa6c2b68fea2ee0dbff9a","This article proposes the MASSCAH method realization for Apache Spark clustering algorithms selection and configuration. Optimization of one of the clustering quality measures is used to configure the algorithm. In the course of this study, additional clustering quality measures were implemented that are not included in the Apache Spark framework, since at the moment only the silhouette criterion is available in the framework. © 2021 ACM.",Conference Paper,"Final","",Scopus,2-s2.0-85123791298
"Czako Z., Sebestyen G., Hangan A.","57203068304;24178273200;24465133000;","AutomaticAI – A hybrid approach for automatic artificial intelligence algorithm selection and hyperparameter tuning",2021,"Expert Systems with Applications","182",,"115225","","",,4,"10.1016/j.eswa.2021.115225","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108708321&doi=10.1016%2fj.eswa.2021.115225&partnerID=40&md5=6eb950163997998fb730e7ad16e26a8a","Recently, more and more real life problems are solved using artificial intelligence (AI) algorithms. One of the biggest challenges when working with AI is the selection and tuning of the best algorithm for solving the problem. The results generated by a given AI algorithm heavily depend on the way in which its hyperparameters are set. In most cases the process of algorithm selection and tuning is a manual, time consuming process in which the developer, based on experience and intuition tries to find the best solution from quality and execution time perspective. In this paper we present a method for solving the problem of AI algorithm selection and tuning, without human intervention, in a fully automated way. The method is a hybrid approach, a combination between particle swarm optimization and simulated annealing. We compare our approach with other similar tools like Auto-sklearn or Hyperopt-sklearn. We demonstrate the time efficiency and high accuracy of this method with some experiments on some known datasets. The paper also presents a platform for AI processing that include a set of procedures and services necessary in case of automatic processing of big datasets as well as the method for AI algorithm selection and tuning. This platform is useful for researchers and developers in an incipient phase of application development, when the best solution must be decided; it is also useful for specialists in different domains (physics, industry, economy) with less experience in using AI algorithms, but which has to process huge amount of data in an automated way. © 2021 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85108708321
"O’Shea R.J., Tsoka S., Cook G.J.R., Goh V.","57223089362;6603414724;8880415200;7004035095;","Sparse Regression in Cancer Genomics: Comparing Variable Selection and Predictions in Real World Data",2021,"Cancer Informatics","20",,,"","",,1,"10.1177/11769351211056298","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120473475&doi=10.1177%2f11769351211056298&partnerID=40&md5=d052d66724e2511e9eff8d3badd67331","Background: Evaluation of gene interaction models in cancer genomics is challenging, as the true distribution is uncertain. Previous analyses have benchmarked models using synthetic data or databases of experimentally verified interactions – approaches which are susceptible to misrepresentation and incompleteness, respectively. The objectives of this analysis are to (1) provide a real-world data-driven approach for comparing performance of genomic model inference algorithms, (2) compare the performance of LASSO, elastic net, best-subset selection, (Formula presented.) penalisation and (Formula presented.) penalisation in real genomic data and (3) compare algorithmic preselection according to performance in our benchmark datasets to algorithmic selection by internal cross-validation. Methods: Five large (Formula presented.) genomic datasets were extracted from Gene Expression Omnibus. ‘Gold-standard’ regression models were trained on subspaces of these datasets ((Formula presented.), (Formula presented.)). Penalised regression models were trained on small samples from these subspaces ((Formula presented.)) and validated against the gold-standard models. Variable selection performance and out-of-sample prediction were assessed. Penalty ‘preselection’ according to test performance in the other 4 datasets was compared to selection internal cross-validation error minimisation. Results: (Formula presented.) -penalisation achieved the highest cosine similarity between estimated coefficients and those of gold-standard models. (Formula presented.) -penalised models explained the greatest proportion of variance in test responses, though performance was unreliable in low signal:noise conditions. (Formula presented.) also attained the highest overall median variable selection F1 score. Penalty preselection significantly outperformed selection by internal cross-validation in each of 3 examined metrics. Conclusions: This analysis explores a novel approach for comparisons of model selection approaches in real genomic data from 5 cancers. Our benchmarking datasets have been made publicly available for use in future research. Our findings support the use of (Formula presented.) penalisation for structural selection and (Formula presented.) penalisation for coefficient recovery in genomic data. Evaluation of learning algorithms according to observed test performance in external genomic datasets yields valuable insights into actual test performance, providing a data-driven complement to internal cross-validation in genomic regression tasks. © The Author(s) 2021.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85120473475
"Ottervanger G., Baratchi M., Hoos H.H.","57226744426;38361070000;6701843335;","MultiETSC: automated machine learning for early time series classification",2021,"Data Mining and Knowledge Discovery","35","6",,"2602","2654",,,"10.1007/s10618-021-00781-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112581958&doi=10.1007%2fs10618-021-00781-5&partnerID=40&md5=4bcf940722287ae8c6fc37f3e9929204","Early time series classification (EarlyTSC) involves the prediction of a class label based on partial observation of a given time series. Most EarlyTSC algorithms consider the trade-off between accuracy and earliness as two competing objectives, using a single dedicated hyperparameter. To obtain insights into this trade-off requires finding a set of non-dominated (Pareto efficient) classifiers. So far, this has been approached through manual hyperparameter tuning. Since the trade-off hyperparameters only provide indirect control over the earliness-accuracy trade-off, manual tuning is tedious and tends to result in many sub-optimal hyperparameter settings. This complicates the search for optimal hyperparameter settings and forms a hurdle for the application of EarlyTSC to real-world problems. To address these issues, we propose an automated approach to hyperparameter tuning and algorithm selection for EarlyTSC, building on developments in the fast-moving research area known as automated machine learning (AutoML). To deal with the challenging task of optimising two conflicting objectives in early time series classification, we propose MultiETSC, a system for multi-objective algorithm selection and hyperparameter optimisation (MO-CASH) for EarlyTSC. MultiETSC can potentially leverage any existing or future EarlyTSC algorithm and produces a set of Pareto optimal algorithm configurations from which a user can choose a posteriori. As an additional benefit, our proposed framework can incorporate and leverage time-series classification algorithms not originally designed for EarlyTSC for improving performance on EarlyTSC; we demonstrate this property using a newly defined, “naïve” fixed-time algorithm. In an extensive empirical evaluation of our new approach on a benchmark of 115 data sets, we show that MultiETSC performs substantially better than baseline methods, ranking highest (avg. rank 1.98) compared to conceptually simpler single-algorithm (2.98) and single-objective alternatives (4.36). © 2021, The Author(s).",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85112581958
"Monteiro J.P., Ramos D., Carneiro D., Duarte F., Fernandes J.M., Novais P.","57224997560;57211981093;24723727400;57221245820;7201540270;8248071100;","Meta-learning and the new challenges of machine learning",2021,"International Journal of Intelligent Systems","36","11",,"6240","6272",,7,"10.1002/int.22549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108896033&doi=10.1002%2fint.22549&partnerID=40&md5=520717087778063a0e85fb09304b180c","In the last years, organizations and companies in general have found the true potential value of collecting and using data for supporting decision-making. As a consequence, data are being collected at an unprecedented rate. This poses several challenges, including, for example, regarding the storage and processing of these data. Machine Learning (ML) is also not an exception, in the sense that algorithms must now deal with novel challenges, such as learn from streaming data or deal with concept drift. ML engineers also have a harder task when it comes to selecting the most appropriate model, given the wealth of algorithms and possible configurations that exist nowadays. At the same time, training time is a stronger restriction as the computational complexity of the training model increases. In this paper we propose a framework for dealing with these challenges, based on meta-learning. Specifically, we tackle two well-defined problems: automatic algorithm selection and continuous algorithm updates that do not require the retraining of the whole algorithm to adapt to new data. Results show that the proposed framework can contribute to ameliorate the identified issues. © 2021 Wiley Periodicals LLC",Article,"Final","",Scopus,2-s2.0-85108896033
"De Araújo Costa I.P., Basílio M.P., Do Nascimento Maêda S.M., Rodrigues M.V.G., Moreira M.A.L., Gomes C.F.S., Dos Santos M.","57360012700;57360158700;57218706465;57226028106;57220186881;57395579100;57169600900;","Algorithm selection for machine learning classification: An application of the MELCHIOR multicriteria method",2021,"Frontiers in Artificial Intelligence and Applications","341",,,"154","161",,1,"10.3233/FAIA210243","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120491570&doi=10.3233%2fFAIA210243&partnerID=40&md5=a34f09496b16c4a21ad3a30ab5273fc4","This paper aims to select an algorithm for the Machine Learning (ML) classification task. For the proposed analysis, the Multi-criteria Decision Aid (MCDA) Méthode d'ELimination et de CHoix Includent les relations d'ORdre (MELCHIOR) method was applied. The experiment considered the following criteria as relevant: Accuracy, sensitivity, and processing time of the algorithms. The data used refers to the intention of buying on the Internet and the purpose is to predict whether the customer will finalize a particular purchase. Among various MCDA techniques available, MELCHIOR was chosen to support the decision-making process because this method provides the evaluation of alternatives without the need to elicit the weights of the criteria. As a result, the Gradient Boosting Decision Tree algorithm has been selected as the most suitable for the ML classification task. © 2021 The authors and IOS Press.",Conference Paper,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85120491570
"Huang R., Ma C., Ma J., Huangfu X., He Q.","57204058349;57191893991;56873526100;55857098300;35227415600;","Machine learning in natural and engineered water systems",2021,"Water Research","205",,"117666","","",,22,"10.1016/j.watres.2021.117666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115190352&doi=10.1016%2fj.watres.2021.117666&partnerID=40&md5=325f32d08e0052c42ac31c87f14ddc2a","Water resources of desired quality and quantity are the foundation for human survival and sustainable development. To better protect the water environment and conserve water resources, efficient water management, purification, and transportation are of critical importance. In recent years, machine learning (ML) has exhibited its practicability, reliability, and high efficiency in numerous applications; furthermore, it has solved conventional and emerging problems in both natural and engineered water systems. For example, ML can predict various water quality indicators in situ and real-time by considering the complex interactions among water-related variables. ML approaches can also solve emerging pollution problems with proven rules or universal mechanisms summarized from the related research. Moreover, by applying image recognition technology to analyze the relationships between image information and physicochemical properties of the research object, ML can effectively identify and characterize specific contaminants. In view of the bright prospects of ML, this review comprehensively summarizes the development of ML applications in natural and engineered water systems. First, the concept and modeling steps of ML are briefly introduced, including data preparation, algorithm selection and model evaluation. In addition, comprehensive applications of ML in recent studies, including predicting water quality, mapping groundwater contaminants, classifying water resources, tracing contaminant sources, and evaluating pollutant toxicity in natural water systems, as well as modeling treatment techniques, assisting characterization analysis, purifying and distributing drinking water, and collecting and treating sewage water in engineered water systems, are summarized. Finally, the advantages and disadvantages of commonly used algorithms are analyzed according to their structures and mechanisms, and recommendations on the selection of ML algorithms for different studies, as well as prospects on the application and development of ML in water science are proposed. This review provides references for solving a wider range of water-related problems and brings further insights into the intelligent development of water science. © 2021",Review,"Final","",Scopus,2-s2.0-85115190352
"Dias L.V., Miranda P.B.C., Nascimento A.C.A., Cordeiro F.R., Mello R.F., Prudêncio R.B.C.","57218863413;25634399300;23668677400;15020336600;56405263500;6602721735;","ImageDataset2Vec: An image dataset embedding for algorithm selection",2021,"Expert Systems with Applications","180",,"115053","","",,1,"10.1016/j.eswa.2021.115053","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105492498&doi=10.1016%2fj.eswa.2021.115053&partnerID=40&md5=116c5b421eb8c82799a8b2a7c8342cf7","Convolutional Neural Networks (CNNs) have become the main solution for image classification tasks in different applications. Although several CNN architectures are available, there is no best architecture regardless the problem at hand. The selection of the most suitable CNN architecture is usually performed by trial and error, which may take much time and computational resources. Meta-learning (MtL) is a framework developed in machine learning to perform algorithm selection based on the meta-features of each task being solved. Such meta-features are usually descriptive characteristics extracted from the training dataset available in the task at hand. Despite the increasing attention of MtL for algorithm selection, its success strongly depends on defining relevant meta-features to represent the classification tasks of interest. This paper proposes the ImageDataset2Vec method for extracting meta-features to describe image classification datasets. ImageDataset2Vec adopts a pre-trained deep neural network to extract features from images datasets, embedding them in a single feature vector. The derived meta-features are employed by MtL to select CNN architectures for image classification. The proposed approach was evaluated for selecting among six CNN algorithms in 45 two-classes image datasets. The results showed that MtL using ImageDataset2Vec overcame different baseline methods, selecting the best possible CNN algorithm in 84.45% of the datasets. Furthermore, the proposal was statistically equivalent to the ground truth when the best CNN is recommended, i.e., when MtL does not select the best CNN, it selects a competitive algorithm. These results show that the proposal was able to extract representative features from image datasets. © 2021 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85105492498
"Kowsher M., Hossen I., Tahabilder A., Prottasha N.J., Habib K., Azmi Z.R.M.","57215324267;57226829000;57200150589;57216271505;57474601500;36094081400;","Support directional shifting vector: A direction based machine learning classifier",2021,"Emerging Science Journal","5","5",,"700","713",,5,"10.28991/esj-2021-01306","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85118101234&doi=10.28991%2fesj-2021-01306&partnerID=40&md5=7ec031a6ed19e73e1dda7f4a57554fbc","Machine learning models have been very popular nowadays for providing rigorous solutions to complicated real-life problems. There are three main domains named supervised, unsupervised, and reinforcement. Supervised learning mainly deals with regression and classification. There exist several types of classification algorithms, and these are based on various bases. The classification performance varies based on the dataset velocity and the algorithm selection. In this article, we have focused on developing a model of angular nature that performs supervised classification. Here, we have used two shifting vectors named Support Direction Vector (SDV) and Support Origin Vector (SOV) to form a linear function. These vectors form a linear function to measure cosine-angle with both the target class data and the non-target class data. Considering target data points, the linear function takes such a position that minimizes its angle with target class data and maximizes its angle with non-target class data. The positional error of the linear function has been modelled as a loss function which is iteratively optimized using the gradient descent algorithm. In order to justify the acceptability of this method, we have implemented this model on three different standard datasets. The model showed comparable accuracy with the existing standard supervised classification algorithm. © 2021 by the authors. Licensee ESJ, Italy.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85118101234
"Carvalho V.R., Özcan E., Sichman J.S.","57201902341;7004188970;6602534285;","Comparative analysis of selection hyper-heuristics for real-world multi-objective optimization problems",2021,"Applied Sciences (Switzerland)","11","19","9153","","",,1,"10.3390/app11199153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116311851&doi=10.3390%2fapp11199153&partnerID=40&md5=93724cc38ae3c17fc853b1d8545b6fd3","As exact algorithms are unfeasible to solve real optimization problems, due to their computational complexity, meta-heuristics are usually used to solve them. However, choosing a meta-heuristic to solve a particular optimization problem is a non-trivial task, and often requires a time-consuming trial and error process. Hyper-heuristics, which are heuristics to choose heuristics, have been proposed as a means to both simplify and improve algorithm selection or configuration for optimization problems. This paper novel presents a novel cross-domain evaluation for multi-objective optimization: we investigate how four state-of-the-art online hyper-heuristics with different characteristics perform in order to find solutions for eighteen real-world multi-objective optimization problems. These hyperheuristics were designed in previous studies and tackle the algorithm selection problem from different perspectives: Election-Based, based on Reinforcement Learning and based on a mathematical function. All studied hyper-heuristics control a set of five Multi-Objective Evolutionary Algorithms (MOEAs) as Low-Level (meta-)Heuristics (LLHs) while finding solutions for the optimization problem. To our knowledge, this work is the first to deal conjointly with the following issues: (i) selection of meta-heuristics instead of simple operators (ii) focus on multi-objective optimization problems, (iii) experiments on real world problems and not just function benchmarks. In our experiments, we computed, for each algorithm execution, Hypervolume and IGD+ and compared the results considering the Kruskal–Wallis statistical test. Furthermore, we ranked all the tested algorithms considering three different Friedman Rankings to summarize the cross-domain analysis. Our results showed that hyper-heuristics have a better cross-domain performance than single meta-heuristics, which makes them excellent candidates for solving new multi-objective optimization problems. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85116311851
"Pinto-Bernal M.J., Cifuentes C.A., Perdomo O., Rincón-Roncancio M., Múnera M.","57215347962;36717243800;56694822100;57031517200;6505875786;","A data-driven approach to physical fatigue management using wearable sensors to classify four diagnostic fatigue states",2021,"Sensors","21","19","6401","","",,2,"10.3390/s21196401","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115659888&doi=10.3390%2fs21196401&partnerID=40&md5=9263f82e0909d8ea0fd7f340bf1cf8bc","Physical exercise contributes to the success of rehabilitation programs and rehabilitation processes assisted through social robots. However, the amount and intensity of exercise needed to obtain positive results are unknown. Several considerations must be kept in mind for its imple-mentation in rehabilitation, as monitoring of patients’ intensity, which is essential to avoid extreme fatigue conditions, may cause physical and physiological complications. The use of machine learning models has been implemented in fatigue management, but is limited in practice due to the lack of understanding of how an individual’s performance deteriorates with fatigue; this can vary based on physical exercise, environment, and the individual’s characteristics. As a first step, this paper lays the foundation for a data analytic approach to managing fatigue in walking tasks. The proposed framework establishes the criteria for a feature and machine learning algorithm selection for fatigue management, classifying four fatigue diagnoses states. Based on the proposed framework and the classifier implemented, the random forest model presented the best performance with an average accuracy of ≥98% and F-score of ≥93%. This model was comprised of ≤16 features. In addition, the prediction performance was analyzed by limiting the sensors used from four IMUs to two or even one IMU with an overall performance of ≥88%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85115659888
"Cohen Shapira N., Rokach L.","57211939999;9276243500;","Automatic selection of clustering algorithms using supervised graph embedding",2021,"Information Sciences","577",,,"824","851",,2,"10.1016/j.ins.2021.08.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113395368&doi=10.1016%2fj.ins.2021.08.028&partnerID=40&md5=1821eb7223145bfd769ecf3d720d0412","The widespread adoption of machine learning (ML) techniques and the extensive expertise required to apply them have led to increased interest in automated ML solutions that reduce the need for human intervention. One of the main challenges in applying ML to previously unseen problems is algorithm selection – the identification of high-performing algorithm(s) for a given dataset, task, and evaluation measure. This study addresses the algorithm selection challenge for data clustering, a fundamental task in data mining that is aimed at grouping similar objects. We present MARCO-GE, a novel meta-learning approach for the automated recommendation of clustering algorithms. MARCO-GE first transforms datasets into graphs and then utilizes a graph convolutional neural network technique to extract their latent representation. Using the embedding representations obtained, MARCO-GE trains a ranking meta-model capable of accurately recommending top-performing algorithms for a new dataset and clustering evaluation measure. An extensive evaluation on 210 datasets, 17 clustering algorithms, and 10 clustering measures demonstrates the effectiveness of our approach and its superiority in terms of predictive and generalization performance over state-of-the-art clustering meta-learning approaches. © 2021 Elsevier Inc.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85113395368
"Peres L.M., Barbon Junior S., Lopes J.F., Fuzyi E.M., Barbon A.P.A.C., Armangue J.G., Bridi A.M.","55753370700;57212560486;57208920036;57190302841;57147404100;57226532677;6603288790;","Meta-recommendation of pork technological quality standards",2021,"Biosystems Engineering","210",,,"13","19",,,"10.1016/j.biosystemseng.2021.07.012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111807828&doi=10.1016%2fj.biosystemseng.2021.07.012&partnerID=40&md5=fcdeb526a66b2cfd4fc8f30a96228e82","Pork quality classification is supported by different reference standards that are widely reported in the literature. However, selecting the most suitable standard for each type of meat samples remains a challenge, due to their intrinsic variation according to the quality parameters’ interval. The usage of meta-learning was proposed to automatically recommend the most adequate standard for a determined sample collection, leading to a more accurate classification. The meta-learning procedure has emerged from the machine learning research field to solve the algorithm selection dilemma, outlining a new method for pork quality classification. The applicability and advantages of using a suitable classification standard for pork quality were addressed using the J48 Decision Tree (DT) algorithm, which serves as the meta-recommender. Experiments conducted with six pork standards revealed promising results based on a few meta-attributes (L∗, water hold capacity, and dataset entropy) as the approach successfully recommended all scenarios. © 2021 IAgrE",Article,"Final","",Scopus,2-s2.0-85111807828
"Abdollah V., Parent E.C., Dolatabadi S., Marr E., Croutze R., Wachowicz K., Kawchuk G.","57197828849;7003962012;57224058672;57224058757;55959831000;15836022200;6701754578;","Texture analysis in the classification of T2-weighted magnetic resonance images in persons with and without low back pain",2021,"Journal of Orthopaedic Research","39","10",,"2187","2196",,5,"10.1002/jor.24930","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106837355&doi=10.1002%2fjor.24930&partnerID=40&md5=5bac6bc5e65a372340b8553ad2aa910f","Magnetic resonance imaging findings often do not distinguish between people with and without low back pain (LBP). However, there are still a large number of people who undergo magnetic resonance imaging to help determine the etiology of their back pain. Texture analysis shows promise for the classification of tissues that look similar, and machine learning can minimize the number of comparisons. This study aimed to determine if texture features from lumbar spine magnetic resonance imaging differ between people with and without LBP. In total, 14 participants with chronic LBP were matched for age, weight, and gender with 14 healthy volunteers. A custom texture analysis software was used to construct a gray-level co-occurrence matrix with one to four pixels offset in 0° direction for the disc and superior and inferior endplate regions. The Random Forests Algorithm was used to select the most promising classifiers. The linear mixed-effect model analysis was used to compare groups (pain vs. pain-free) at each level controlling for age. The Random Forest Algorithm recommended focusing on intervertebral discs and endplate zones at L4-5 and L5-S1. Differences were observed between groups for L5-S1 superior endplate contrast, homogeneity, and energy (p =.02). Differences were observed for L5-S1 disc contrast and homogeneity (p <.01), as well as for the inferior endplates contrast, homogeneity, and energy (p <.03). Magnetic resonance imaging textural features may have potential in identifying structures that may be the target of further investigations about the reasons for LBP. © 2020 Orthopaedic Research Society. Published by Wiley Periodicals LLC",Article,"Final","",Scopus,2-s2.0-85106837355
"Sachan R., Iqbal S.","57290248200;57205443835;","Application of Machine Learning Technique for Development of Indirect Tire Pressure Monitoring System",2021,"SAE Technical Papers",,"2021",,"","",,1,"10.4271/2021-26-0016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116844596&doi=10.4271%2f2021-26-0016&partnerID=40&md5=ac5b077c3c8f7485b2d95115a1c906d6","Tire inflation pressure has a significant impact over vehicle driving dynamics, fuel consumption as well as tire life. Therefore, continuous monitoring of tire pressure becomes imperative for ride comfort, safety and optimum vehicle handling performance. Two types of tire pressure monitoring systems (TPMS) used by vehicles are - direct and indirect TPMS. Direct systems deploy pressure sensors at each wheel and directly send pressure value to the vehicle Controller Area Network (CAN). Indirect sensors on the other hand use the information from already existing sensors and some physics-based equations to predict the value of tire pressure. Direct TPMS tend to be more accurate but have higher cost of installation while indirect TPMS comes with a minimum cost but compromised accuracy. A digital proof-of-concept study for indirect TPMS development of a non-ESP vehicle based on machine learning (ML) technique is elaborated in this paper. The study aims to propose a methodology for development of an indirect TPMS having an accuracy equivalent to that of a direct TPMS. A full vehicle model designed in Amesim software is used to extract data to train the machine-learning algorithm for different test cases. Simulation model is validated against the test data of vehicle dynamics parameters to ensure the accuracy of data extracted for ML model training. Multilayered feed forward, back-propagation artificial neural network is trained using three prediction algorithms and sensitivity of different algorithms, network parameters is analyzed against selected driving scenarios. Proof-of-concept study suggests that the proposed tire pressure prediction algorithm has a potential to predict tire pressure accurately at par with Direct TPMS. It lays a foundation for developing ML based indirect TPMS using physical testing data by providing assistance in test plan preparation, exploring data pre-processing techniques and algorithm selection. Furthermore, the generic methodology mentioned in this paper can be referred for initial development of any ML based project. © 2021 SAE International. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85116844596
"Kuk E., Stopa J., Kuk M., Janiga D., Wojnarowski P.","57191738774;6701583271;57218769050;56416451900;12244215900;","Petroleum reservoir control optimization with the use of the auto-adaptive decision trees",2021,"Energies","14","18","5702","","",,3,"10.3390/en14185702","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114911283&doi=10.3390%2fen14185702&partnerID=40&md5=cf46aac6fa8887b85e3b3228fbb309cf","The global increase in energy demand and the decreasing number of newly discovered hydrocarbon reservoirs caused by the relatively low oil price means that it is crucial to exploit existing reservoirs as efficiently as possible. Optimization of the reservoir control may increase the technical and economic efficiency of the production. In this paper, a novel algorithm that automatically determines the intelligent control maximizing the NPV of a given production process was developed. The idea is to build an auto-adaptive parameterized decision tree that replaces the arbitrarily selected limit values for the selected attributes of the decision tree with parameters. To select the optimal values of the decision tree parameters, an AI-based optimization tool called SMAC (Sequential Model-based Algorithm Configuration) was used. In each iteration, the generated control sequence is introduced into the reservoir simulator to compute the NVP, which is then utilized by the SMAC tool to vary the limit values to generate a better control sequence, which leads to an improved NPV. A new tool connecting the parameterized decision tree with the reservoir simulator and the optimization tool was developed. Its application on a simulation model of a real reservoir for which the CCS-EOR process was considered allowed oil production to be increased by 3.5% during the CO2-EOR phase, reducing the amount of carbon dioxide injected at that time by 16%. Hence, the created tool allowed revenue to be increased by 49%. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85114911283
"Abo M.E.M., Idris N., Mahmud R., Qazi A., Hashem I.A.T., Maitama J.Z., Naseem U., Khan S.K., Yang S.","57204854210;13608179100;36666779200;55976046400;57225107550;56565623800;57212385431;57212381354;36197636300;","A multi-criteria approach for arabic dialect sentiment analysis for online reviews: Exploiting optimal machine learning algorithm selection",2021,"Sustainability (Switzerland)","13","18","10018","","",,9,"10.3390/su131810018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114745154&doi=10.3390%2fsu131810018&partnerID=40&md5=fbe443b4e4fa07f937ef510a55f91257","A sentiment analysis of Arabic texts is an important task in many commercial applications such as Twitter. This study introduces a multi-criteria method to empirically assess and rank classifiers for Arabic sentiment analysis. Prominent machine learning algorithms were deployed to build classification models for Arabic sentiment analysis classifiers. Moreover, an assessment of the top five machine learning classifiers’ performances measures was discussed to rank the performance of the classifier. We integrated the top five ranking methods with evaluation metrics of machine learning classifiers such as accuracy, recall, precision, F-measure, CPU Time, classification error, and area under the curve (AUC). The method was tested using Saudi Arabic product reviews to compare five popular classifiers. Our results suggest that deep learning and support vector machine (SVM) classifiers perform best with accuracy 85.25%, 82.30%; precision 85.30, 83.87%; recall 88.41%, 83.89; F-measure 86.81, 83.87%; classification error 14.75, 17.70; and AUC 0.93, 0.90, respectively. They outperform decision trees, K-nearest neighbours (K-NN), and Naïve Bayes classifiers. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85114745154
"Solórzano J.V., Mas J.F., Gao Y., Gallardo-Cruz J.A.","57191527096;55993660100;55731329000;26029823700;","Land use land cover classification with U-net: Advantages of combining sentinel-1 and sentinel-2 imagery",2021,"Remote Sensing","13","18","3600","","",,10,"10.3390/rs13183600","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114688043&doi=10.3390%2frs13183600&partnerID=40&md5=aaba8e2361ff1e3f8023976390b7dade","The U-net is nowadays among the most popular deep learning algorithms for land use/land cover (LULC) mapping; nevertheless, it has rarely been used with synthetic aperture radar (SAR) and multispectral (MS) imagery. On the other hand, the discrimination between plantations and forests in LULC maps has been emphasized, especially for tropical areas, due to their differences in biodiversity and ecosystem services provision. In this study, we trained a U-net using different imagery inputs from Sentinel-1 and Sentinel-2 satellites, MS, SAR and a combination of both (MS + SAR); while a random forests algorithm (RF) with the MS + SAR input was also trained to evaluate the difference in algorithm selection. The classification system included ten classes, including old-growth and secondary forests, as well as old-growth and young plantations. The most accurate results were obtained with the MS + SAR U-net, where the highest overall accuracy (0.76) and average F1-score (0.58) were achieved. Although MS + SAR and MS U-nets gave similar results for almost all of the classes, for old-growth plantations and secondary forest, the addition of the SAR band caused an F1-score increment of 0.08–0.11 (0.62 vs. 0.54 and 0.45 vs. 0.34, respectively). Consecutively, in comparison with the MS + SAR RF, the MS + SAR U-net obtained higher F1-scores for almost all the classes. Our results show that using the U-net with a combined input of SAR and MS images enabled a higher F1-score and accuracy for a detailed LULC map, in comparison with other evaluated methods. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85114688043
"Li Q., Liu Y., Zhu J., Chen Z., Liu L., Yang S., Zhu G., Zhu B., Li J., Jin R., Tao J., Chen L.","26421264000;57192559940;57194447040;57206240871;57248142700;8505735800;57248568800;57247923600;55844174900;27167555200;49664030200;12762158100;","Upper-limb motion recognition based on hybrid feature selection: Algorithm development and validation",2021,"JMIR mHealth and uHealth","9","9","e24402","","",,1,"10.2196/24402","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114314331&doi=10.2196%2f24402&partnerID=40&md5=1aa74fafa5dd2793fece307e13f7abd9","Background: For rehabilitation training systems, it is essential to automatically record and recognize exercises, especially when more than one type of exercise is performed without a predefined sequence. Most motion recognition methods are based on feature engineering and machine learning algorithms. Time-domain and frequency-domain features are extracted from original time series data collected by sensor nodes. For high-dimensional data, feature selection plays an important role in improving the performance of motion recognition. Existing feature selection methods can be categorized into filter and wrapper methods. Wrapper methods usually achieve better performance than filter methods; however, in most cases, they are computationally intensive, and the feature subset obtained is usually optimized only for the specific learning algorithm. Objective: This study aimed to provide a feature selection method for motion recognition of upper-limb exercises and improve the recognition performance. Methods: Motion data from 5 types of upper-limb exercises performed by 21 participants were collected by a customized inertial measurement unit (IMU) node. A total of 60 time-domain and frequency-domain features were extracted from the original sensor data. A hybrid feature selection method by combining filter and wrapper methods (FESCOM) was proposed to eliminate irrelevant features for motion recognition of upper-limb exercises. In the filter stage, candidate features were first selected from the original feature set according to the significance for motion recognition. In the wrapper stage, k-nearest neighbors (kNN), Naïve Bayes (NB), and random forest (RF) were evaluated as the wrapping components to further refine the features from the candidate feature set. The performance of the proposed FESCOM method was verified using experiments on motion recognition of upper-limb exercises and compared with the traditional wrapper method. Results: Using kNN, NB, and RF as the wrapping components, the classification error rates of the proposed FESCOM method were 1.7%, 8.9%, and 7.4%, respectively, and the feature selection time in each iteration was 13 seconds, 71 seconds, and 541 seconds, respectively. Conclusions: The experimental results demonstrated that, in the case of 5 motion types performed by 21 healthy participants, the proposed FESCOM method using kNN and NB as the wrapping components achieved better recognition performance than the traditional wrapper method. The FESCOM method dramatically reduces the search time in the feature selection process. The results also demonstrated that the optimal number of features depends on the classifier. This approach serves to improve feature selection and classification algorithm selection for upper-limb motion recognition based on wearable sensor data, which can be extended to motion recognition of more motion types and participants. © 2021 JMIR Publications. All rights reserved.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85114314331
"Li K.-Y., Burnside N.G., de Lima R.S., Peciña M.V., Sepp K., Cabral Pinheiro V.H., de Lima B.R.C.A., Yang M.-D., Vain A., Sepp K.","57224108850;6507784867;57193433684;57190759667;6603222112;57226794359;57226792211;7404925923;35079498700;6603222112;","An automated machine learning framework in unmanned aircraft systems: New insights into agricultural management practices recognition approaches",2021,"Remote Sensing","13","16","3190","","",,3,"10.3390/rs13163190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112704361&doi=10.3390%2frs13163190&partnerID=40&md5=86a3c5769d114531af362a89f5fbc0bc","The recent trend of automated machine learning (AutoML) has been driving further significant technological innovation in the application of artificial intelligence from its automated algorithm selection and hyperparameter optimization of the deployable pipeline model for unrav-eling substance problems. However, a current knowledge gap lies in the integration of AutoML technology and unmanned aircraft systems (UAS) within image-based data classification tasks. Therefore, we employed a state-of-the-art (SOTA) and completely open-source AutoML framework, Auto-sklearn, which was constructed based on one of the most widely used ML systems: Scikit-learn. It was combined with two novel AutoML visualization tools to focus particularly on the recognition and adoption of UAS-derived multispectral vegetation indices (VI) data across a diverse range of agricultural management practices (AMP). These include soil tillage methods (STM), cultivation methods (CM), and manure application (MA), and are under the four-crop combination fields (i.e., red clover-grass mixture, spring wheat, pea-oat mixture, and spring barley). Furthermore, they have currently not been efficiently examined and accessible parameters in UAS applications are absent for them. We conducted the comparison of AutoML performance using three other common machine learning classifiers, namely Random Forest (RF), support vector machine (SVM), and artificial neural network (ANN). The results showed AutoML achieved the highest overall classification accuracy numbers after 1200 s of calculation. RF yielded the second-best classification accuracy, and SVM and ANN were revealed to be less capable among some of the given datasets. Regarding the classification of AMPs, the best recognized period for data capture occurred in the crop vegetative growth stage (in May). The results demonstrated that CM yielded the best performance in terms of classification, followed by MA and STM. Our framework presents new insights into plant–environment interactions with capable classification capabilities. It further illustrated the automatic system would become an important tool in furthering the understanding for future sustainable smart farming and field-based crop phenotyping research across a diverse range of agricultural environmental assessment and management applications. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85112704361
"Roy B.","57226074098;","Optimum machine learning algorithm selection for forecasting vegetation indices: MODIS NDVI & EVI",2021,"Remote Sensing Applications: Society and Environment","23",,"100582","","",,8,"10.1016/j.rsase.2021.100582","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110335545&doi=10.1016%2fj.rsase.2021.100582&partnerID=40&md5=8e8e44f65dcbf8ceeb71ebadf4020b13","Vegetation as a key environmental indicator both influences and is influenced by other factors. Normalized Difference Vegetation Index (NDVI) and Enhanced Vegetation Index (EVI) are quantitative measurements of vegetation that follow a monthly trend. Machine learning algorithms are now widely used to forecast several environmental indicators, including vegetation indices. In this study, Moderate Resolution Imaging Spectroradiometer (MODIS) dataset MOD13A2.006 (2001–2018) was used to extract NDVI and EVI values. These quantitative values were used to forecast vegetation indices for 2019 and scored on their performance. Four supervised machine learning algorithms: Support Vector Regression, Random Forest, Linear & Polynomial Regression were tested. The models forecasted data with 5.73%–1.51% error in NDVI and 6.99%–4.33% error in EVI. However, sudden loss and sudden gain in NDVI or EVI midyear could not be forecasted with these four algorithms in some cases. There was an observed upwards linear trend in the data suggesting increasing vegetation cover. © 2021 Elsevier B.V.",Article,"Final","",Scopus,2-s2.0-85110335545
"Strohschein J., Fischbach A., Bunte A., Faeskorn-Woyke H., Moriz N., Bartz-Beielstein T.","57211503062;56315875200;6507585753;56505486000;49964082800;57190702501;","Cognitive capabilities for the CAAI in cyber-physical production systems",2021,"International Journal of Advanced Manufacturing Technology","115","11-12",,"3513","3532",,,"10.1007/s00170-021-07248-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107768777&doi=10.1007%2fs00170-021-07248-3&partnerID=40&md5=c88b126249dd5a182126e8409203f3a6","This paper presents the cognitive module of the Cognitive Architecture for Artificial Intelligence (CAAI) in cyber-physical production systems (CPPS). The goal of this architecture is to reduce the implementation effort of artificial intelligence (AI) algorithms in CPPS. Declarative user goals and the provided algorithm-knowledge base allow the dynamic pipeline orchestration and configuration. A big data platform (BDP) instantiates the pipelines and monitors the CPPS performance for further evaluation through the cognitive module. Thus, the cognitive module is able to select feasible and robust configurations for process pipelines in varying use cases. Furthermore, it automatically adapts the models and algorithms based on model quality and resource consumption. The cognitive module also instantiates additional pipelines to evaluate algorithms from different classes on test functions. CAAI relies on well-defined interfaces to enable the integration of additional modules and reduce implementation effort. Finally, an implementation based on Docker, Kubernetes, and Kafka for the virtualization and orchestration of the individual modules and as messaging technology for module communication is used to evaluate a real-world use case. © 2021, The Author(s).",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85107768777
"Zhao K., Liu S., Yu J.X., Rong Y.","57016004700;57188809104;57203435589;56410469700;","Towards Feature-free TSP Solver Selection: A Deep Learning Approach",2021,"Proceedings of the International Joint Conference on Neural Networks","2021-July",,,"","",,1,"10.1109/IJCNN52387.2021.9533538","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116469115&doi=10.1109%2fIJCNN52387.2021.9533538&partnerID=40&md5=d7c81c2ba4eb4a9f3f59c2ea7c93f33b","It is widely recognized that for the traveling salesman problem (TSP), there exists no universal best solver for all problem instances. This observation has greatly facilitated the research on Algorithm Selection (AS), which seeks to identify the solver best suited for each TSP instance. Such segregation usually relies on a prior representation step, in which problem instances are first represented by carefully established problem features. However, the creation of good features is non-trivial, typically requiring considerable domain knowledge and human effort. To alleviate this issue, this paper proposes a deep learning framework, named CTAS, for TSP solver selection. Specifically, CTAS exploits deep convolutional neural networks (CNN) to automatically extract informative features from TSP instances and utilizes data augmentation to handle the scarcity of labeled instances. Extensive experiments are conducted on a challenging TSP benchmark with 6, 000 instances, which is the largest benchmark ever considered in this area. CTAS achieves over 2 × speedup of the average running time, compared with the single best solver. More importantly, CTAS is the first feature-free approach that notably outperforms classical AS models, showing huge potential of applying deep learning to AS tasks. © 2021 IEEE.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85116469115
"Friess S., Tino P., Xu Z., Menzel S., Sendhoff B., Yao X.","57211823871;6701490097;55601722500;36974904200;35619676800;7402530404;","Artificial Neural Networks as Feature Extractors in Continuous Evolutionary Optimization",2021,"Proceedings of the International Joint Conference on Neural Networks","2021-July",,,"","",,1,"10.1109/IJCNN52387.2021.9533915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116461693&doi=10.1109%2fIJCNN52387.2021.9533915&partnerID=40&md5=b5780ffd6db813ac48504644f1bd100e","Recent years have seen the advancement of data-driven paradigms in population-based and evolutionary optimization. This reflects on one hand the mere abundance of available data, but on the other hand also progresses in the refinement of previously available machine learning methods. Surprisingly, deep pattern recognition methods emerging from the studies of neural networks have only been sparingly applied. This comes unexpected, as the complex data generated by evolutionary search algorithms can be considered tedious and intractable for manual analysis with mere practical intuitions. In this work, we therefore explore opportunities to employ deep networks to directly learn problem characteristics of continuous optimization problems. Particularly, with data obtained during initial runs of an optimization algorithm. We find that a graph neural network, trained upon a graph representation of continuous search spaces, shows in comparison to more traditional approaches higher validation accuracy and retrieves characteristics within the latent space which are better at distinguishing different continuous optimization problems. We hope that our study can pave the way towards new approaches which allow us to learn problem-dependent algorithm components and recall these from predictions of inputs generated during the run-time of an optimization algorithm. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85116461693
"Ferreira L., Pilastri A., Martins C.M., Pires P.M., Cortez P.","57199844620;56386043800;57216249077;57286752700;7003574407;","A Comparison of AutoML Tools for Machine Learning, Deep Learning and XGBoost",2021,"Proceedings of the International Joint Conference on Neural Networks","2021-July",,,"","",,10,"10.1109/IJCNN52387.2021.9534091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115440741&doi=10.1109%2fIJCNN52387.2021.9534091&partnerID=40&md5=e4b5693798d55ac4cccf9cad16a5c4f4","This paper presents a benchmark of supervised Automated Machine Learning (AutoML) tools. Firstly, we analyze the characteristics of eight recent open-source AutoML tools (Auto-Keras, Auto-PyTorch, Auto-Sklearn, AutoGluon, H2O AutoML, rminer, TPOT and TransmogrifAI) and describe twelve popular OpenML datasets that were used in the benchmark (divided into regression, binary and multi-class classification tasks). Then, we perform a comparison study with hundreds of computational experiments based on three scenarios: General Machine Learning (GML), Deep Learning (DL) and XGBoost (XGB). To select the best tool, we used a lexicographic approach, considering first the average prediction score for each task and then the computational effort. The best predictive results were achieved for GML, which were further compared with the best OpenML public results. Overall, the best GML AutoML tools obtained competitive results, outperforming the best OpenML models in five datasets. These results confirm the potential of the general-purpose AutoML tools to fully automate the Machine Learning (ML) algorithm selection and tuning. © 2021 IEEE.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85115440741
"Hay Chung L.C., Xie J., Ren C.","57224483940;57207417856;36931090200;","Improved machine-learning mapping of local climate zones in metropolitan areas using composite Earth observation data in Google Earth Engine",2021,"Building and Environment","199",,"107879","","",,15,"10.1016/j.buildenv.2021.107879","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107638523&doi=10.1016%2fj.buildenv.2021.107879&partnerID=40&md5=f4e82d1c89e1f06f5a0d391cc78734eb","Accurate, large-scale local climate zone (LCZ) maps with data consistency are crucial for urban environmental studies globally. However, current approaches using Earth observation data and machine learning (ML) algorithms with local computation power are limited by low accuracy, coverage, and efficiency. Here, we present an improved workflow for generating consistent large-scale LCZ maps based on optimal data and ML algorithm selection using the Google Earth Engine (GEE) platform. Twelve data-composition scenarios and one optimized scenario were designed to explore the effects and synergetic use of nine Earth observation datasets, based on their reported potential in pixel-based classification. Our results show that depending on the intended use of the map, the random forest (RF) classifier and support vector machine (SVM) classifier are by far the most appropriate ML algorithms for pixel-based LCZ classification. While the RF classifier achieves a significantly higher overall accuracy and shows advantages in most of the individual classes, the SVM classifier exhibits significantly less variability with regard to accuracy. In addition, the competitive accuracy of the optimized scenario shows that using “elite variables” in the RF classifier can significantly improve classification accuracy while also reducing computational burden. Furthermore, thermal-infrared variables are far more influential than other variables in LCZ classification. Our study is the first attempt to make a cross-comparison of various remote sensing datasets and ML algorithms for LCZ classification using the GEE platform. As such, our results provide valuable new insights, workflows, and future directions for large-scale LCZ classification to support urban environmental studies globally. © 2021 Elsevier Ltd",Article,"Final","",Scopus,2-s2.0-85107638523
"Fellers J., Quevedo J., Abdelatti M., Steinhaus M., Sodhi M.","57226274368;7005800334;57203824445;57188759406;7004463395;","Selecting between evolutionary and classical algorithms for the CVRP using machine learning: Optimization of vehicle routing problems",2021,"GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion",,,,"127","128",,,"10.1145/3449726.3459459","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111004441&doi=10.1145%2f3449726.3459459&partnerID=40&md5=8a5ddd382f29d20b7a99d6ef834625f5","Solutions for NP-hard problems are often obtained using heuristics that yield results relatively quickly, at some cost to the objective. Many different heuristics are usually available for the same problem type, and the solution quality of a heuristic may depend on characteristics of the instance being solved. This paper explores the use of machine learning to predict the best heuristic for solving Capacitated Vehicle Routing Problems (CVRPs). A set of 23 features related to the CVRP were identified from the literature. A large set of CVRP instances were generated across the feature space, and solved using four heuristics including a genetic algorithm and a novel self-organizing map. A neural network was trained to predict the best performing heuristic for a given problem instance. The model correctly selected the best heuristic for 79% of the CVRP test instances, while the single best heuristic was dominant for only 50% of the test instances. © 2021 Owner/Author.",Conference Paper,"Final","",Scopus,2-s2.0-85111004441
"Gijsbers P., Pfisterer F., Van Rijn J.N., Bischl B., Vanschoren J.","57198770524;57194522444;56347016200;23090199300;23394289300;","Meta-learning for symbolic hyperparameter defaults",2021,"GECCO 2021 Companion - Proceedings of the 2021 Genetic and Evolutionary Computation Conference Companion",,,,"151","152",,,"10.1145/3449726.3459532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111002931&doi=10.1145%2f3449726.3459532&partnerID=40&md5=7dd8d1f74011bb4ce10a71827c3912fb","Hyperparameter optimization in machine learning (ML) deals with the problem of empirically learning an optimal algorithm configuration from data, usually formulated as a black-box optimization problem. In this work, we propose a zero-shot method to meta-learn symbolic default hyperparameter configurations that are expressed in terms of the properties of the dataset. This enables a much faster, but still data-dependent, configuration of the ML algorithm, compared to standard hyperparameter optimization approaches. In the past, symbolic and static default values have usually been obtained as hand-crafted heuristics. We propose an approach of learning such symbolic configurations as formulas of dataset properties from a large set of prior evaluations on multiple datasets by optimizing over a grammar of expressions using an evolutionary algorithm. We evaluate our method on surrogate empirical performance models as well as on real data across 6 ML algorithms on more than 100 datasets and demonstrate that our method indeed finds viable symbolic defaults. © 2021 Owner/Author.",Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85111002931
"Li R., Deng Q., Tian D., Zhu D., Lin B.","57212755726;57226018878;55614947000;57221035751;36165916900;","Predicting perovskite performance with multiple machine-learning algorithms",2021,"Crystals","11","7","818","","",,5,"10.3390/cryst11070818","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111166843&doi=10.3390%2fcryst11070818&partnerID=40&md5=ac7142b80bd62abc086df9db7ce169bb","Perovskites have attracted increasing attention because of their excellent physical and chemical properties in various fields, exhibiting a universal formula of ABO3 with matching compatible sizes of A-site and B-site cations. In this work, four different prediction models of machine learning algorithms, including support vector regression based on radial basis kernel function (SVM-RBF), ridge regression (RR), random forest (RF), and back propagation neural network (BPNN), are established to predict the formation energy, thermodynamic stability, crystal volume, and oxygen vacancy formation energy of perovskite materials. Combined with the fitting diagrams of the predicted values and DFT calculated values, the results show that SVM-RBF has a smaller bias in predicting the crystal volume. RR has a smaller bias in predicting the thermodynamic stability. RF has a smaller bias in predicting the formation energy, crystal volume, and thermodynamic stability. BPNN has a smaller bias in predicting the formation energy, thermodynamic stability, crystal volume, and oxygen vacancy formation energy. Obviously, different machine learning algorithms exhibit different sensitivity to data sample distribution, indicating that we should select different algorithms to predict different performance parameters of perovskite materials. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85111166843
"Leenings R., Winter N.R., Plagwitz L., Holstein V., Ernsting J., Sarink K., Fisch L., Steenweg J., Kleine- Vennekate L., Gebker J., Emden D., Grotegerd D., Opel N., Risse B., Jiang X., Dannlowski U., Hahn T.","57201271676;57201273898;57219587409;57219589067;56728137600;57219490962;57222520627;57222522468;57222521630;57222530350;55251290600;37101507600;56124047800;55233492900;7404628260;13806470600;57193454227;","PHOTONAI-A Python API for rapid machine learning model development",2021,"PLoS ONE","16","July","e0254062","","",,2,"10.1371/journal.pone.0254062","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110935560&doi=10.1371%2fjournal.pone.0254062&partnerID=40&md5=cc8df1120218615f5f114b97d1793e3f","PHOTONAI is a high-level Python API designed to simplify and accelerate machine learning model development. It functions as a unifying framework allowing the user to easily access and combine algorithms from different toolboxes into custom algorithm sequences. It is especially designed to support the iterative model development process and automates the repetitive training, hyperparameter optimization and evaluation tasks. Importantly, the workflow ensures unbiased performance estimates while still allowing the user to fully customize the machine learning analysis. PHOTONAI extends existing solutions with a novel pipeline implementation supporting more complex data streams, feature combinations, and algorithm selection. Metrics and results can be conveniently visualized using the PHOTONAI Explorer and predictive models are shareable in a standardized format for further external validation or application. A growing add-on ecosystem allows researchers to offer data modality specific algorithms to the community and enhance machine learning in the areas of the life sciences. Its practical utility is demonstrated on an exemplary medical machine learning problem, achieving a state-of-the-art solution in few lines of code. Source code is publicly available on Github, while examples and documentation can be found at www. photon-ai.com. © 2021 Leenings et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85110935560
"Li R., Xu A., Sun W., Wang S.","57202595361;57203989932;56434868600;57852575100;","Recommendation method for avionics feature selection algorithm based on meta-learning [基于元学习的航空电子设备特征选择算法推荐方法]",2021,"Xi Tong Gong Cheng Yu Dian Zi Ji Shu/Systems Engineering and Electronics","43","7",,"2011","2020",,,"10.12305/j.issn.1001-506X.2021.07.34","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109286449&doi=10.12305%2fj.issn.1001-506X.2021.07.34&partnerID=40&md5=0ad8015cfdcfbe289c8c4a85a9542bd9","In order to reduce the test data of avionics effectively and remove redundant information and irrelevant features, based on the existing feature selection algorithms in the field of machine learning, a recommendation method for avionics feature selection algorithm under the meta-learning framework is proposed. The proposed method aims to recommend appropriate feature selection algorithms according to the information contained in the test data of different avionics. Firstly, the description method of data set feature is analyzed. Then, the algorithm performance evaluation method based on the multi-metric index is introduced. Finally, the framework of recommendation method for feature selection algorithm is given. A metadata database is established on 42 avionics data sets and 13 filtering feature selection algorithms, the leave-one-out method is used for cross validation. The recommended hit radio reaches more than 90% and the recommended performance radio reaches more than 97%. © 2021, Editorial Office of Systems Engineering and Electronics. All right reserved.",Article,"Final","",Scopus,2-s2.0-85109286449
"Román-Portabales A., López-Nores M., Pazos-Arias J.J.","57211230644;35614992300;6506851530;","Systematic review of electricity demand forecast using ann-based machine learning algorithms",2021,"Sensors","21","13","4544","","",,3,"10.3390/s21134544","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108966558&doi=10.3390%2fs21134544&partnerID=40&md5=4e08a3f8d49fcb44d66cb8028cc345bb","The forecast of electricity demand has been a recurrent research topic for decades, due to its economical and strategic relevance. Several Machine Learning (ML) techniques have evolved in parallel with the complexity of the electric grid. This paper reviews a wide selection of approaches that have used Artificial Neural Networks (ANN) to forecast electricity demand, aiming to help newcomers and experienced researchers to appraise the common practices and to detect areas where there is room for improvement in the face of the current widespread deployment of smart meters and sensors, which yields an unprecedented amount of data to work with. The review looks at the specific problems tackled by each one of the selected papers, the results attained by their algorithms, and the strategies followed to validate and compare the results. This way, it is possible to highlight some peculiarities and algorithm configurations that seem to consistently outperform others in specific settings. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85108966558
"Varsha P.S., Akter S., Kumar A., Gochhait S., Patagundi B.","57210919302;36058277700;57221101882;57202706369;57223440204;","The Impact of Artificial Intelligence on Branding: A Bibliometric Analysis (1982-2019)",2021,"Journal of Global Information Management","29","4",,"221","246",,8,"10.4018/JGIM.20210701.oa10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105863212&doi=10.4018%2fJGIM.20210701.oa10&partnerID=40&md5=06c5fa32688b1e9c788c4a967a5687e5","Understanding the growth paths of artificial intelligence (AI) and its impact on branding is extremely pertinent of technology-driven marketing. This explorative research covers a complete bibliometric analysis of the impact of AI on branding. The sample for this research included all 117 articles from the period of 1982-2019 in the Scopus database. A bibliometric study was conducted using cooccurrence, citation analysis and co-citation analysis. The empirical analysis investigates the value propositions of AI on branding. The study revealed the nine clusters of co-occurrence: Social Media Analytics and Brand Equity; Neural Networks and Brand Choice; Chat Bots-Brand Intimacy; Twitter, Facebook, Instagram-Luxury Brands; Interactive Agent-Brand Love and User Choice; Algorithm Recommendations and E-Brand Experience; User-Generated Content-Brand Sustainability; Brand Intelligence Analytics; and Digital Innovations and Brand Excellence. The findings also identify four clusters of citation analysis Social Media Analysis and Brand Photos, Network Analysis and E-Commerce, Hybrid Simulating Modelling, and Real-Time Knowledge-Based Systems and four clusters of co-citation analysis: B2B Technology Brands, AI Fostered E-Brands, Information Cascades and Online Brand Ratings, and Voice Assistants-Brand Eureka Moments. Overall, the study presents the patterns of convergence and divergence of themes, narrowing to the specific topic, and multidisciplinary engagement in research, thus offering the recent insights in the field of AI on branding. © 2021 IGI Global. All rights reserved.",Review,"Final","All Open Access, Bronze",Scopus,2-s2.0-85105863212
"Eyler Dang L., Hubbard A., Dissak-Delon F.N., Chichom Mefire A., Juillard C.","57221328800;57219106524;56912269900;12788289900;55965988800;","Right population, right resources, right algorithm: Using machine learning efficiently and effectively in surgical systems where data are a limited resource",2021,"Surgery (United States)","170","1",,"325","328",,2,"10.1016/j.surg.2020.11.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098876064&doi=10.1016%2fj.surg.2020.11.043&partnerID=40&md5=77379eb3d56d7ed3e9b6c2a6896972d8","There is a growing interest in using machine learning algorithms to support surgical care, diagnostics, and public health surveillance in low- and middle-income countries. From our own experience and the literature, we share several lessons for developing such models in settings where the data necessary for algorithm training and implementation is a limited resource. First, the training cohort should be as similar as possible to the population of interest, and recalibration can be used to improve risk estimates when a model is transported to a new context. Second, algorithms should incorporate existing data sources or data that is easily obtainable by frontline health workers or assistants in order to optimize available resources and facilitate integration into clinical practice. Third, the Super Learner ensemble machine learning algorithm can be used to define the optimal model for a given prediction problem while minimizing bias in the algorithm selection process. By considering the right population, right resources, and right algorithm, researchers can train prediction models that are both context-appropriate and resource-conscious. There remain gaps in data availability, affordable computing capacity, and implementation studies that hinder clinical algorithm development and use in low-resource settings, although these barriers are decreasing over time. We advocate for researchers to create open-source code, apps, and training materials to allow new machine learning models to be adapted to different populations and contexts in order to support surgical providers and health care systems in low- and middle-income countries worldwide. © 2020 Elsevier Inc.",Article,"Final","",Scopus,2-s2.0-85098876064
"De Loera J.A., Haddock J., Ma A., Needell D.","6701387752;57202949794;56810425400;24335767100;","Data-driven algorithm selection and tuning in optimization and signal processing",2021,"Annals of Mathematics and Artificial Intelligence","89","7",,"711","735",,,"10.1007/s10472-020-09717-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096013238&doi=10.1007%2fs10472-020-09717-z&partnerID=40&md5=3b6684d0d06c30307ebc59ab77d16ac6","Machine learning algorithms typically rely on optimization subroutines and are well known to provide very effective outcomes for many types of problems. Here, we flip the reliance and ask the reverse question: can machine learning algorithms lead to more effective outcomes for optimization problems? Our goal is to train machine learning methods to automatically improve the performance of optimization and signal processing algorithms. As a proof of concept, we use our approach to improve two popular data processing subroutines in data science: stochastic gradient descent and greedy methods in compressed sensing. We provide experimental results that demonstrate the answer is “yes”, machine learning algorithms do lead to more effective outcomes for optimization problems, and show the future potential for this research direction. In addition to our experimental work, we prove relevant Probably Approximately Correct (PAC) learning theorems for our problems of interest. More precisely, we show that there exists a learning algorithm that, with high probability, will select the algorithm that optimizes the average performance on an input set of problem instances with a given distribution. © 2020, Springer Nature Switzerland AG.",Article,"Final","",Scopus,2-s2.0-85096013238
"Eftimov T., Jankovic A., Popovski G., Doerr C., Korošec P.","56178012800;57210408871;56121099800;55827803400;35734181400;","Personalizing performance regression models to black-box optimization problems",2021,"GECCO 2021 - Proceedings of the 2021 Genetic and Evolutionary Computation Conference",,,,"669","677",,1,"10.1145/3449639.3459407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110205544&doi=10.1145%2f3449639.3459407&partnerID=40&md5=5d3ba98964e1078389b9be1ebcc34061","Accurately predicting the performance of different optimization algorithms for previously unseen problem instances is crucial for high-performing algorithm selection and configuration techniques. In the context of numerical optimization, supervised regression approaches built on top of exploratory landscape analysis are becoming very popular. From the point of view of Machine Learning (ML), however, the approaches are often rather naïve, using default regression or classification techniques without proper investigation of the suitability of the ML tools. With this work, we bring to the attention of our community the possibility to personalize regression models to specific types of optimization problems. Instead of aiming for a single model that works well across a whole set of possibly diverse problems, our personalized regression approach acknowledges that different models may suite different types of problems. Going one step further, we also investigate the impact of selecting not a single regression model per problem, but personalized ensembles. We test our approach on predicting the performance of numerical optimization heuristics on the BBOB benchmark collection. © 2021 ACM.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85110205544
"Jankovic A., Popovski G., Eftimov T., Doerr C.","57210408871;56121099800;56178012800;55827803400;","The impact of hyper-parameter tuning for landscape-aware performance regression and algorithm selection",2021,"GECCO 2021 - Proceedings of the 2021 Genetic and Evolutionary Computation Conference",,,,"687","696",,6,"10.1145/3449639.3459406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110061713&doi=10.1145%2f3449639.3459406&partnerID=40&md5=3811abc36fb711c650ac5414495daa85","Automated algorithm selection and configuration methods that build on exploratory landscape analysis (ELA) are becoming very popular in Evolutionary Computation. However, despite a significantly growing number of applications, the underlying machine learning models are often chosen in an ad-hoc manner. We show in this work that three classical regression methods are able to achieve meaningful results for ELA-based algorithm selection. For those three models - random forests, decision trees, and bagging decision trees - the quality of the regression models is highly impacted by the chosen hyper-parameters. This has significant effects also on the quality of the algorithm selectors that are built on top of these regressions. By comparing a total number of 30 different models, each coupled with 2 complementary regression strategies, we derive guidelines for the tuning of the regression models and provide general recommendations for a more systematic use of classical machine learning models in landscape-aware algorithm selection. We point out that a choice of the machine learning model merits to be carefully undertaken and further investigated. © 2021 ACM.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85110061713
"Pontello V., Beckmann H., Lanquillon C.","57337925300;56128714500;24178557600;","Meta-learning approach for implementation of AI methods in the context of CRISP-DM with case studies from master data management",2021,"2021 IEEE International Conference on Engineering, Technology and Innovation, ICE/ITMC 2021 - Proceedings",,,,"","",,,"10.1109/ICE/ITMC52061.2021.9570265","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119068374&doi=10.1109%2fICE%2fITMC52061.2021.9570265&partnerID=40&md5=d59646abe360ea8989e56ca45c09946c","Implementing artificial intelligence (AI) methods is often not a trivial task. Especially for beginners, it is a big challenge to find out a suitable AI method to a certain problem. Moreover, a heuristic approach to algorithm selection is often error-prone and suboptimal. As a solution to this problem, a process model was developed on the basis of a meta-learning approach in the context of CRISP-DM. The model uses a criteria grid to characterize the problem and, based on these characteristics, the process model is able to propose optimal AI methods as possible solutions. The model was evaluated using tasks from master data management and the observed results find support and recognition in the literature. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85119068374
"Balcan M.-F., Deblasio D., Dick T., Kingsford C., Sandholm T., Vitercik E.","8954993900;35104548000;55921353200;35611336100;57203083791;56829527400;","How much data is sufficient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design",2021,"Proceedings of the Annual ACM Symposium on Theory of Computing",,,,"919","932",,2,"10.1145/3406325.3451036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108164557&doi=10.1145%2f3406325.3451036&partnerID=40&md5=c15bbde576a4c9f5e46034d06ed518bd","Algorithms often have tunable parameters that impact performance metrics such as runtime and solution quality. For many algorithms used in practice, no parameter settings admit meaningful worst-case bounds, so the parameters are made available for the user to tune. Alternatively, parameters may be tuned implicitly within the proof of a worst-case guarantee. Worst-case instances, however, may be rare or nonexistent in practice. A growing body of research has demonstrated that data-driven algorithm design can lead to significant improvements in performance. This approach uses a training set of problem instances sampled from an unknown, application-specific distribution and returns a parameter setting with strong average performance on the training set. We provide a broadly applicable theory for deriving generalization guarantees that bound the difference between the algorithm's average performance over the training set and its expected performance on the unknown distribution. Our results apply no matter how the parameters are tuned, be it via an automated or manual approach. The challenge is that for many types of algorithms, performance is a volatile function of the parameters: slightly perturbing the parameters can cause a large change in behavior. Prior research (e.g., Gupta and Roughgarden, SICOMP'17; Balcan et al., COLT'17, ICML'18, EC'18) has proved generalization bounds by employing case-by-case analyses of greedy algorithms, clustering algorithms, integer programming algorithms, and selling mechanisms. We uncover a unifying structure which we use to prove extremely general guarantees, yet we recover the bounds from prior research. Our guarantees, which are tight up to logarithmic factors in the worst case, apply whenever an algorithm's performance is a piecewise-constant, -linear, or - more generally - piecewise-structured function of its parameters. Our theory also implies novel bounds for voting mechanisms and dynamic programming algorithms from computational biology. © 2021 Owner/Author.",Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85108164557
"Niño-Adan I., Landa-Torres I., Manjarres D., Portillo E., Orbe L.","57208834385;39061406000;36602976600;22986390400;57224364864;","Soft-sensor for class prediction of the percentage of pentanes in butane at a debutanizer column",2021,"Sensors","21","12","3991","","",,2,"10.3390/s21123991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107443358&doi=10.3390%2fs21123991&partnerID=40&md5=ca4bae0f171e3db40b0be98b602935bb","Refineries are complex industrial systems that transform crude oil into more valuable subproducts. Due to the advances in sensors, easily measurable variables are continuously monitored and several data-driven soft-sensors are proposed to control the distillation process and the quality of the resultant subproducts. However, data preprocessing and soft-sensor modelling are still complex and time-consuming tasks that are expected to be automatised in the context of Industry 4.0. Although recently several automated learning (autoML) approaches have been proposed, these rely on model configuration and hyper-parameters optimisation. This paper advances the state-of-the-art by proposing an autoML approach that selects, among different normalisation and feature weighting preprocessing techniques and various well-known Machine Learning (ML) algorithms, the best configuration to create a reliable soft-sensor for the problem at hand. As proven in this research, each normalisation method transforms a given dataset differently, which ultimately affects the ML algorithm performance. The presented autoML approach considers the features preprocessing importance, including it, and the algorithm selection and configuration, as a fundamental stage of the methodology. The proposed autoML approach is applied to real data from a refinery in the Basque Country to create a soft-sensor in order to complement the operators’ decision-making that, based on the operational variables of a distillation process, detects 400 min in advance with 98.925% precision if the resultant product does not reach the quality standards. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85107443358
"Stefana E., Paltrinieri N.","56507524600;6504465608;","ProMetaUS: A proactive meta-learning uncertainty-based framework to select models for Dynamic Risk Management",2021,"Safety Science","138",,"105238","","",,3,"10.1016/j.ssci.2021.105238","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102079641&doi=10.1016%2fj.ssci.2021.105238&partnerID=40&md5=b549e99572ea89db98ca8e054eb1683e","Safety managers, practitioners, and researchers can employ different models for estimating and assessing hazards, consequences, likelihoods, risks, and/or mitigation measures in the safety field. The selection of a specific model may depend on the uncertainty associated with its estimation and its impact on the safety-related decision-making process. The recognition of this issue as an example of Algorithm Selection Problem (ASP) allows investigating the applicability of meta-learning principles that are scarcely adopted in the risk and safety literature. Consequently, we propose a novel meta-learning inspired framework to proactively rank a set of candidate models for Dynamic Risk Management (DRM) based on desired uncertainty conditions. We denominate this framework ProMetaUS (Proactive Meta-learning and Uncertainty-based Selection for dynamic risk management). To achieve this purpose, our meta-learning system acquires knowledge that relates the characteristics extracted both directly and indirectly from datasets (e.g. data-based, domain-based, simple and fast uncertainty-based, simple and fast sensitivity-based meta-features) to some performance measures of the models. Performance measures include confidence information, shape measurable quantities, safety decision criteria and threshold limits, and sensitivity analysis outputs. We tested the proposed framework in a case study about Oxygen Deficiency Hazard (ODH) assessment by means of @RISK. For each of the five datasets, single-performance measure rankings and a final ranking of the three models are generated. Such rankings are aggregated to obtain the global recommended ranking. © 2021 The Author(s)",Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85102079641
"Tang K., Liu S., Yang P., Yao X.","35389743100;57188809104;56403722600;7402530404;","Few-Shots Parallel Algorithm Portfolio Construction via Co-Evolution",2021,"IEEE Transactions on Evolutionary Computation","25","3","9354852","595","607",,7,"10.1109/TEVC.2021.3059661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100923940&doi=10.1109%2fTEVC.2021.3059661&partnerID=40&md5=13c07877b17d8f57b45e0e77f9a10b87","Generalization, i.e., the ability of solving problem instances that are not available during the system design and development phase, is a critical goal for intelligent systems. A typical way to achieve good generalization is to learn a model from vast data. In the context of heuristic search, such a paradigm could be implemented as configuring the parameters of a parallel algorithm portfolio (PAP) based on a set of 'training' problem instances, which is often referred to as PAP construction. However, compared to the traditional machine learning, PAP construction often suffers from the lack of training instances, and the obtained PAPs may fail to generalize well. This article proposes a novel competitive co-evolution scheme, named co-evolution of parameterized search (CEPS), as a remedy to this challenge. By co-evolving a configuration population and an instance population, CEPS is capable of obtaining generalizable PAPs with few training instances. The advantage of CEPS in improving generalization is analytically shown in this article. Two concrete algorithms, namely, CEPS-TSP and CEPS-VRPSPDTW, are presented for the traveling salesman problem (TSP) and the vehicle routing problem with simultaneous pickup-delivery and time windows (VRPSPDTW), respectively. The experimental results show that CEPS has led to better generalization, and even managed to find new best-known solutions for some instances. © 1997-2012 IEEE.",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85100923940
"Sheidaeian H., Fatemi O.","36343920000;6701394056;","Toward a general framework for jointly processor-workload empirical modeling",2021,"Journal of Supercomputing","77","6",,"5319","5353",,,"10.1007/s11227-020-03475-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095420219&doi=10.1007%2fs11227-020-03475-9&partnerID=40&md5=d4ec031b09803abaecbd84028c7f9d65","The complexity of state-of-the-art processor architectures and their consequent vast design spaces have made it difficult and time-consuming to explore the best configuration for them. Design space exploration (DSE) refers to systematic analysis and pruning of unwanted design points based on parameters of interest. DSE requires analysis and estimation of performance criteria of design points. A more accurate estimation produces a more efficient target design. A typical estimation method is machine learning approaches based on statistical inference, also known as empirical modeling, which requires only a limited number of simulations. Undoubtedly, an empirical model finds the optima much faster than using cycle-accurate simulations and is much more accurate than employing analytical models. For that purpose, our paper proposes a general methodology and a framework to find an appropriate and most accurate empirical model to estimate the performance of general-purpose or embedded multiprocessors running multithreaded workloads. This framework consists of three main steps: (1) Workload characterization and clustering, (2) Finding optimal model, and (3) Estimating the performance of a new workload outside the training set. These optimal performance prediction models could be utilized in the process of exploring the architectural design space. An experimental case is also tested using this framework for feasibility purposes. Validation experiments show MAEs less than 10% for this case. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85095420219
"Liu N.","57261696300;","Research on traditional media content distribution platform by technology",2021,"Proceedings - 2021 International Symposium on Artificial Intelligence and its Application on Media, ISAIAM 2021",,,,"184","190",,,"10.1109/ISAIAM53259.2021.00046","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115095584&doi=10.1109%2fISAIAM53259.2021.00046&partnerID=40&md5=3714a95e9f99dbe1158b4321202a585e","In the current media living environment, with the rapid development of Internet technology, new media taking the Internet technology Express has become the main way for users to obtain information. At the same time, the traditional media is declining because of the loss of the initiative to control the information channel. What is the development dilemma of traditional media in the new media environment?How can we break through this dilemma?Based on the analysis of the existing problems and development status of traditional media content distribution platform, the author draws the following conclusion that the traditional media can innovate the content distribution platform through four aspects:using algorithm recommendation to change the passive situation of communication platform;Build diversified communication platform to achieve sustainable development;Highlight UGC function of content distribution platform to increase user stickiness;With the help of new media platform to achieve the broken circle effect. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85115095584
"Wang Q., Jiang J., Zhao Y., Cao W., Wang C., Li S.","57199479507;57218866746;36122020600;57195309486;57237662200;57236553000;","Algorithm selection for software verification based on adversarial LSTM",2021,"Proceedings - 2021 7th IEEE International Conference on Big Data Security on Cloud, IEEE International Conference on High Performance and Smart Computing, and IEEE International Conference on Intelligent Data and Security, BigDataSecurity/HPSC/IDS 2021",,,"9463528","87","92",,1,"10.1109/BigDataSecurityHPSCIDS52275.2021.00026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113745498&doi=10.1109%2fBigDataSecurityHPSCIDS52275.2021.00026&partnerID=40&md5=ed78e6d37c0602a4b6d2d370f3436962","As a prevalent technique for checking the correctness of software, software verification has achieved a significant progress in the past decades, reaching a point where a large number of verification algorithms and tools are available and sophisticated enough to handle the large-scale industrial software. However, it remains a difficult task to select a suitable verification algorithm or tool for the software at hand, given the fact that the underlying algorithms are diverse and the performance tradeoffs are hard to accurately characterize. In this paper, we study the algorithm selection problem for software verification, and propose a novel algorithm selection model based on the Long Short Term Memory network (LSTM). Our solution employs word2vec to obtain the embedding representation of the code, avoiding constructing the software features manually. We also propose a novel approach to construct the adversarial code examples in order to solve the sparsity and data imbalance problem. The experimental evaluations on the latest available dataset show that our solution improves the prediction accuracy by about 7% compared with the state-of-the-art selection algorithm. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85113745498
"De Togni G., Erikainen S., Chan S., Cunningham-Burley S.","57221545036;57193620217;15019154300;6603769446;","What makes AI ‘intelligent’ and ‘caring’? Exploring affect and relationality across three sites of intelligence and care",2021,"Social Science and Medicine","277",,"113874","","",,2,"10.1016/j.socscimed.2021.113874","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104621685&doi=10.1016%2fj.socscimed.2021.113874&partnerID=40&md5=af6d48c2d6275f228934d2ea4f554211","This paper scrutinises how AI and robotic technologies are transforming the relationships between people and machines in new affective, embodied and relational ways. Through investigating what it means to exist as human ‘in relation’ to AI across health and care contexts, we aim to make three main contributions. (1) We start by highlighting the complexities of philosophical issues surrounding the concepts of “artificial intelligence” and “ethical machines.” (2) We outline some potential challenges and opportunities that the creation of such technologies may bring in the health and care settings. We focus on AI applications that interface with health and care via examples where AI is explicitly designed as an ‘augmenting’ technology that can overcome human bodily and cognitive as well as socio-economic constraints. We focus on three dimensions of ‘intelligence’ - physical, interpretive, and emotional - using the examples of robotic surgery, digital pathology, and robot caregivers, respectively. Through investigating these areas, we interrogate the social context and implications of human-technology interaction in the interrelational sphere of care practice. (3) We argue, in conclusion, that there is a need for an interdisciplinary mode of theorising ‘intelligence’ as relational and affective in ways that can accommodate the fragmentation of both conceptual and material boundaries between human and AI, and human and machine. Our aim in investigating these sociological, philosophical and ethical questions is primarily to explore the relationship between affect, relationality and ‘intelligence,’ the intersection and integration of ‘human’ and ‘artificial’ intelligence, through an examination of how AI is used across different dimensions of intelligence. This allows us to scrutinise how ‘intelligence’ is ultimately conveyed, understood and (technologically or algorithmically) configured in practice through emerging relationships that go beyond the conceptual divisions between humans and machines, and humans vis-à-vis artificial intelligence-based technologies. © 2021 The Authors",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85104621685
"Al-Kababji A., Amira A., Bensaali F., Jarouf A., Shidqi L., Djelouat H.","57204100672;23003399100;6506260453;57214654329;57216463744;57191832738;","An IoT-based framework for remote fall monitoring",2021,"Biomedical Signal Processing and Control","67",,"102532","","",,2,"10.1016/j.bspc.2021.102532","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102129876&doi=10.1016%2fj.bspc.2021.102532&partnerID=40&md5=929b217e3f2dd665b91863214dd4e680","Fall detection is a serious healthcare issue that needs to be solved. Falling without quick medical intervention would lower elderly's chances of survival, especially if living alone. Hence, the need is there for developing fall detection algorithms with high accuracy. This paper presents a novel IoT-based system for fall detection that includes a sensing device transmitting data to a mobile application through a cloud-connected gateway device. Then, the focus is shifted to the algorithmic aspect where multiple features are extracted from 3-axis accelerometer data taken from existing datasets. The results emphasize on the significance of Continuous Wavelet Transform (CWT) as an influential feature for determining falls. CWT, Signal Energy (SE), Signal Magnitude Area (SMA), and Signal Vector Magnitude (SVM) features have shown promising classification results using K-Nearest Neighbors (KNN) and E-Nearest Neighbors (ENN). For all performance metrics (accuracy, recall, precision, specificity, and F1 score), the achieved results are higher than 95% for a dataset of small size, while more than 98.47% score is achieved in the aforementioned criteria over the UniMiB-SHAR dataset by the same algorithms, where the classification time for a single test record is extremely efficient and is real-time. © 2021 Elsevier Ltd",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85102129876
"Niloy T.-S.R., Rahman Md.A.","57210418938;57212184292;","Diagnosis of COVID-19 infected lungs from chest x-ray images using deep learning algorithms",2021,"AIUB Journal of Science and Engineering","20","1",,"33","40",,,"10.53799/AJSE.V20I1.142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112444390&doi=10.53799%2fAJSE.V20I1.142&partnerID=40&md5=d7235184929b451ef32b897d41e6427f","Severe Acute Respiratory Symptom Coronavirus 2 (SARS-CoV-2) is newly discovered as a beta coronavirus. The virus-induced unexplained etiological pneumonia and is referred to as the 2019 Coronavirus Disease (COVID-19). Though the disease has appeared in a new way, there is no medication for transited patients. So, for diagnosing the COVID-19 infected lungs from X-Ray images, an automated technique has been suggested in this manuscript. The proposed system is divided into two stages: Image Acquisition and Selection of Algorithms. In the IAA, the training data's size has been increased by augmenting the image in different ways. The Algorithm Selection portion explained the Convolutional Neural Network (CNN) and VGG19. The Tuning of hyperparameters section determines the precise hyperparameter combination in order to maximize the model's performance. In this study, CNN and VGG19 are used and found accuracy scores of 97% and 67%, respectively. The comparative analysis shows that the propound method acts better than the solution that exists. Eventually, Precision, Recall, and F1 score have been extracted and interpreted the model's loss functions in the research. This research has carried out by focusing on essential aspects in terms of COVID-19. Therefore, for the diagnosis of coronavirus infection, the technique can be used effectively. © 2021 AIUB Office of Research and Publication. All rights reserved.",Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85112444390
"Zahrt A.F., Rose B.T., Darrow W.T., Henle J.J., Denmark S.E.","57193602887;57205465310;57195324925;56619199500;35359575200;","Computational methods for training set selection and error assessment applied to catalyst design: Guidelines for deciding which reactions to run first and which to run next",2021,"Reaction Chemistry and Engineering","6","4",,"694","708",,5,"10.1039/d1re00013f","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103764152&doi=10.1039%2fd1re00013f&partnerID=40&md5=da166395cc96d0342fe20854a3fa2e9d","The application of machine learning (ML) to problems in homogeneous catalysis has emerged as a promising avenue for catalyst optimization. An important aspect of such optimization campaigns is determining which reactions to run at the outset of experimentation and which future predictions are the most reliable. Herein, we explore methods for these two tasks in the context of our previously developed chemoinformatics workflow. First, different methods for training set selection for library-based optimization problems are compared, including algorithmic selection and selection informed by unsupervised learning methods. Next, an array of different metrics for assessment of prediction confidence are examined in multiple catalyst manifolds. These approaches will inform future computer-guided studies to accelerate catalyst selection and reaction optimization. Finally, this work demonstrates the generality of the average steric occupancy (ASO) and average electronic indicator field (AEIF) descriptors in their application to transition metal catalysts for the first time. This journal is © The Royal Society of Chemistry.",Article,"Final","",Scopus,2-s2.0-85103764152
"Wu R.M., Fisher L.R.","57204975669;35232776600;","Role of Video Capsule in Small Bowel Bleeding",2021,"Gastrointestinal Endoscopy Clinics of North America","31","2",,"277","306",,,"10.1016/j.giec.2020.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102611246&doi=10.1016%2fj.giec.2020.12.003&partnerID=40&md5=ae5bd96ad2103680748964872a298116","Video capsule endoscopy has an essential role in the diagnosis and management of small bowel bleeding and is the first-line study recommended for this purpose. This article reviews the risk factors for small bowel bleeding, optimal timing for video capsule endoscopy testing, and algorithms recommended for evaluation. Used primarily for the assessment of nonacute gastrointestinal blood loss, video capsule endoscopy has an emerging role for more urgent use in emergency settings and in special populations. Future software incorporation of neural networks to enhance lesion detection will likely result in an augmented role of video capsule endoscopy in small bowel bleeding. © 2020 Elsevier Inc.",Review,"Final","",Scopus,2-s2.0-85102611246
"Vasey B., Ursprung S., Beddoe B., Taylor E.H., Marlow N., Bilbro N., Watkinson P., McCulloch P.","57217590652;57212195478;57222473650;57219127034;55884381900;57202534807;16070706600;57203499420;","Association of Clinician Diagnostic Performance with Machine Learning-Based Decision Support Systems: A Systematic Review",2021,"JAMA Network Open","4","3","e211276","","",,10,"10.1001/jamanetworkopen.2021.1276","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102825936&doi=10.1001%2fjamanetworkopen.2021.1276&partnerID=40&md5=9c9c051510f1556786db238346b38308","Importance: An increasing number of machine learning (ML)-based clinical decision support systems (CDSSs) are described in the medical literature, but this research focuses almost entirely on comparing CDSS directly with clinicians (human vs computer). Little is known about the outcomes of these systems when used as adjuncts to human decision-making (human vs human with computer). Objectives: To conduct a systematic review to investigate the association between the interactive use of ML-based diagnostic CDSSs and clinician performance and to examine the extent of the CDSSs' human factors evaluation. Evidence Review: A search of MEDLINE, Embase, PsycINFO, and grey literature was conducted for the period between January 1, 2010, and May 31, 2019. Peer-reviewed studies published in English comparing human clinician performance with and without interactive use of an ML-based diagnostic CDSSs were included. All metrics used to assess human performance were considered as outcomes. The risk of bias was assessed using Quality Assessment of Diagnostic Accuracy Studies (QUADAS-2) and Risk of Bias in Non-Randomised Studies-Intervention (ROBINS-I). Narrative summaries were produced for the main outcomes. Given the heterogeneity of medical conditions, outcomes of interest, and evaluation metrics, no meta-analysis was performed. Findings: A total of 8112 studies were initially retrieved and 5154 abstracts were screened; of these, 37 studies met the inclusion criteria. The median number of participating clinicians was 4 (interquartile range, 3-8). Of the 107 results that reported statistical significance, 54 (50%) were increased by the use of CDSSs, 4 (4%) were decreased, and 49 (46%) showed no change or an unclear change. In the subgroup of studies carried out in representative clinical settings, no association between the use of ML-based diagnostic CDSSs and improved clinician performance could be observed. Interobserver agreement was the commonly reported outcome whose change was the most strongly associated with CDSS use. Four studies (11%) reported on user feedback, and, in all but 1 case, clinicians decided to override at least some of the algorithms' recommendations. Twenty-eight studies (76%) were rated as having a high risk of bias in at least 1 of the 4 QUADAS-2 core domains, and 6 studies (16%) were considered to be at serious or critical risk of bias using ROBINS-I. Conclusions and Relevance: This systematic review found only sparse evidence that the use of ML-based CDSSs is associated with improved clinician diagnostic performance. Most studies had a low number of participants, were at high or unclear risk of bias, and showed little or no consideration for human factors. Caution should be exercised when estimating the current potential of ML to improve human diagnostic performance, and more comprehensive evaluation should be conducted before deploying ML-based CDSSs in clinical settings. The results highlight the importance of considering supported human decisions as end points rather than merely the stand-alone CDSSs outputs.. © 2021 Laser Institute of America. All rights reserved.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85102825936
"Kabaivanov S., Roberts K., Kovacheva S.","57063255900;7402041957;6701671180;","Machine learning assisted social system analysis: Youth transitions in five south and east Mediterranean countries",2021,"AIP Conference Proceedings","2333",,"030002","","",,,"10.1063/5.0041807","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102743249&doi=10.1063%2f5.0041807&partnerID=40&md5=02bd0989d623eee734f3daade9918811","While youth transitions to adulthood have been subject to various social studies, it is often the case that statistical tools of choice are limited in terms of sophistication and flexibility. Our study uses information collected as part of the SAHWA project [1] with the primary goal being to verify if machine learning can help rule out inappropriate assumptions and improve transition to adulthood analysis by outlining youth groups, their common characteristics and outlier cases (as well as if they are significant). As data includes numeric, as well as categorical and nominal variables use of common algorithms like K-Means clustering is not possible. It's also not reasonable to build on Eucledian distances in this mixed space, ruling out other classification methods that rely on it. We split the clustering algorithm selection into: (1) selection of distance calculation function, (2) algorithm and (3) decision on number of groups. A valuable information about transition to adulthood is obtained without imposing restrictive theoretical framework. © 2021 Author(s).",Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85102743249
"Ortiz-Bayliss J.C., Amaya I., Cruz-Duarte J.M., Gutierrez-Rodriguez A.E., Conant-Pablos S.E., Terashima-Marín H.","25723655700;54991678900;57192916679;36760830300;6506753404;13609297000;","A general framework based on machine learning for algorithm selection in constraint satisfaction problems",2021,"Applied Sciences (Switzerland)","11","6","2749","","",,,"10.3390/app11062749","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103496054&doi=10.3390%2fapp11062749&partnerID=40&md5=a20b606e8ed2d87bc8a425beab952891","Many of the works conducted on algorithm selection strategies—methods that choose a suitable solving method for a particular problem—start from scratch since only a few investigations on reusable components of such methods are found in the literature. Additionally, researchers might unintentionally omit some implementation details when documenting the algorithm selection strategy. This makes it difficult for others to reproduce the behavior obtained by such an approach. To address these problems, we propose to rely on existing techniques from the Machine Learning realm to speed-up the generation of algorithm selection strategies while improving the modularity and reproducibility of the research. The proposed solution model is implemented on a domain-independent Machine Learning module that executes the core mechanism of the algorithm selection task. The algorithm selection strategies produced in this work are implemented and tested rapidly compared against the time it would take to build a similar approach from scratch. We produce four novel algorithm selectors based on Machine Learning for constraint satisfaction problems to verify our approach. Our data suggest that these algorithms outperform the best performing algorithm on a set of test instances. For example, the algorithm selectors Multiclass Neural Network (MNN) and Multiclass Logistic Regression (MLR), powered by a neural network and linear regression, respectively, reduced the search cost (in terms of consistency checks) of the best performing heuristic (KAPPA), on average, by 49% for the instances considered for this work. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85103496054
"Sun A.Y., Scanlon B.R., Save H., Rateb A.","9279316500;7003345902;36060321300;57193404653;","Reconstruction of GRACE Total Water Storage Through Automated Machine Learning",2021,"Water Resources Research","57","2","e2020WR028666","","",,19,"10.1029/2020WR028666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101481774&doi=10.1029%2f2020WR028666&partnerID=40&md5=8b04c4a958e5b625a15210429de05f3a","The Gravity Recovery and Climate Experiment (GRACE) satellite mission and its follow-on, GRACE-FO, have provided unprecedented opportunities to quantify the impact of climate extremes and human activities on total water storage at large scales. The ∼1-year data gap between the two GRACE missions needs to be filled to maintain data continuity and maximize mission benefits. In this study, we applied an automated machine learning (AutoML) workflow to perform gridwise GRACE-like data reconstruction. AutoML represents a new paradigm for optimal algorithm selection, model structure selection, and hyperparameter tuning, addressing some of the most challenging issues in machine learning applications. We demonstrated the workflow over the conterminous U.S. (CONUS) using six types of machine learning models and multiple groups of meteorological and climatic variables as predictors. Results indicate that the AutoML-assisted gap filling achieved satisfactory performance over the CONUS. On the testing data, the mean gridwise Nash-Sutcliffe efficiency is around 0.85, the mean correlation coefficient is around 0.95, and the mean normalized root-mean-square-error is about 0.09. Trained models maintain good performance when extrapolating to the mission gap and to GRACE-FO periods (after June 2017). Results further suggest that no single algorithm provides the best predictive performance over the entire CONUS, stressing the importance of using an end-to-end workflow to train, optimize, and combine multiple machine learning models to deliver robust performance, especially when building large-scale hydrological prediction systems and when predictor importance exhibiting strong spatial variability. © 2020. American Geophysical Union. All Rights Reserved.",Article,"Final","",Scopus,2-s2.0-85101481774
"Malan K.M.","6602259896;","A survey of advances in landscape analysis for optimisation",2021,"Algorithms","14","2","40","","",,20,"10.3390/a14020040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100606798&doi=10.3390%2fa14020040&partnerID=40&md5=11ed91f8fa640cd7ebe67413d36aa227","Fitness landscapes were proposed in 1932 as an abstract notion for understanding biological evolution and were later used to explain evolutionary algorithm behaviour. The last ten years has seen the field of fitness landscape analysis develop from a largely theoretical idea in evolutionary computation to a practical tool applied in optimisation in general and more recently in machine learning. With this widened scope, new types of landscapes have emerged such as multiobjective landscapes, violation landscapes, dynamic and coupled landscapes and error landscapes. This survey is a follow-up from a 2013 survey on fitness landscapes and includes an additional 11 landscape analysis techniques. The paper also includes a survey on the applications of landscape analysis for understanding complex problems and explaining algorithm behaviour, as well as algorithm performance prediction and automated algorithm configuration and selection. The extensive use of landscape analysis in a broad range of areas highlights the wide applicability of the techniques and the paper discusses some opportunities for further research in this growing field. © 2021 by the author. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85100606798
"Krymov R.A., Khamukhin A.V.","57223085838;57202355041;","Machine Learning Approach to Efficient Hyperparameters Search for Video Streams Semantic Analysis Algorithms",2021,"Proceedings of the 2021 IEEE Conference of Russian Young Researchers in Electrical and Electronic Engineering, ElConRus 2021",,,"9396177","2141","2144",,,"10.1109/ElConRus51938.2021.9396177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104752149&doi=10.1109%2fElConRus51938.2021.9396177&partnerID=40&md5=5f386c5ef7a8a1e785295adffa5d7590","Computer vision algorithms have complex set of parameters that significantly influence resulting quality. On the one hand, such a big amount of parameters that can be varied results in algorithm ability to fit for particular usage circumstances. On the other hand, fine-tuning of even several parameters may require a lot of human resources unless this task is automated. An approach to such automatization is provided considering a machine learning problem with several peculiarities. First, some parameters are restricted to have integer values. Second, multiple metrics describing algorithm quality are restricted to have integer values as well. Third, there are no analytical formulae describing dependences between parameters and metrics values. Mentioned features make gradient-descent-like methods widely used for neural networks training inapplicable for solving that optimization problem. As result, automatically tuned parameters provides better quality than manually selected ones. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85104752149
"La Cava W., Williams H., Fu W., Vitale S., Srivatsan D., Moore J.H.","56313869000;57194773345;57209607571;57189369026;57221137273;57199210697;","Evaluating recommender systems for AI-driven biomedical informatics",2021,"Bioinformatics","37","2",,"250","256",,4,"10.1093/bioinformatics/btaa698","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105698384&doi=10.1093%2fbioinformatics%2fbtaa698&partnerID=40&md5=14048e889f91ab0f290c35fcdb229e0c","Motivation: Many researchers with domain expertise are unable to easily apply machine learning (ML) to their bioinformatics data due to a lack of ML and/or coding expertise. Methods that have been proposed thus far to automate ML mostly require programming experience as well as expert knowledge to tune and apply the algorithms correctly. Here, we study a method of automating biomedical data science using a web-based AI platform to recommend model choices and conduct experiments. We have two goals in mind: first, to make it easy to construct sophisticated models of biomedical processes; and second, to provide a fully automated AI agent that can choose and conduct promising experiments for the user, based on the user's experiments as well as prior knowledge. To validate this framework, we conduct an experiment on 165 classification problems, comparing to state-of-the-art, automated approaches. Finally, we use this tool to develop predictive models of septic shock in critical care patients. Results: We find that matrix factorization-based recommendation systems outperform metalearning methods for automating ML. This result mirrors the results of earlier recommender systems research in other domains. The proposed AI is competitive with state-of-the-art automated ML methods in terms of choosing optimal algorithm configurations for datasets. In our application to prediction of septic shock, the AI-driven analysis produces a competent ML model (AUROC 0.85±0.02) that performs on par with state-of-the-art deep learning results for this task, with much less computational effort. © 2020 The Author(s) 2020. Published by Oxford University Press.",Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85105698384
"Muñoz M.A., Yan T., Leal M.R., Smith-Miles K., Lorena A.C., Pappa G.L., Rodrigues R.M.","55499624900;57222747748;57222748528;55369980900;7801325731;8884637200;57222748109;","An instance space analysis of regression problems",2021,"ACM Transactions on Knowledge Discovery from Data","15","2","28","","",,6,"10.1145/3436893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103984562&doi=10.1145%2f3436893&partnerID=40&md5=c2fd0a300e227911fe8bd8b35d5b69f6","The quest for greater insights into algorithm strengths and weaknesses, as revealed when studying algorithm performance on large collections of test problems, is supported by interactive visual analytics tools. A recent advance is Instance Space Analysis, which presents a visualization of the space occupied by the test datasets, and the performance of algorithms across the instance space. The strengths and weaknesses of algorithms can be visually assessed, and the adequacy of the test datasets can be scrutinized through visual analytics. This article presents the first Instance Space Analysis of regression problems in Machine Learning, considering the performance of 14 popular algorithms on 4,855 test datasets from a variety of sources. The two-dimensional instance space is defined by measurable characteristics of regression problems, selected from over 26 candidate features. It enables the similarities and differences between test instances to be visualized, along with the predictive performance of regression algorithms across the entire instance space. The purpose of creating this framework for visual analysis of an instance space is twofold: one may assess the capability and suitability of various regression techniques; meanwhile the bias, diversity, and level of difficulty of the regression problems popularly used by the community can be visually revealed. This article shows the applicability of the created regression instance space to provide insights into the strengths and weaknesses of regression algorithms, and the opportunities to diversify the benchmark test instances to support greater insights. © 2021 ACM.",Article,"Final","",Scopus,2-s2.0-85103984562
"Floegel D.","57200314706;","Labor, classification and productions of culture on Netflix",2021,"Journal of Documentation","77","1",,"209","228",,2,"10.1108/JD-06-2020-0108","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092525020&doi=10.1108%2fJD-06-2020-0108&partnerID=40&md5=ccb0c18e6b36732291210f6013043abe","Purpose: This paper examines promotional practices Netflix employs via Twitter and its automated recommendation system in order to deepen our understanding of how streaming services contribute to sociotechnical inequities under capitalism. Design/methodology/approach: Tweets from two Netflix Twitter accounts as well as material features of Netflix's recommendation system were qualitatively analyzed using inductive analysis and the constant comparative method in order to explore dimensions of Netflix's promotional practices. Findings: Twitter accounts and the recommendation system profit off people's labor to promote content, and such labor allows Netflix to create and refine classification practices wherein both people and content are categorized in inequitable ways. Labor and classification feed into Netflix's production of culture via appropriation on Twitter and algorithmic decision-making within both the recommendation system and broader AI-driven production practices. Social implications: Assemblages that include algorithmic recommendation systems are imbued with structural inequities and therefore unable to be fixed by merely diversifying cultural industries or retooling algorithms on streaming platforms. It is necessary to understand systemic injustices within these systems so that we may imagine and enact just alternatives. Originality/value: Findings demonstrate that via surveillance tactics that exploit people's labor for promotional gains, enforce normative classification schemes, and culminate in normative cultural productions, Netflix engenders practices that regulate bodies and culture in ways that exemplify interconnections between people, machines, and social institutions. These interconnections further reflect and result in material inequities that crystalize within sociotechnical processes. © 2020, Emerald Publishing Limited.",Article,"Final","",Scopus,2-s2.0-85092525020
"Scott J., Niemetz A., Preiner M., Nejati S., Ganesh V.","57197623960;55321001000;55320805900;57191253738;36018364100;","MachSMT: A Machine Learning-based Algorithm Selector for SMT Solvers",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12652 LNCS",,,"303","325",,6,"10.1007/978-3-030-72013-116","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135378970&doi=10.1007%2f978-3-030-72013-116&partnerID=40&md5=2108011cbfac9ef3c8d5a012ff3bf1bd","In this paper, we present MachSMT, an algorithm selection tool for Satisfiability Modulo Theories (SMT) solvers. MachSMT supports the entirety of the SMT-LIB language. It employs machine learning (ML) methods to construct both empirical hardness models (EHMs) and pairwise ranking comparators (PWCs) over state-of-the-art SMT solvers. Given an SMT formula I as input, MachSMT leverages these learnt models to output a ranking of solvers based on predicted run time on the formula I. We evaluate MachSMT on the solvers, benchmarks, and data obtained from SMT-COMP 2019 and 2020. We observe MachSMT frequently improves on competition winners, winning 54 divisions outright and up to a 198.4% improvement in PAR-2 score, notably in logics that have broad applications (e.g., BV, LIA, NRA, etc.) in verification, program analysis, and software engineering. The MachSMT tool is designed to be easily tuned and extended to any suitable solver application by users. MachSMT is not a replacement for SMT solvers by any means. Instead, it is a tool that enables users to leverage the collective strength of the diverse set of algorithms implemented as part of these sophisticated solvers. © The Author(s) 2021.",Conference Paper,"Final","",Scopus,2-s2.0-85135378970
"Freeman C., Beaver I., Mueen A.","57191862354;14010002500;35243537000;","Improving Univariate Time Series Anomaly Detection Through Automatic Algorithm Selection and Human-in-the-Loop False-Positive Removement",2021,"Proceedings of the International Florida Artificial Intelligence Research Society Conference, FLAIRS","34",,,"","",,,"10.32473/flairs.v34i1.128543","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131135633&doi=10.32473%2fflairs.v34i1.128543&partnerID=40&md5=cae03226ab241f18517b468d0c8c5ad7","The existence of a time series anomaly detection method that performs well for all domains is a myth. Given a massive library of available methods, how can one select the best method for their application? An extensive evaluation of every anomaly detection method is not feasible. Many existing anomaly detection systems do not include an avenue for human feedback, essential given the subjective nature of what even is anomalous. We present a technique for improving univariate time series anomaly detection through automatic algorithm selection and human-in-the-loop false-positive removement. These determinations were made by extensively experimenting with over 30 pre-annotated time series from the open-source Numenta Anomaly Benchmark repository. Once the highest performing anomaly detection methods are selected via these characteristics, humans can annotate the predicted outliers which are used to tune anomaly scores via subsequence similarity search and improve the selected methods for their data, increasing evaluation scores and reducing the need for annotation by 70% on predicted anomalies where annotation is used to improve F-scores. © 2021by the authors. All rights reserved.",Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85131135633
"Tuero J., Buro M.","57221812926;10244652400;","Bayes DistNet - A Robust Neural Network for Algorithm Runtime Distribution Predictions",2021,"35th AAAI Conference on Artificial Intelligence, AAAI 2021","14A",,,"12418","12426",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85130090729&partnerID=40&md5=e6c794c8d40790e5c1c55e6983200231","Randomized algorithms are used in many state-of-theart solvers for constraint satisfaction problems (CSP) and Boolean satisfiability (SAT) problems. For many of these problems, there is no single solver which will dominate others. Having access to the underlying runtime distributions (RTD) of these solvers can allow for better use of algorithm selection, algorithm portfolios, and restart strategies. Previous state-of-the-art methods directly try to predict a fixed parametric distribution that the input instance follows. In this paper, we extend RTD prediction models into the Bayesian setting for the first time. This new model achieves robust predictive performance in the low observation setting, as well as handling censored observations. This technique also allows for richer representations which cannot be achieved by the classical models which restrict their output representations. Our model outperforms the previous state-of-the-art model in settings in which data is scarce, and can make use of censored data such as lower bound time estimates, where that type of data would otherwise be discarded. It can also quantify its uncertainty in its predictions, allowing for algorithm portfolio models to make better informed decisions about which algorithm to run on a particular instance. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85130090729
"Shekhar S., Bansode A., Salim A.","57425963300;57425090500;57425963400;","A Comparative study of Hyper-Parameter Optimization Tools",2021,"2021 IEEE Asia-Pacific Conference on Computer Science and Data Engineering, CSDE 2021",,,,"","",,,"10.1109/CSDE53843.2021.9718485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127833071&doi=10.1109%2fCSDE53843.2021.9718485&partnerID=40&md5=8a6804be1bba2d84ae51e2ed9e7dfe98","Most of the machine learning models have associated hyper-parameters along with their parameters. While the algorithm gives the solution for parameters, its utility for model performance is highly dependent on the choice of hyperparameters. For a robust performance of a model, it is necessary to find out the right hyper-parameter combination. Hyper-parameter optimization (HPO) is a systematic process that helps in finding the right values for them. The conventional methods for this purpose are grid search and random search and both methods create issues in industrial-scale applications. Hence a set of strategies have been recently proposed based on Bayesian optimization and evolutionary algorithm principles that help in runtime issues in a production environment and robust performance. In this paper, we compare the performance of four python libraries, namely Optuna, Hyper-opt, Optunity, and sequential model-based algorithm configuration (SMAC) that has been proposed for hyper-parameter optimization. The performance of these tools is tested using two benchmarks. The first one is to solve a combined algorithm selection and hyper-parameter optimization (CASH) problem The second one is the NeurIPS black-box optimization challenge in which a multilayer perceptron (MLP) architecture has to be chosen from a set of related architecture constraints and hyper-parameters. The benchmarking is done with six real-world datasets. From the experiments, we found that Optuna has better performance for CASH problem and HyperOpt for MLP problem. © IEEE 2022.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85127833071
"Manikonda K., Hasan A.R., Obi C.E., Islam R., Sleiti A.K., Abdelrazeq M.W., Rahman M.A.","57215123921;35576550700;57516045200;57214301745;6506648702;57219327937;56018090700;","Application of Machine Learning Classification Algorithms for Two-Phase Gas-Liquid Flow Regime Identification",2021,"Society of Petroleum Engineers - Abu Dhabi International Petroleum Exhibition and Conference, ADIP 2021",,,,"","",,,"10.2118/208214-MS","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127705005&doi=10.2118%2f208214-MS&partnerID=40&md5=fd3fc5d309eb63f600fbbf73797eb5f1","This research aims to identify the best machine learning (ML) classification techniques for classifying the flow regimes in vertical gas-liquid two-phase flow. Two-phase flow regime identification is crucial for many operations in the oil and gas industry. Processes such as flow assurance, well control, and production rely heavily on accurate identification of flow regimes for their respective systems' smooth functioning. The primary motivation for the proposed ML classification algorithm selection processes was drilling and well control applications in Deepwater wells. The process started with vertical two-phase flow data collection from literature and two different flow loops. One, a 140 ft. tall vertical flow loop with a centralized inner metal pipe and a larger outer acrylic pipe. Second, an 18-ft long flow loop, also with a centralized, inner metal drill pipe. After extensive experimental and historical data collection, supervised and unsupervised ML classification models such as Multi-class Support vector machine (MCSVM), K-Nearest Neighbor Classifier (KNN), K-means clustering, and hierarchical clustering were fit on the datasets to separate the different flow regions. The next step was fine-tuning the models' parameters and kernels. The last step was to compare the different combinations of models and refining techniques for the best prediction accuracy and the least variance. Among the different models and combinations with refining techniques, the 5- fold cross-validated KNN algorithm, with 37 neighbors, gave the optimal solution with a 98% classification accuracy on the test data. The KNN model distinguished five major, distinct flow regions for the dataset and a few minor regions. These five regions were bubbly flow, slug flow, churn flow, annular flow, and intermittent flow. The KNN-generated flow regime maps matched well with those presented by Hasan and Kabir (2018). The MCSVM model produced visually similar flow maps to KNN but significantly underperformed them in prediction accuracy. The MCSVM training errors ranged between 50% - 60% at normal parameter values and costs but went up to 99% at abnormally high values. However, their prediction accuracy was below 50% even at these highly overfitted conditions. In unsupervised models, both clustering techniques pointed to an optimal cluster number between 10 and 15, consistent with the 14 we have in the dataset. Within the context of gas kicks and well control, a well-trained, reliable two-phase flow region classification algorithm offers many advantages. When trained with well-specific data, it can act as a black box for flow regime identification and subsequent well-control measure decisions for the well. Further advancements with more robust statistical training techniques can render these algorithms as a basis for well-control measures in drilling automation software. On a broader scale, these classification techniques have many applications in flow assurance, production, and any other area with gas-liquid two-phase flow. © Copyright 2021, Society of Petroleum Engineers",Conference Paper,"Final","",Scopus,2-s2.0-85127705005
"Adebiyi M.O., Arowolo M.O., Archibong G.I., Mshelia M.D., Adebiyi A.A.","36011449300;57214819505;57551970600;57551576800;22978651100;","An Sql Injection Detection Model Using Chi-Square with Classification Techniques",2021,"International Conference on Electrical, Computer, and Energy Technologies, ICECET 2021",,,,"","",,,"10.1109/ICECET52533.2021.9698771","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85127060990&doi=10.1109%2fICECET52533.2021.9698771&partnerID=40&md5=5fbd65ea3459d68229ca5a4556110211","SQL Injection attacks is a common threat to web applications that utilizes poor input validation to implement attack on a target database. It is becoming a very serious problem in web application as successful leads to loss of integrity and confidentiality and this makes it a very sensitive issue of software security. This study gives a review on SQL Injection detection and prevention techniques using machine learning classifiers. Machine Learning approach has been found to be profound for SQLIA mitigation, which is implemented through defensive coding approach. An experimental analysis was performed to evaluate the performance of the learning classification algorithms to choose the best algorithm. It is imperative to note that a good number of the evaluated techniques were able to detect and prevent the SQLIA based on the KDD Test dataset. From the findings, Naive Bayes had the minimum Accuracy 80.01%, Sensitivity as well as Specificity while Decision Tree had the highest Accuracy 98.11%, Sensitivity and Specificity and therefore was chosen as the best classifier for SQLIA detection and prevention. Therefore, beyond Accuracy, other performance evaluation metrics are critical for optimal algorithm selection for predictive analytics. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85127060990
"Taratukhin O., Muravyov S.","57490002000;57194035005;","Meta-Learning Based Feature Selection for Clustering",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13113 LNCS",,,"548","559",,,"10.1007/978-3-030-91608-4_54","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126444868&doi=10.1007%2f978-3-030-91608-4_54&partnerID=40&md5=43344ae49732b3c191a144de8a09edc4","Clustering is a highly demanded task nowadays. However, it requires the attention of human experts and it might certainly benefit from automation. This paper presents a method to perform simultaneous automatic algorithm selection and hyperparameter tuning with feature selection for clustering. The algorithm also features a meta-model to predict promising algorithm configurations based on dataset properties. Experimental results are provided for a set of benchmark datasets, it is shown that the proposed method outperforms known alternatives. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85126444868
"Kumar A., Thacker H.K., Gupta A., Jagannathachar K.K., Yoon D.","57479142200;57212484902;57212483074;57478720900;57478997400;","A Deep Learning Model for Redundancy Analysis Algorithm Recommendation",2021,"Proceedings of the 2021 IEEE 18th India Council International Conference, INDICON 2021",,,,"","",,,"10.1109/INDICON52576.2021.9691578","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126396470&doi=10.1109%2fINDICON52576.2021.9691578&partnerID=40&md5=fe12cdae2651251837893b042b7b1b91","Manufacturing errors, external impurities or faulty deposition during chip fabrication could generate chips with faulty memory cells, rendering the chip unusable. To repair these faulty memory cells, redundancies are included in the memory in the form of spare rows and columns. The process of mapping faulty lines to redundant cells is Redundancy Analysis. Applying a uniform Redundancy Analysis algorithm on the wafers or running algorithms sequentially one after the other would either compromise on the repair time or wafer yield. An end-to-end solution for memory repair is proposed in this paper. A clustering algorithm to classify, identify and extract features from chip errors on a wafer is proposed. These features along with other derived parameters are used as an input to the neural network recommender system to select algorithms allowing an increase in the wafer yield keeping a low repair time per wafer. We have performed comparisons of the generated result with and without clustering and with other methods of classification of chips for Redundancy Analysis algorithm selection such as Decision Trees. Experimental results demonstrate that this solution out-performs the heuristic algorithmic solutions by 9.1% and 32.9% in terms of yield for medium and high error rates. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85126396470
"Al-Tirawi A., Reynolds R.G., Senior","57207737424;7401830067;","How to Design a Trustable Cultural Algorithm Using Common Value Auctions",2021,"Proceedings - 3rd International Conference on Transdisciplinary AI, TransAI 2021",,,,"74","84",,,"10.1109/TransAI51903.2021.00022","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85126203746&doi=10.1109%2fTransAI51903.2021.00022&partnerID=40&md5=d66fb9054ad62e01de291c1e532d93f1","One of the major challenges facing Artificial Intelligence in the future is the design of trustworthy algorithms. In this paper four basic features of trustworthy algorithms are presented. A Cultural Algorithm based upon Common Value Auctions is presented. It is demonstrated that this framework is able to support each of these fundamental principles. The basic principles are: fairness, explainability, responsibility, and sustainability. The first three are features that are part of the Cultural Algorithm configuration used here. The fourth properties was established experimentally. It was shown that the CVA based Cultural Algorithm exhibited improved sustainability in terms of both resilience and robustness over the of a Cultural Algorithm based upon a Wisdom of the Crowds or voting approach.. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85126203746
"Dos Santos Silva L.N., Silva L.C., Zagatti F.R., Sette B.S., De Medeiros Caseli H., Lucredio D., Silva D.F.","57478442400;57286791700;57478860200;57285413600;57224206297;55912459600;55585876200;","CurL-AutoML: Curriculum Learning-based AutoML",2021,"Proceedings - 20th IEEE International Conference on Machine Learning and Applications, ICMLA 2021",,,,"766","771",,,"10.1109/ICMLA52953.2021.00127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125840522&doi=10.1109%2fICMLA52953.2021.00127&partnerID=40&md5=f00a51b28b9d7e8216beb3e072698df9","AutoML aims to find the best Machine Learning (ML) pipeline in a complex and high-dimensional search space by evaluating multiple algorithm configurations. However, training multiple ML algorithms is time-consuming, and as AutoML tools are frequently time-constrained, the exploration of the search space may find sub-optimal results. In this work, we explore the application of curriculum learning techniques to overcome this limitation. Curriculum and anti-curriculum learning have improved model performance and accelerated the training process on previous empirical investigations using optimization-based models by ordering examples during model training based on their difficulty. We apply and compare curriculum strategies on an AutoML system to accelerate the search space exploration and find good-performing machine learning pipelines efficiently. The results indicate that AutoML can benefit from a curriculum strategy. Furthermore, in most of the evaluated scenarios, the curriculum strategies led to better classification results. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85125840522
"Prager R.P., Seiler M.V., Trautmann H., Kerschke P.","57211237852;57219109157;23974957500;56336853600;","Towards Feature-Free Automated Algorithm Selection for Single-Objective Continuous Black-Box Optimization",2021,"2021 IEEE Symposium Series on Computational Intelligence, SSCI 2021 - Proceedings",,,,"","",,1,"10.1109/SSCI50451.2021.9660174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125816674&doi=10.1109%2fSSCI50451.2021.9660174&partnerID=40&md5=6d7ecab1adf6dd2fc73391a4c0ed93b8","We propose a novel method for automated algorithm selection in the domain of single-objective continuous black-box optimization. In contrast to existing methods, we use convolutional neural networks as the selection apparatus which bases its decision on a so-called 'fitness map'. This fitness map is a 2D representation of a two dimensional search space where different gray scales indicate the quality of found solutions in certain areas. Our devised approach uses a modular CMA-ES framework which offers the option to create the conventional CMA-ES, CMA-ES with the alternate step-size adaptation and many other variants proposed over the years. In total, 4 608 different configurations are possible where most configurations are of complementary nature. In this proof-of-concept work, we consider a subset of 32 possible configurations. The developed method is evaluated against an excerpt of BBOB functions and its performance is compared against baselines that are commonly used in automated algorithm selection - the best standalone algorithm (configuration) and the best obtainable sequence of configurations. While the results indicate that the use of the fitness map is not superior on every benchmark problem, it indubitably shows its merit on more hard-to-solve problems. This offers a promising perspective for generalizing to other types of optimization problems and problem domains. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85125816674
"Rajapaksha N., Jayasinghe S., Enshaei H., Jayarathne N.","57478046700;35793741000;55110438100;57202289702;","Supervised Machine Learning Algorithm Selection for Condition Monitoring of Induction Motors",2021,"2021 IEEE Southern Power Electronics Conference, SPEC 2021",,,,"","",,,"10.1109/SPEC52827.2021.9709436","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125785285&doi=10.1109%2fSPEC52827.2021.9709436&partnerID=40&md5=25d5b4de6f074e3526dfebe3e5e1e833","Three-phase induction motors (IMs) are one of the most employed electric machines in industrial and household applications. Condition monitoring of these machines is essential to avoid unplanned maintenance and thereby enhance the availability. Artificial Intelligence (AI) technologies are emerging as an advanced tool for automating condition monitoring process to detect incipient faults at early stages. Machine Learning (ML) algorithms have been identified as a promising approach for condition monitoring of IMs and predicting maintenance to avoid failures. However, selecting the suitable ML algorithm for a given application is challenging because there is no predefined set of application-based algorithms. In addition, raw data processing and feature selection need careful attention to improve the accuracy of the results. This paper reviews supervised ML algorithms that can be used for condition monitoring of IMs and identifies their benefits and drawbacks. It then discusses how the dominant features from raw data can be selected through time domain and frequency domain analysis using the acoustic data collected from a three-phase induction motor. The study investigates classification accuracy of each ML algorithm and a procedure for selecting an algorithm based on the experimental results. Results of this study show that Support Vector Machines (SVM) algorithm outperforms other competing algorithms in condition monitoring of IMs when the dominant frequency components obtained through Fast Fourier Transform (FFT) are used as training data. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85125785285
"Anastacio M.","55957884700;","Greybox Automated Algorithm Configuration",2021,"IJCAI International Joint Conference on Artificial Intelligence",,,,"4875","4876",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125452799&partnerID=40&md5=5a7f0db8ac2ac29f5c41b1dcc8b5bb04","The performance of state-of-the-art algorithms is highly dependent on their parameter values, and choosing the right configuration can make the difference between solving a problem in a few minutes or hours. Automated algorithm configurators have shown their efficiency on a wide range of applications. However, they still encounter limitations when confronted to a large number of parameters to tune or long algorithm running time. We believe that there is untapped knowledge that can be gathered from the elements of the configuration problem, such as the default value in the configuration space, the source code of the algorithm, and the distribution of the problem instances at hand. We aim at utilising this knowledge to improve algorithm configurators. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85125452799
"Elshawi R., Lekunze H., Sakr S.","56507337800;57468528700;25031688900;","CSmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Clustering",2021,"Proceedings - 2021 IEEE International Conference on Big Data, Big Data 2021",,,,"1119","1126",,,"10.1109/BigData52589.2021.9671542","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125345192&doi=10.1109%2fBigData52589.2021.9671542&partnerID=40&md5=44e34399cbacab29944d1680bd57e834","Novel technologies in automated machine learning ease the complexity of algorithm selection and hyper-parameter optimization. However, these are usually restricted to supervised learning tasks such as classification and regression, while unsupervised learning remains a largely unexplored problem. In this paper, we offer a solution for automating machine learning specifically for the case of unsupervised learning with clustering, in a domain-agnostic manner. This is achieved through a combination of state-of-the-art processes based on meta-learning for algorithm and evaluation criteria selection, and evolutionary algorithm for hyper-parameter tuning. We introduce a robust and scalable interactive tool, named cSmartML, built on scikit-learn with 8 clustering algorithms. In order to capture more than a single measure of goodness of the output clustering solution, cSmartML optimizes multiple objective functions. A pareto-approach evaluates each objective simultaneously for each clustering solution. On each of the 27 real and synthetic benchmark datasets, we show that the performance of cSmartML is often much better than using standard selection and hyper-parameter optimization methods. In addition, experimentation reveals that cSmartML takes advantage of the defined objective functions on multi-objective functions framework. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85125345192
"Tang Y.-P., Huang S.-J.","57218924302;7405422492;","Dual Active Learning for Both Model and Data Selection",2021,"IJCAI International Joint Conference on Artificial Intelligence",,,,"3052","3058",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125261298&partnerID=40&md5=74ec185e37a9e593c0d4804260b2dcc0","To learn an effective model with less training examples, existing active learning methods typically assume that there is a given target model, and try to fit it by selecting the most informative examples. However, it is less likely to determine the best target model in prior, and thus may get suboptimal performance even if the data is perfectly selected. To tackle with this practical challenge, this paper proposes a novel framework of dual active learning (DUAL) to simultaneously perform model search and data selection. Specifically, an effective method with truncated importance sampling is proposed for Combined Algorithm Selection and Hyperparameter optimization (CASH), which mitigates the model evaluation bias on the labeled data. Further, we propose an active query strategy to label the most valuable examples. The strategy on one hand favors discriminative data to help CASH search the best model, and on the other hand prefers informative examples to accelerate the convergence of winner models. Extensive experiments are conducted on 12 openML datasets. The results demonstrate the proposed method can effectively learn a superior model with less labeled examples. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85125261298
"Cohen-Shapira N., Rokach L.","57211939999;9276243500;","TRIO: Task-agnostic dataset representation optimized for automatic algorithm selection",2021,"Proceedings - IEEE International Conference on Data Mining, ICDM","2021-December",,,"81","90",,1,"10.1109/ICDM51629.2021.00018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125187839&doi=10.1109%2fICDM51629.2021.00018&partnerID=40&md5=c8db3ddcf668bf907da21aec5c487c80","With the growing number of machine learning (ML) algorithms, the selection of the top-performing algorithms for a given dataset, task, and evaluation measure is known to be a challenging task. The human expertise required for this task has fueled the demand for automatic solutions. Meta-learning is a popular approach for automatic algorithm selection based on dataset characterization. Existing meta-learning methods often represent the datasets using predefined features and thus cannot be generalized for various ML tasks, or alternatively, learn their representations in a supervised fashion, and thus cannot address unsupervised tasks. In this study, we first propose a novel learning-based task-agnostic method for dataset representation. Second, we present TRIO, a meta-learning approach based on the proposed dataset representation, which is capable of accurately recommending top-performing algorithms for unseen datasets. TRIO first learns graphical representations from the datasets and then utilizes a graph convolutional neural network technique to extract their latent representations. An extensive evaluation on 337 datasets and 195 ML algorithms demonstrates the effectiveness of our approach over state-of-the-art methods for algorithm selection for both supervised (classification and regression) and unsupervised (clustering) tasks. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85125187839
"Al-Jawad A., Comsa I.-S., Shah P., Gemikonakli O., Trestian R.","57194483379;54683690700;15849501400;6602247439;35868237900;","REDO: A Reinforcement Learning-based Dynamic Routing Algorithm Selection Method for SDN",2021,"2021 IEEE Conference on Network Function Virtualization and Software Defined Networks, NFV-SDN 2021 - Proceedings",,,,"54","59",,2,"10.1109/NFV-SDN53031.2021.9665140","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85125008752&doi=10.1109%2fNFV-SDN53031.2021.9665140&partnerID=40&md5=e235784ebe26db052e036f9df3244d2d","The current increase in the Internet traffic along with the global crisis have accelerated the roll-out of the next generation 5G network and key enabling technologies. In this context, addressing the end-To-end Quality of Service (QoS) provisioning in order to guarantee a sustainable service delivery to the end-users became of paramount importance. Some of the enabling technologies that could play a key role in this regard are Software Defined Network (SDN) and Machine Learning (ML). This paper proposes REDO, a Reinforcement lEarning-based Dynamic rOuting algorithm selection method that decides on the conventional routing algorithm to be applied on the traffic flows within a SDN environment. REDO will dynamically select the most appropriate routing algorithm from a set of centralized routing algorithms (MHA, WSP, SWP, MIRA) that maximizes the reward function from the network. The proposed REDO solution is implemented and evaluated using an experimental setup based on Mininet, Floodlight controller and Open vSwitch switches. The results show that REDO outperforms other state-of-The-Art solutions. © 2021 IEEE.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85125008752
"Sala R., Corona M., Pirola F., Pezzotta G.","57192373140;57454238600;55811909700;36503017300;","The Machine Learning Algorithm Selection Model: test with multiple datasets",2021,"Proceedings of the Summer School Francesco Turco",,,,"","",7,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124653965&partnerID=40&md5=758363a91c705e96e612678fe67ed85b","The technological revolution known as Industry 4.0 is permeating and changing the way companies of all sizes manage their processes. The revolution is influencing companies process at all levels, including production, service, and management ones. Not surprisingly, the strong digitalisation currently occurring in the industrial scenario is contributing to the generation of unprecedented quantities of data that companies can exploit for several purposes and scopes. New data analysis approaches, able to exploit the computational power of modern PCs and workstations are being studied by researchers and practitioners to identify patterns and generate knowledge from data. Yet, despite being able to collect increasing quantities of data, many companies still lack the capabilities and competencies to use analytic approaches such as Machine Learning (ML), elaborate data into information and, thus, generate value. A model, namely the Machine Learning Algorithm Selection Model (MLASM), has been proposed to guide the unexperienced users in selecting a set of ML algorithms suitable for their analysis according to the scope of the analysis and the characteristics of the dataset. This paper describes the process used to test the MLASM with several datasets to verify its usefulness and the correctness of its suggestions. In accordance with the results, improvements and updates have been proposed for the MLASM. © 2021, AIDI - Italian Association of Industrial Operations Professors. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85124653965
"Vieira C., de Araújo A., Andrade Júnior J.E., Bezerra L.C.T.","57207775925;7202092369;57387070800;37078903900;","iSklearn: Automated Machine Learning with irace",2021,"2021 IEEE Congress on Evolutionary Computation, CEC 2021 - Proceedings",,,,"2354","2361",,,"10.1109/CEC45853.2021.9504696","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124604362&doi=10.1109%2fCEC45853.2021.9504696&partnerID=40&md5=97e9dfb2e99b4b3ab6ed54ee3fe8b443","Automated algorithm engineering has become an important asset for academia and industry. irace, for instance, is an algorithm configurator (AC) that has successfully designed effective algorithms for optimization problems. The major advantage of irace is combining learning and parallelization, but no fully-functional automated machine learning (AutoML) system powered by irace has yet been proposed. This is rather striking, as some of the most relevant existing AutoML tools are powered by ACs, of which irace is one of the most effective examples. In this work, we propose iSklearn, an irace-powered AutoML system. Our proposal improves existing work applying an AC to engineer a machine learning (ML) pipeline. First, our configuration space represents a minimalist pipeline template, demonstrating that simpler pipelines can be competitive with elaborate approaches (e.g. ensembles). Second, our configuration setup improves the application of AC-based AutoML to time series (TS) problems, and is more flexible to fit other applications. We evaluate iSklearn on three major ML domains, namely computer vision (CV), natural language processing (NLP), and TS. Results prove competitive to AUTOSKLEARN, a state-of-the-art AutoML system also built on scikit-learn. Furthermore, the compositions of the pipelines devised vary with the problem domain and dataset considered, providing further evidence for the need of AutoML tools. We conclude our investigation ablating through the proposed configuration space and setup to understand their impact on the performance of iSklearn. © 2021 IEEE",Conference Paper,"Final","",Scopus,2-s2.0-85124604362
"Nguyen D.A., Kong J., Wang H., Menzel S., Sendhoff B., Kononova A.V., Back T.","57211820911;57211822686;56313894000;36974904200;35619676800;8729046100;57203312747;","Improved Automated CASH Optimization with Tree Parzen Estimators for Class Imbalance Problems",2021,"2021 IEEE 8th International Conference on Data Science and Advanced Analytics, DSAA 2021",,,,"","",,3,"10.1109/DSAA53316.2021.9564147","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124329076&doi=10.1109%2fDSAA53316.2021.9564147&partnerID=40&md5=fcad433707deefd9f9df77a941806481","The imbalanced classification problem is very relevant in both academic and industrial applications. The task of finding the best machine learning model to use for a specific imbalanced dataset is complicated due to a large number of existing algorithms, each with its own hyperparameters. The Combined Algorithm Selection and Hyperparameter optimization (CASH) has been introduced to tackle both aspects at the same time. However, CASH has not been studied in detail in the class imbalance domain, where the best combination of resampling technique and classification algorithm is searched for, together with their optimized hyperparameters. Thus, we target the CASH problem for imbalanced classification. We experiment with a search space of 5 classification algorithms, 21 resampling approaches and 64 relevant hyperparameters in total. Moreover, we investigate performance of 2 well-known optimization approaches: Random search and Tree Parzen Estimators approach which is a kind of Bayesian optimization. For comparison, we also perform grid search on all combinations of resampling techniques and classification algorithms with their default hyperparameters. Our experimental results show that a Bayesian optimization approach outperforms the other approaches for CASH in this application domain. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85124329076
"Chen J., Chen E., Wang X., Zhao S., Zhang Y.","53363286500;57445840600;57212381905;34969832100;35303881100;","Deep collaborative filtering based on user's long and short intention for Recommendation",2021,"Proceedings - 2021 6th International Symposium on Computer and Information Processing Technology, ISCIPT 2021",,,,"294","300",,,"10.1109/ISCIPT53667.2021.00066","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124265750&doi=10.1109%2fISCIPT53667.2021.00066&partnerID=40&md5=594f3258f760f8438946a69904be75bf","The user-based collaborative filtering (UCF) algorithms have been widely used in recommender systems. Existing UCF algorithms recommend based on the user's interaction history, but they rarely consider the time trajectory information of user behavior. Actually, the user's intention will change with time. Generally, the user's long-term intentions change slowly over time, while short-term intentions change drastically. How to dynamic recommendation introducing nonlinearity and assigning different weights to user's long-term and short-term intentions is currently a challenge. Therefore, in this paper, we propose a deep collaborative filtering recommendation algorithm based on user's long-term and short-term intentions, short for DeepLS. Firstly, the time-based interaction sequence utilizes a self-attention mechanism to mine the user's short-term intention. Secondly, representation learning-based methods map users and items into a common representation space and model the user's long-term intention. Then, the two intentions are fused to jointly learn the representation of users and items using concatenation or empirical scheme (ES). Finally, we send the joint representation into a fully connected layer to calculate the matching score for prediction. Experiments on three real datasets show that our proposed DeepLS framework better than the state-of-the-art methods. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85124265750
"Panchapakesan A., Abielmona R., Petriu E.","55822456000;36885090100;35428404800;","Improving shipping container damage claims prediction through level 4 information fusion",2021,"International Journal of Logistics Systems and Management","40","4",,"489","509",,1,"10.1504/IJLSM.2021.120530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85124088576&doi=10.1504%2fIJLSM.2021.120530&partnerID=40&md5=c5c75f73875135f8172a02e79d7dc405","Maritime trade accounts for approximately 90% of global trade, and most global supply chains incorporate some form of maritime travel (Cheraghchi et al., 2017). Optimising operations at commercial maritime ports is therefore of significant importance worldwide, and impacts global trade. While damage to vessels and cargo has been studied extensively, as has optimising portside operational efficiency, investigations of damage to shipping containers themselves and the resultant disruption of port-side efficiency remains unstudied. The application of machine learning (ML) techniques to uncover causes of shipping container damage allows for more efficient handling of the service-disruptions they cause, as well as insights into the veracity of the current wisdom held by domain experts in the industry. Further, the application of ML methodologies for dynamic algorithm selection (per level 4 of the JDL/DFIG data fusion model) allows for significant improvements to the overall performance of the aforementioned ML methodologies. Copyright © 2021 Inderscience Enterprises Ltd.",Article,"Final","",Scopus,2-s2.0-85124088576
"Dalla M., Visentin A., O'Sullivan B.","57222380791;57191283187;7103413670;","Automated SAT Problem Feature Extraction using Convolutional Autoencoders",2021,"Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI","2021-November",,,"232","239",,,"10.1109/ICTAI52525.2021.00039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123938041&doi=10.1109%2fICTAI52525.2021.00039&partnerID=40&md5=53c124cc1a29c4dc0f9fde4b661dea5d","The Boolean Satisfiability Problem (SAT) was the first known NP-complete problem and has a very broad literature focusing on it. It has been applied successfully to various real-world problems, such as scheduling, planning and cryptography. SAT problem feature extraction plays an essential role in this field. SAT solvers are complex, fine-tuned systems that exploit problem structure. The ability to represent/encode a large SAT problem using a compact set of features has broad practical use in instance classification, algorithm portfolios, and solver configuration. The performance of these techniques relies on the ability of feature extraction to convey helpful information. Researchers often craft these features ""by hand""to capture particular structures of the problem. Instead, in this paper, we extract features using semi-supervised deep learning. We train a convolutional autoencoder (AE) to compress the SAT problem into a limited latent space and reconstruct it minimizing the reconstruction error. The latent space projection should preserve much of the structural features of the problem. We compare our approach to a set of features commonly used for algorithm selection. Firstly, we train classifiers on the projection to predict if the problems are satisfiable or not. If the compression conveys valuable information, a classifier should be able to take correct decisions. In the second experiment, we check if the classifiers can identify the original problem that was encoded as SAT. The empirical analysis shows that the autoencoder is able to represent problem features in a limited latent space efficiently, as well as convey more information than current feature extraction methods. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85123938041
"Dos Santos R., Aguilar J., Puerto E.","57162329600;55434810600;39861895500;","A Meta-Learning Architecture based on Linked Data",2021,"Proceedings - 2021 47th Latin American Computing Conference, CLEI 2021",,,,"","",,,"10.1109/CLEI53233.2021.9640223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123867384&doi=10.1109%2fCLEI53233.2021.9640223&partnerID=40&md5=43c107c785b0f4490c615cab2b688cc8","In Machine Learning (ML), there is a lot of research that seek to automate specific processes carried out by data scientists in the generation of knowledge models (predictive, classification, clustering, etc.); however, an open problem is to find mechanisms that allow conferring the ability of self-learning. Thus, a meta-learning mechanism is required to allow ML techniques to self-adapt in order to improve their performance in problem solving, and even in some cases, to induce the learning algorithm itself. In this context, our research defines a meta-learning architecture using Linked Data (LD) for the automatic generation of knowledge models. Specifically, this intelligent architecture is formed by the layers of Knowledge Sources, Meta-Knowledge and Knowledge Modelling, to unify all processes to guarantee a Meta-Learning process. The Knowledge Sources layer is responsible for providing semantic knowledge about the processes of generation of knowledge models; the Meta-Knowledge layer is responsible for controlling the different processes and strategies for the automatic generation of knowledge models; and finally, the Knowledge Modelling layer is responsible for executing ML tasks defined by the Meta-Knowledge layer, among which are the tasks of feature engineering, ML algorithm configuration, model building, among others. Additionally, this article presents a case study to analyze the behavior of the different layers of the architecture, to generate knowledge models. Thus, the main contribution of this research is the definition of a Meta-Learning architecture for ML techniques, which takes advantage of the semantic information described as LD when generating the knowledge models. The preliminary results are very encouraging ©2021 IEEE",Conference Paper,"Final","",Scopus,2-s2.0-85123867384
"Kabir P.B., Akter S.","57433055900;57217740126;","Emphasised Research on Heart Disease Divination Applying Tree Based Algorithms and Feature Selection",2021,"Proceedings of the 2021 IEEE International Conference on Innovative Computing, Intelligent Communication and Smart Electrical Systems, ICSES 2021",,,,"","",,,"10.1109/ICSES52305.2021.9633870","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123760935&doi=10.1109%2fICSES52305.2021.9633870&partnerID=40&md5=2fbec65c845889ad5c0999cd5e9ae61f","Heart disease has evolved to become the most deadly ailment on the earth, and it has been the top reason for mortality worldwide. As a result, a dependable, efficient, and practical method for diagnosing and treating such disorders promptly is required. This study examines and compares several Machine Learning (ML) algorithms and approaches. Six ML classifiers are tested to see which one's the most successful at diagnosing heart disease. Tree-based techniques are among the most basic and extensively used ensemble learning approaches. According to the analysis, tree-based models such as Decision Tree (DT) and Random Forest (RF) deliver actionable insights with high efficacy, uniformity, and applicability. Relevant features are identified by using the Feature Selection (FS) process, and the output of classifiers is calculated based on these features. FS removes irrelevant features without impacting learning output. Our research intends to improve the system's efficiency. The goal of this research is to combine FS with tree-based algorithms to improve the accuracy of heart disease prediction. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85123760935
"Fang Y., Pu J., Zhou H., Liu S., Cao Y., Liang Y.","57191752339;7102908964;57427241700;55523728200;57426828400;57426828500;","Attitude Control Based Autonomous Underwater Vehicle Multi-mission Motion Control with Deep Reinforcement Learning",2021,"2021 5th International Conference on Automation, Control and Robots, ICACR 2021",,,,"120","129",,1,"10.1109/ICACR53472.2021.9605171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123485461&doi=10.1109%2fICACR53472.2021.9605171&partnerID=40&md5=e8e18ffc92bb8fdfc48625e30051fab9","Due to the limitations of classical control strategies for underwater vehicles, the artificial intelligent control technologies have attracted more attention than ever, especially the Deep Reinforcement Learning. However, it is usually confusing to choose a proper Deep Reinforcement Learning algorithm and to configurate an effective reward function according to different autonomous underwater vehicle assignments. To solve the problem, this research explores three different missions that autonomous underwater vehicle might perform, with two Deep Reinforcement Learning algorithms and three reward functions adopted respectively. Deep Reinforcement Learning controllers take the attitude sensor information as input, the control signals of X-rudder blades as output. The simulation experiments results are compared and analyzed, which has an important reference value for the reward function setting and Deep Reinforcement Learning algorithm selection for different autonomous underwater vehicle control tasks. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85123485461
"Kouassi K., Moodley D.","57214699932;8601861800;","Automated deep learning for trend prediction in time series data",2021,"Proceedings of 2021 IEEE 24th International Conference on Information Fusion, FUSION 2021",,,,"","",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123421979&partnerID=40&md5=7883ea87cdedaa15226b9ddafe7e97e2","Recently, Deep Neural Network (DNN) algorithms have been explored for predicting trends in time series data. In many real world applications time series data are captured from dynamic systems, which change over time. DNN models must provide stable performance when they are updated and retrained as new observations becomes available. In this work we explore the use of automated machine learning techniques to automate the algorithm selection and hyperparameter optimisation process for trend prediction with DNNs. We demonstrate how a recent AutoML tool, specifically the HpBandSter framework, can be effectively used to automate DNN model development. Our AutoML experiments found optimal configurations that produced models that compared well against the average performance and stability levels of configurations found from manual experiments across four datasets. © 2021 International Society of Information Fusion (ISIF).",Conference Paper,"Final","",Scopus,2-s2.0-85123421979
"Zeebaree D.Q., Hasan D.A., Abdulazeez A.M., Ahmed F.Y.H., Hasan R.T.","57204890026;57212683352;57202201131;36197845900;57424121000;","Machine Learning Semi-Supervised Algorithms for Gene Selection: A Review",2021,"2021 IEEE 11th International Conference on System Engineering and Technology, ICSET 2021 - Proceedings",,,,"165","170",,1,"10.1109/ICSET53708.2021.9612526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123356224&doi=10.1109%2fICSET53708.2021.9612526&partnerID=40&md5=34e96132df51ae6a637d41951a2ce54d","Machine learning and data mining have established several effective applications in gene selection analysis. This paper review semi-supervised learning algorithms and gene selection. Semi-Supervised learning is learning that includes experiences that are familiar with the environment because it can deal with labelled and unnamed data. Gene selection is dimension reduction defined as the discovery process of the perfect selection of attributes comprising the whole collected dataset. We review many previous studies on gene selection in semi-supervised learning where each previous research paper tests a group of algorithms to select a gene on a specific set of selected medical data. Each study proposes its algorithm and compares it with previous existing algorithms and compares their accuracy. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85123356224
"Wang X.-Y., Chen N.-N., Zhu B.-H.","57208249536;57421672600;57420914500;","Research and Application of Garbage Foreign Object Recognition Algorithm",2021,"Proceedings of SPIE - The International Society for Optical Engineering","12079",,"1207908","","",,,"10.1117/12.2622736","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123240690&doi=10.1117%2f12.2622736&partnerID=40&md5=263f2ce5b62bf0b5840258d122505471","Aiming at the difficulties and pain points of garbage quality identification in the transfer process, technical demonstrations are mainly carried out from the aspects of image preprocessing, deep learning algorithm selection, and model training. A model of dry and wet garbage abnormalities in complex scenarios was established. The specific garbage image preprocessing method, deep learning algorithm and model training technology used in this project are determined. A set of intelligent identification system for automatically detecting the quality of garbage classification has been developed, and all the identification indicators of the system are better than expected. © 2021 SPIE",Conference Paper,"Final","",Scopus,2-s2.0-85123240690
"Deshpande N., Sharma N., Yu Q., Krutz D.E.","57219160269;57213712442;8355184200;55616807100;","R-CASS: Using Algorithm Selection for Self-Adaptive Service Oriented Systems",2021,"Proceedings - 2021 IEEE International Conference on Web Services, ICWS 2021",,,,"61","72",,,"10.1109/ICWS53863.2021.00021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85123161511&doi=10.1109%2fICWS53863.2021.00021&partnerID=40&md5=37f7f1b17ae4b80c6f12601933810e2c","In service composition, complex applications are built by combining web services to fulfill user Quality of Service (QoS) and business requirements. To meet these requirements, applications are composed by evaluating all possible web service combinations using search algorithms. These algorithms need to be accurate and inexpensive to evaluate a large number of possible service combinations and services' fluctuating QoS attributes while meeting the constraints of limited computational resources. Recent research has shown that different search algorithms can outperform others on specific instances of a problem domain, in terms of solution quality and computational resource usage. Problematically, current service composition approaches ignore this property, leading to inefficient compositions. To address these limitations, we propose a composition algorithm selection framework which selects an algorithm per composition task at runtime, R-CASS. Our evaluations demonstrate that R-CASS leads to more efficient compositions, reducing composition time by 55.1% and memory by 37.5%. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85123161511
"Stalin S., Roy V., Shukla P.K., Zaguia A., Khan M.M., Shukla P.K., Jain A.","57188753376;57197479820;57190738806;41662526200;36350785300;56599752300;57218216736;","A Machine Learning-Based Big EEG Data Artifact Detection and Wavelet-Based Removal: An Empirical Approach",2021,"Mathematical Problems in Engineering","2021",,"2942808","","",,22,"10.1155/2021/2942808","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122732853&doi=10.1155%2f2021%2f2942808&partnerID=40&md5=b4b50c2fdf8a884240b1cc586d751ef4","The electroencephalogram (EEG) signals are a big data which are frequently corrupted by motion artifacts. As human neural diseases, diagnosis and analysis need a robust neurological signal. Consequently, the EEG artifacts' eradication is a vital step. In this research paper, the primary motion artifact is detected from a single-channel EEG signal using support vector machine (SVM) and preceded with further artifacts' suppression. The signal features' abstraction and further detection are done through ensemble empirical mode decomposition (EEMD) algorithm. Moreover, canonical correlation analysis (CCA) filtering approach is applied for motion artifact removal. Finally, leftover motion artifacts' unpredictability is removed by applying wavelet transform (WT) algorithm. Finally, results are optimized by using Harris hawks optimization (HHO) algorithm. The results of the assessment confirm that the algorithm recommended is superior to the algorithms currently in use. © 2021 Shalini Stalin et al.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85122732853
"Ansótegui C., Sellmann M., Shah T., Tierney K.","6603338937;15045576600;57221839846;55361261500;","Learning to Optimize Black-Box Functions with Extreme Limits on the Number of Function Evaluations",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12931 LNCS",,,"7","24",,,"10.1007/978-3-030-92121-7_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85121902122&doi=10.1007%2f978-3-030-92121-7_2&partnerID=40&md5=531a42569a580499b93d95e1f56c0b7f","We consider black-box optimization in which only an extremely limited number of function evaluations, on the order of around 100, are affordable and the function evaluations must be performed in even fewer batches of a limited number of parallel trials. This is a typical scenario when optimizing variable settings that are very costly to evaluate, for example in the context of simulation-based optimization or machine learning hyperparameterization. We propose an original method that uses established approaches to propose a set of points for each batch and then down-selects from these candidate points to the number of trials that can be run in parallel. The key novelty of our approach lies in the introduction of a hyperparameterized method for down-selecting the number of candidates to the allowed batch-size, which is optimized offline using automated algorithm configuration. We tune this method for black box optimization and then evaluate on classical black box optimization benchmarks. Our results show that it is possible to learn how to combine evaluation points suggested by highly diverse black box optimization methods conditioned on the progress of the optimization. Compared with the state of the art in black box minimization and various other methods specifically geared towards few-shot minimization, we achieve an average reduction of 50% of normalized cost, which is a highly significant improvement in performance. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85121902122
"Pan X., Montreuil B.","57369336000;7003796719;","Assessing multi-purpose forecasting accuracy",2021,"IISE Annual Conference and Expo 2021",,,,"728","733",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120975938&partnerID=40&md5=5e640bea7cbfeb6b4c157bcce7555f1f","Forecasting is a crucial aspect of the supply chain process. Inaccurate forecasting, notably of demand, can lead to lost sales, customer dissatisfactions, inventory shortage or pileups, and millions of lost revenues and/or induced costs. Meanwhile, the performance evaluation of the demand forecast depends highly on the selection of prediction error metrics. It is common for the accuracy of the forecasting methods (models and algorithms) to be measured based on the overall performance of the testing data. However, these kinds of error metrics cannot fully unveil the properties and advantages of each forecasting algorithm. For example, methods may perform differently depending on the forecasting horizon. Sometimes in industry, different algorithms are selected based on different requirements and the process of algorithm selection is conducted multiple times, which is time-consuming. Therefore, there is a need for a criterion enabling to assess multi-dimensional and multi-purpose accuracy of forecasting methods. The main goal of this paper is to propose a multi-purpose forecasting accuracy metric that allows users to assess the accuracy of a forecasting method to be used for diverse purposes associated to alternative contexts. Purposes may differ according to combinations of punctual versus cumulative forecasts, over diverse time windows, reflecting how in advance the forecast is made and for how many periods. Empirical evidence on the value of the proposed criterion is provided through comparative analysis for demand forecasting of alternative methods based on time series models like Holt-Winters, Multiple Seasonal and Bouchard-Montreuil, and based on tree-based machine learning models like XGBoost, Random Forest and Adaboost. © 2021 IISE Annual Conference and Expo 2021. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85120975938
"Ijab M.T., Shahril M.S., Hamid S.","25960099000;55213547600;36921749100;","Infodemiology Framework for COVID-19 and Future Pandemics Using Artificial Intelligence to Address Misinformation and Disinformation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","13051 LNCS",,,"530","539",,,"10.1007/978-3-030-90235-3_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85120523573&doi=10.1007%2f978-3-030-90235-3_46&partnerID=40&md5=f9e940cf8c1c7cf468b6ae3b3cde5edb","The global fatality caused by the deadly COVID-19 already took 4.5 million lives and is still rapidly increasing. At the same time, Malaysian citizens have been inundated with the overabundance of news and information about COVID-19 since it hit the world in December 2019. Recent study by ISIS Malaysia discovered that WhatsApp and Facebook are the most used social media for misinformation at 39% and 34%, respectively. This phenomenon is termed as an infodemic which occurs when there is an excessive amount of information with undetermined level of accuracy. Hence, this situation makes it difficult for people to find reliable and truthful sources of information when they require it. Infodemiology is the scientific term used to describe the massive spread of information in a digital format particularly on the Internet which aims to guide the stakeholders such as the government on public health policy. Artificial Intelligence (AI) techniques hold potential solutions to address infodemic issue. This paper conceptualizes an Infodemiology Framework for COVID-19 and future pandemics towards addressing the proliferation of misinformation and disinformation on the Internet. Leveraging on AI techniques such as classification via clustering and decision tree algorithms, the research works will be conducted in five phases beginning with dataset collection phase, model building and algorithm selection phase, model refinement phase, model verification phase, and the model deployment phase. The proposed infodemiology framework has the potential to be integrated into the nation’s healthcare data warehousing system, the Malaysian Health Data Warehouse (MyHDW). © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85120523573
"Swaid S.I., Suid T.Z.","55314031500;57202830964;","Celebrating Design Thinking in Tech Education: The Data Science Education Case",2021,"Communications in Computer and Information Science","1498 CCIS",,,"66","70",,,"10.1007/978-3-030-90176-9_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85119889443&doi=10.1007%2f978-3-030-90176-9_10&partnerID=40&md5=7da00fb87d85ba36effd567e115e73c7","Today, corporates are moving toward the adoption of Design-Thinking techniques to develop products and services, putting their consumer as the heart of the development process. Tim Brown, president, and CEO of IDEO, defines design thinking as “A human-centered approach to innovation that draws from the designer’s toolkit to integrate the needs of people, the possibilities of technology, and the requirements for business success”. The application of design thinking has been witnessed to be the road to develop innovative applications, interactive systems, scientific software, healthcare application, and even to utilize Design Thinking to re-think business operation as the case of Airbnb. Recently, there has been a movement to apply design thinking to machine learning and artificial intelligence to ensure creating the “waw” affect to consumers. ACM Taskforce on Data Science program states that “Data Scientists should be able to implement and understand algorithms for data collection and analysis. They should understand the time and space considerations of algorithms. They should follow good design principles developing software, understanding the importance of those principles for testability and maintainability” However, this definition hides the user behind the machine who works on data preparation, algorithm selection and model interpretation. Thus, Data Science program to include design thinking to ensure meeting the user demands, generating more usable machine learning tools, and developing new ways of framing computational thinking. In this poster, we describe the motivation behind injecting DT in Data Science programs, an example course, its learning objective and teaching modules. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85119889443
"Jiang D., Lin W., Raghavan N.","57221868867;57222115401;55663303300;","Semiconductor Manufacturing Final Test Yield Optimization and Wafer Acceptance Test Parameter Inverse Design Using Multi-Objective Optimization Algorithms",2021,"IEEE Access","9",,,"137655","137666",,,"10.1109/ACCESS.2021.3117576","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117166706&doi=10.1109%2fACCESS.2021.3117576&partnerID=40&md5=4621373de53dbda5dd8892d5cdc1d0fc","In the semiconductor industry, many previous optimization studies have been carried out at the integrated circuit front-end design phase to identify optimal circuit elements' size and design parameters. With the scaling of device dimensions, semiconductor manufacturing back-end Final Test (FT) yield is increasingly influenced by systematic or random process variations. As a result, the FT yield is not fully guaranteed by design phase optimization due to existing limitations of yield simulation models and coverage. Very few studies have attempted to incorporate the production FT yield data into process variation optimization and inverse design. In this paper, we introduce a novel framework for FT yield optimization and Wafer Acceptance Test (WAT) parameter inverse design using multi-objective optimization algorithms. This provides a solution to monitor and quickly adjust process variations to maximize FT yield without expensive characterization or design correction after products are released into the production phase. Both yield optimization and process shift feasibility are to be taken into consideration by formulating these factors into a two objective optimization problem. One objective is to minimize FT yield loss, wherein the yield loss is predicted by a machine learning model. The other objective is to minimize the total WAT parameters shift distance from the current standard setting. Three widely used multi-objective algorithms NSGA-II, SMPSO and GDE3 are applied, compared and discussed. An automatic parameter tuning approach using Sequential Model-based Algorithm Configuration (SMAC) and entropy-based termination criterion is applied to reduce the execution time whilst maintaining the optimization algorithms' performance. Real production data for CMOS 55nm chip are used to validate the framework and the results point to the effectiveness of yield improvement and accurate identification of the optimal WAT parameter combination. © 2013 IEEE.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85117166706
"SEAVER N.","54882684800;","CARE AND SCALE: Decorrelative Ethics in Algorithmic Recommendation",2021,"Cultural Anthropology","36","3",,"509","537",,3,"10.14506/ca36.3.11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85115785448&doi=10.14506%2fca36.3.11&partnerID=40&md5=3459d74382d1bba3ab36540e04693393","The people who make algorithmic recommender systems want apparently incompatible things: they pride themselves on the scale at which their software works, but they also want to treat their materials and users with care. Care and scale are commonly understood as contradictory goals: to be careful is to work at small scale, while working at large scale requires abandoning the small concerns of care. Drawing together anthropological work on care and scale, this article analyzes how people who make music recommender systems try to reconcile these values, reimagining what care and scale mean and how they relate to each other in the process. It describes decorrelation, an ethical technique that metaphorically borrows from the mathematics of machine learning, which practitioners use to reimagine how values might relate with each other. This “decorrelative ethics” facilitates new arrangements of care and scale, which challenge conventional anthropological theorizing. © 2021. All rights reserved.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85115785448
"Yi J., Zhang H., Liu H., Zhong G., Li G.","57240408500;55685621600;56184013700;56298085100;57200326930;","Flight Delay Classification Prediction Based on Stacking Algorithm",2021,"Journal of Advanced Transportation","2021",,"4292778","","",,2,"10.1155/2021/4292778","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113959035&doi=10.1155%2f2021%2f4292778&partnerID=40&md5=ebf9d245d4f9b0763395f40425faa6a6","With the development of civil aviation, the number of flights keeps increasing and the flight delay has become a serious issue and even tends to normality. This paper aims to prove that Stacking algorithm has advantages in airport flight delay prediction, especially for the algorithm selection problem of machine learning technology. In this research, the principle of the Stacking classification algorithm is introduced, the SMOTE algorithm is selected to process imbalanced datasets, and the Boruta algorithm is utilized for feature selection. There are five supervised machine learning algorithms in the first-level learner of Stacking including KNN, Random Forest, Logistic Regression, Decision Tree, and Gaussian Naive Bayes. The second-level learner is Logistic Regression. To verify the effectiveness of the proposed method, comparative experiments are carried out based on Boston Logan International Airport flight datasets from January to December 2019. Multiple indexes are used to comprehensively evaluate the prediction results, such as Accuracy, Precision, Recall, F1 Score, ROC curve, and AUC Score. The results show that the Stacking algorithm not only could improve the prediction accuracy but also maintains great stability. © 2021 Jia Yi et al.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85113959035
"Hu X., Li B., Mo Y., Alselwi O.","57226546145;25631051300;57231289400;57231522900;","Progress in artificial intelligence-based prediction of concrete performance",2021,"Journal of Advanced Concrete Technology","19","8",,"924","936",,4,"10.3151/jact.19.924","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85113475694&doi=10.3151%2fjact.19.924&partnerID=40&md5=8d7bd9ca041c80133466a77fdae8c90e","Artificial intelligence technology has super high-dimensional nonlinear computing capabilities, intelligent comprehensive analysis and judgment functions, and self-learning knowledge reserve expression functions. It can unlock the potential of high-dimensional nonlinearity relation between tangible components and performance indicators when compared to the empirical formula generated from classic statistical approaches. This article summarizes the types of artificial intelligence algorithms used to predict concrete performance, comprehensively sorts out the research progress of artificial intelligence technology in predicting the mechanical properties, work performance, and durability of concrete, and compares and analyzes the effects of algorithm selection, sample data, and model construction on the concrete compressive prediction system. The analysis shows that artificial intelligence technology has obvious advantages in measurement accuracy in predicting concrete performance compared to conventional statistical methods. Multiple algorithms should be used to cross-validate the model prediction findings. For tiny data sets, support vector machines are utilized. Decision tree evolution techniques should be used in algorithm models that require feature optimization or dispersed index prediction. Artificial neural networks can be used to solve different challenges. To improve the prediction model and boost its prediction accuracy, measures such as optimized features, integrated algorithms, hyperparameter optimization, enlarged sample data set, richer data sources, and data pretreatment are proposed. © 2021 Japan Concrete Institute. All rights reserved.",Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85113475694
"Man T., Zhukova N.A., Thaw A.M., Abbas S.A.","57209975532;56406142300;57216852269;57216351205;","A decision support system for DM algorithm selection based on module extraction",2021,"Procedia Computer Science","186",,,"529","537",,3,"10.1016/j.procs.2021.04.173","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112695631&doi=10.1016%2fj.procs.2021.04.173&partnerID=40&md5=5c67e2506a6964203101ee1f1b091734","Data mining techniques are needed in various fields. However, most data researchers do not have sufficient knowledge and experience. Due to the significant number of algorithms and parameters for data analysis, intuition-based decisions can't lead to optimal solutions. Ontology is widely adopted to build knowledge-driven decision support systems since it is suited to encapsulate the concepts and relationships of terms associated with various domains. It is suitable for capturing knowledge by computers, allowing sharing and reusing it whenever necessary. All concepts and relationships about data analysis can be described using Web Ontology Language (OWL). Its reasoning mechanism is vital in any knowledge-based system. Ontology can be reasoned to recommend suitable solutions for a specific analysis task by considering the data characteristics and task requirements. In the previous work, we have developed a system that describes comprehensive knowledge and logical internal relationships about data mining. We found that the reasoning and query complexity is high in the large size ontology, so this paper focuses on the modeling and implementation of an ontology-based decision support system for data mining algorithm selection that contains a new sub-ontology extraction method. Extracting effective content can significantly reduce the size of the ontology. This extraction method can improve query efficiency on the premise of ensuring the quality of information. © 2021 Elsevier B.V.. All rights reserved.",Conference Paper,"Final","All Open Access, Gold",Scopus,2-s2.0-85112695631
"Bryś M.","57226787234;","Classification Algorithms Applications for Information Security on the Internet: A Review",2021,"Studies in Classification, Data Analysis, and Knowledge Organization",,,,"49","63",,,"10.1007/978-3-030-75190-6_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112637135&doi=10.1007%2f978-3-030-75190-6_4&partnerID=40&md5=671caf0c28f6806357b0dddbb1f443dc","The growing use of the Internet in every life area creates an emerging need to provide information security (IS), and numerous classification algorithms approach this problem. This study provides a systematic literature review on the classification algorithms applications for information security on the Internet and cybersecurity. The classification algorithms use cases considered are abusive content, malicious code, information gathering, intrusion attempts, intrusions, availability, information content security, fraud, and vulnerable. As many research papers on that topic were published, this research focuses on recent studies from 2015 to 2020 and includes new areas, like mobile devices and the Internet of things (IoT). The analysis of 1446 selected publications provides insights on classification algorithms applied to IS tasks, their popularity, and the algorithm selection challenges. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85112637135
"Ramos M., Pereira C., Almeida L.","57226776936;56613638700;35727555700;","A First Sensitivity Study of Multi-object Multi-camera Tracking Performance",2021,"IFIP Advances in Information and Communication Technology","628",,,"269","280",,,"10.1007/978-3-030-79157-5_22","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112614046&doi=10.1007%2f978-3-030-79157-5_22&partnerID=40&md5=5166e67d69f8b2cf529aa30cc7b9938c","Computer Vision is becoming widely used for a myriad of purposes, e.g. people counting and tracking. To execute this application in real-time, a relatively complex algorithm processes intensive data streams to identify people in a visual scenario. Although such algorithms frequently run in powerful servers on the Cloud, it is also common that they have to run in local commodity computers with limited capacity. In this work we used the Multi-Camera Multi-Target algorithm of the recent OpenVINOTM toolkit to detect and track people in small retail stores. We ran the algorithm in a common personal computer and analyzed the variation of its performance for a set of different relevant scenarios and algorithm configurations, providing insights into how these affect the algorithm performance and computational cost. In the tested scenarios, the most influential factor was the number of people in the scene. The average frame processing time observed varied around 200 ms. © 2021, IFIP International Federation for Information Processing.",Conference Paper,"Final","",Scopus,2-s2.0-85112614046
"Ren J., Sathiyanarayanan V., Ewing E., Senbaslar B., Ayanian N.","57222343638;57222336627;57218717685;57222343226;24780572000;","MAPFAST: A deep algorithm selector for multi agent path finding using shortest path embeddings",2021,"Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS","2",,,"1043","1051",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112458156&partnerID=40&md5=3498135b11d5b9a6830b484864629827","Solving the Multi-Agent Path Finding (MAPF) problem optimally is known to be NP-Hard for both make-span and total arrival time minimization. While many algorithms have been developed to solve MAPF problems, there is no dominating optimal MAPF algorithm that works well in all types of problems and no standard guidelines for when to use which algorithm. In this work, we develop the deep convolutional network MAPFAST (Multi-Agent Path Finding Algorithm SelecTor), which takes a MAPF problem instance and attempts to select the fastest algorithm to use from a portfolio of algorithms. We improve the performance of our model by including single-agent shortest paths in the instance embedding given to our model and by utilizing supplemental loss functions in addition to a classification loss. We evaluate our model on a large and diverse dataset of MAPF instances, showing that it outperforms all individual algorithms in its portfolio as well as the state-of-the-art optimal MAPF algorithm selector. We also provide an analysis of algorithm behavior in our dataset to gain a deeper understanding of optimal MAPF algorithms’ strengths and weaknesses to help other researchers leverage different heuristics in algorithm designs. © 2021 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85112458156
"Lan Y.","57640760700;","Chat-oriented social engineering attack detection using attention-based Bi-LSTM and CNN",2021,"Proceedings - 2021 2nd International Conference on Computing and Data Science, CDS 2021",,,"9463230","483","487",,2,"10.1109/CDS52072.2021.00089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112381131&doi=10.1109%2fCDS52072.2021.00089&partnerID=40&md5=76f0065c37abeb963b52bb54546200ae","As more traditional businesses, such as banking and finance, are transferred to online platforms or the cloud, the deepening of system interaction with users and the improvement of technology-based defence system make cyber attackers focus more on human beings, leading to serious financial consequences. This attack utilising social engineering often exploits human nature's weakness. Its complexity, language variability and inductivity are difficult to defend effectively. Therefore, this paper proposes a model for detecting social engineering attacks based on deep neural network by reviewing current methods for social engineering detection, in terms of phishing, deception and content-based detection, in addition to examining deep learning algorithms with excellent data performance. Through the processing and analysis of natural language in chat history, the attention-based Bi-LSTM is used to capture and mine the context semantics, and the ResNet is used to integrate user characteristics and content characteristics for classification and judgment. By describing the features of social engineering attacks and online conversations, the feasibility and effectiveness of the proposed model are demonstrated from the perspective of algorithm selection and applicability. © 2021 IEEE.",Conference Paper,"Final","",Scopus,2-s2.0-85112381131
"Pimpalkhare N., Mora F., Polgreen E., Seshia S.A.","57221462311;57203278915;57190572531;56434045800;","MedleySolver: Online SMT Algorithm Selection",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12831 LNCS",,,"453","470",,2,"10.1007/978-3-030-80223-3_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112197033&doi=10.1007%2f978-3-030-80223-3_31&partnerID=40&md5=470f75f1da7791da169820e7eb64fa96","Satisfiability modulo theories (SMT) solvers implement a wide range of optimizations that are often tailored to a particular class of problems, and that differ significantly between solvers. As a result, one solver may solve a query quickly while another might be flummoxed completely. Predicting the performance of a given solver is difficult for users of SMT-driven applications, particularly when the problems they have to solve do not fall neatly into a well-understood category. In this paper, we propose an online algorithm selection framework for SMT called MedleySolver that predicts the relative performances of a set of SMT solvers on a given query, distributes time amongst the solvers, and deploys the solvers in sequence until a solution is obtained. We evaluate MedleySolver against the best available alternative, an offline learning technique, in terms of pure performance and practical usability for a typical SMT user. We find that with no prior training, MedleySolver solves 93.9% of the queries solved by the virtual best solver selector achieving 59.8% of the par-2 score of the most successful individual solver, which solves 87.3%. For comparison, the best available alternative takes longer to train than MedleySolver takes to solve our entire set of 2000 queries. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85112197033
"He M., Jin L., Song M.","57200641625;56437132800;57661934600;","Interpretability Framework of Network Security Traffic Classification Based on Machine Learning",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12737 LNCS",,,"305","320",,,"10.1007/978-3-030-78612-0_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112115091&doi=10.1007%2f978-3-030-78612-0_25&partnerID=40&md5=950c30e2d577509711e6c3a89a591065","With the increasing number of people accessing the Internet, attacks against users or web servers have become a serious threat to network security. Network traffic can record network behavior, which is an important data source for analyzing network behavior. Using machine learning algorithm to analyze network behavior is one of the effective methods. However, these methods always put the data into black boxes, which is not enough for business understanding and result reliability display. In this paper, we propose an interpretability framework of network security traffic classification and apply it on a network traffic dataset. In this work, we apply some interpretable models, including model structure-based and feature importance-based. We verify that the methods can help researchers better explain the business features of network security traffic and optimize the classification model in algorithm selection and feature selection. We also study the interpretability of network traffic on neural network and make some progress. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85112115091
"Jiang N., Tian T., Chen X., Zhang G., Pan L., Yan C., Yang G., Wang L., Cao X., Wang X.","57206924121;57641127900;57195419816;57195416554;35168097300;57195414651;56566940500;55985606100;57216762397;56011424100;","A Diagnostic Analysis Workflow to Optimal Multiple Tumor Markers to Predict the Nonmetastatic Breast Cancer from Breast Lumps",2021,"Journal of Oncology","2021",,"5579373","","",,,"10.1155/2021/5579373","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111503163&doi=10.1155%2f2021%2f5579373&partnerID=40&md5=a705f9c4a39fd009e9c43dac589d7288","Objective. To assess the diagnostic performance of clinically common single markers and combinations to distinguish nonmetastatic breast cancer and benign breast tumor. A predictive model with a better diagnostic ability for nonmetastatic breast cancer was established by using the diagnostic process. Methods. A total of 222 patients with nonmetastatic breast cancer and 265 patients with benign breast disease were enrolled in this study. CEA, Ca 15-3, Ca 125, Ca 72-4, CYFRA 21-1, FERR, AFP, and NSE were measured by an electrochemiluminescent immunoenzymometric assay on the Elecsys system. There are four key steps for our diagnostic workflow, that is, feature selection, algorithm selection, parameter optimization, and outer test data was used to validate the optimal algorithm and markers. Results. CEA, Ca 15-3, CYFRA 21-1, AFP, and FERR were selected using the t-test in our inner development set. The optimal algorithm among logical regression, decision tree, support vector machine, random forest, and gradient boost machine was selected by 10-fold cross-validation, and we found that random forest and logistic regression are the better classification. The outer test data was used to validate the best markers and classification. The random forest with CEA, Ca 15-3, CYFRA 21-1, AFP, and FERR showed the optimal combination for distinguishing breast cancer and benign breast disease. The AUC value was 0.888, the cut-off point was 0.484, and sensitivity and specificity were 78.9% and 90.1%. Conclusions. No single marker of these eight markers was good at identifying nonmetastatic breast cancer from benign tumors. But a diagnostic analysis workflow was established to develop a predictive model with better diagnostic capability for nonmetastatic breast cancer. This workflow is also applicable to the optimization of other disease markers and diagnostic models. The predictive model showed good diagnostic performance, and it could be gradually incorporated as a support method for the diagnosis of nonmetastatic breast cancer. © 2021 Nan Jiang et al.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85111503163
"Hanselle J., Tornede A., Wever M., Hüllermeier E.","57219057120;57216673585;57195224973;57194516577;","Algorithm Selection as Superset Learning: Constructing Algorithm Selectors from Imprecise Performance Data",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12712 LNAI",,,"152","163",,,"10.1007/978-3-030-75762-5_13","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85111141266&doi=10.1007%2f978-3-030-75762-5_13&partnerID=40&md5=c25f1b784e530daa59d829b263f514eb","Algorithm selection refers to the task of automatically selecting the most suitable algorithm for solving an instance of a computational problem from a set of candidate algorithms. Here, suitability is typically measured in terms of the algorithms’ runtimes. To allow the selection of algorithms on new problem instances, machine learning models are trained on previously observed performance data and then used to predict the algorithms’ performances. Due to the computational effort, the execution of such algorithms is often prematurely terminated, which leads to right-censored observations representing a lower bound on the actual runtime. While simply neglecting these censored samples leads to overly optimistic models, imputing them with precise though hypothetical values, such as the commonly used penalized average runtime, is a rather arbitrary and biased approach. In this paper, we propose a simple regression method based on so-called superset learning, in which right-censored runtime data are explicitly incorporated in terms of interval-valued observations, offering an intuitive and efficient approach to handling censored data. Benchmarking on publicly available algorithm performance data, we demonstrate that it outperforms the aforementioned naïve ways of dealing with censored samples and is competitive to established methods for censored regression in the field of algorithm selection. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85111141266
"Hu Y., Liu X., Li S., Yu Y.","57191364153;57365613900;57226179084;57210071874;","Cascaded Algorithm Selection with Extreme-Region UCB Bandit",2021,"IEEE Transactions on Pattern Analysis and Machine Intelligence",,,,"","",,,"10.1109/TPAMI.2021.3094844","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110865557&doi=10.1109%2fTPAMI.2021.3094844&partnerID=40&md5=29644ec74ba51f6683c075ed92a30a30","AutoML aims at best configuring learning systems automatically. It contains core subtasks of algorithm selection and hyper-parameter tuning. Previous approaches considered searching in the joint hyper-parameter space of all algorithms, which forms a huge but redundant space and causes an inefficient search. We tackle this issue in a \emph{cascaded algorithm selection} way, which contains an upper-level process of algorithm selection and a lower-level process of hyper-parameter tuning for algorithms. While the lower-level process employs an \emph{anytime} tuning approach, the upper-level process is naturally formulated as a multi-armed bandit, deciding which algorithm should be allocated one more piece of time for the lower-level tuning. To achieve the goal of finding the best configuration, we propose the \emph{Extreme-Region Upper Confidence Bound} (ER-UCB) strategy. Unlike UCB bandits that maximize the mean of feedback distribution, ER-UCB maximizes the extreme-region of feedback distribution. We firstly consider stationary distributions and propose the ER-UCB-S algorithm that has <formula><tex>$O(K\ln n)$</tex></formula> regret upper bound with K arms and n trials. We then extend to non-stationary settings and propose the ER-UCB-N algorithm that has <formula><tex>$O(Kn^\nu)$</tex></formula> regret upper bound, where <formula><tex>$\frac{2}{3}&lt;\nu&lt;1$</tex></formula>. Finally, empirical studies on synthetic and AutoML tasks verify the effectiveness of ER-UCB-S/N by their outperformance in corresponding settings. IEEE",Article,"Article in Press","",Scopus,2-s2.0-85110865557
"Oliveira H., Silva C., Machado G.L.S., Nogueira K., dos Santos J.A.","57207107497;57215422210;57211712394;56385805000;25654873000;","Fully convolutional open set segmentation",2021,"Machine Learning",,,,"","",,2,"10.1007/s10994-021-06027-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85110034395&doi=10.1007%2fs10994-021-06027-1&partnerID=40&md5=e87f2ddacb94ad0ae407bd76d46facd2","Abstract: In traditional semantic segmentation, knowing about all existing classes is essential to yield effective results with the majority of existing approaches. However, these methods trained in a Closed Set of classes fail when new classes are found in the test phase, not being able to recognize that an unseen class has been fed. This means that they are not suitable for Open Set scenarios, which are very common in real-world computer vision and remote sensing applications. In this paper, we discuss the limitations of Closed Set segmentation and propose two fully convolutional approaches to effectively address Open Set semantic segmentation: OpenFCN and OpenPCS. OpenFCN is based on the well-known OpenMax algorithm, configuring a new application of this approach in segmentation settings. OpenPCS is a fully novel approach based on feature-space from DNN activations that serve as features for computing PCA and multi-variate gaussian likelihood in a lower dimensional space. In addition to OpenPCS and aiming to reduce the RAM memory requirements of the methodology, we also propose a slight variation of the method (OpenIPCS) that uses an iteractive version of PCA able to be trained in small batches. Experiments were conducted on the well-known ISPRS Vaihingen/Potsdam and the 2018 IEEE GRSS Data Fusion Challenge datasets. OpenFCN showed little-to-no improvement when compared to the simpler and much more time efficient SoftMax thresholding, while being some orders of magnitude slower. OpenPCS achieved promising results in almost all experiments by overcoming both OpenFCN and SoftMax thresholding. OpenPCS is also a reasonable compromise between the runtime performances of the extremely fast SoftMax thresholding and the extremely slow OpenFCN, being able to run close to real-time. Experiments also indicate that OpenPCS is effective, robust and suitable for Open Set segmentation, being able to improve the recognition of unknown class pixels without reducing the accuracy on the known class pixels. We also tested the scenario of hiding multiple known classes to simulate multimodal unknowns, resulting in an even larger gap between OpenPCS/OpenIPCS and both SoftMax thresholding and OpenFCN, implying that gaussian modeling is more robust to settings with greater openness. Graphic Abstract: [Figure not available: see fulltext.] © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Article,"Article in Press","All Open Access, Bronze, Green",Scopus,2-s2.0-85110034395
"Chehbi Gamoura S., Koruca H.İ., Gülmez E., Kocaer E.R., Khelil I.","55854460000;6507903618;57225930654;57225940607;57225947215;","The Evidence of the “No Free Lunch” Theorems and the Theory of Complexity in Business Artificial Intelligence",2021,"Lecture Notes on Data Engineering and Communications Technologies","76",,,"325","343",,,"10.1007/978-3-030-79357-9_32","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109941676&doi=10.1007%2f978-3-030-79357-9_32&partnerID=40&md5=548ca8eeb4f346a86ef56ae9467e088e","For more than decades, the central drivers of business growth have been technological tools. The most important of these is Artificial Intelligence (AI). We name the paradigm “Business Artificial Intelligence (BAI)” the category of AI approaches that can create profitable new business models, earnest business-values, and more competitiveness, particularly Machine Learning (ML). However, ML algorithms are plentiful, and most of them necessitate specific Data structures, particular applicability features, and in-depth analysis of the business context. Because of these considerations, an increasing number of industrial ML-based applications fail. Therefore, it is natural to ask the question of applicability limits and ML algorithms selection, giving the business context. Accordingly, in this paper, we propose the evidence of the “No-Free-Lunch” (NFL) theorems to understand ML use’s applicability in business organizations. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Book Chapter,"Final","",Scopus,2-s2.0-85109941676
"Eimer T., Biedenkapp A., Reimer M., Adriaensen S., Hutter F., Lindauer M.","57219224145;57195955636;57224472597;56315492700;55931808800;56032621900;","DACBench: A Benchmark Library for Dynamic Algorithm Configuration",2021,"IJCAI International Joint Conference on Artificial Intelligence",,,,"1668","1674",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108817360&partnerID=40&md5=f41692baf002ae6147a80014d7cc6655","Dynamic Algorithm Configuration (DAC) aims to dynamically control a target algorithm's hyperparameters in order to improve its performance. Several theoretical and empirical results have demonstrated the benefits of dynamically controlling hyperparameters in domains like evolutionary computation, AI Planning or deep learning. Replicating these results, as well as studying new methods for DAC, however, is difficult since existing benchmarks are often specialized and incompatible with the same interfaces. To facilitate benchmarking and thus research on DAC, we propose DACBench, a benchmark library that seeks to collect and standardize existing DAC benchmarks from different AI domains, as well as provide a template for new ones. For the design of DACBench, we focused on important desiderata, such as (i) flexibility, (ii) reproducibility, (iii) extensibility and (iv) automatic documentation and visualization. To show the potential, broad applicability and challenges of DAC, we explore how a set of six initial benchmarks compare in several dimensions of difficulty. © 2021 International Joint Conferences on Artificial Intelligence. All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85108817360
"Renau Q., Dreo J., Doerr C., Doerr B.","57215307853;55882904600;55827803400;56279123400;","Towards Explainable Exploratory Landscape Analysis: Extreme Feature Selection for Classifying BBOB Functions",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12694 LNCS",,,"17","33",,3,"10.1007/978-3-030-72699-7_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107507064&doi=10.1007%2f978-3-030-72699-7_2&partnerID=40&md5=5d6932a8420ea7274a1c383b422f5fca","Facilitated by the recent advances of Machine Learning (ML), the automated design of optimization heuristics is currently shaking up evolutionary computation (EC). Where the design of hand-picked guidelines for choosing a most suitable heuristic has long dominated research activities in the field, automatically trained heuristics are now seen to outperform human-derived choices even for well-researched optimization tasks. ML-based EC is therefore not any more a futuristic vision, but has become an integral part of our community. A key criticism that ML-based heuristics are often faced with is their potential lack of explainability, which may hinder future developments. This applies in particular to supervised learning techniques which extrapolate algorithms’ performance based on exploratory landscape analysis (ELA). In such applications, it is not uncommon to use dozens of problem features to build the models underlying the specific algorithm selection or configuration task. Our goal in this work is to analyze whether this many features are indeed needed. Using the classification of the BBOB test functions as testbed, we show that a surprisingly small number of features – often less than four – can suffice to achieve a 98% accuracy. Interestingly, the number of features required to meet this threshold is found to decrease with the problem dimension. We show that the classification accuracy transfers to settings in which several instances are involved in training and testing. In the leave-one-instance-out setting, however, classification accuracy drops significantly, and the transformation-invariance of the features becomes a decisive success factor. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85107507064
"Cosson R., Derbel B., Liefooghe A., Aguirre H., Tanaka K., Zhang Q.","57224305365;57203133300;23091255000;6603898641;55430510400;7406718367;","Decomposition-Based Multi-objective Landscape Features and Automated Algorithm Selection",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12692 LNCS",,,"34","50",,2,"10.1007/978-3-030-72904-2_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107355853&doi=10.1007%2f978-3-030-72904-2_3&partnerID=40&md5=ab153209b4d45f18a04c761e5e63069e","Landscape analysis is of fundamental interest for improving our understanding on the behavior of evolutionary search, and for developing general-purpose automated solvers based on techniques from statistics and machine learning. In this paper, we push a step towards the development of a landscape-aware approach by proposing a set of landscape features for multi-objective combinatorial optimization, by decomposing the original multi-objective problem into a set of single-objective sub-problems. Based on a comprehensive set of bi-objective and three variants of the state-of-the-art Moea/d algorithm, we study the association between the proposed features, the global properties of the considered landscapes, and algorithm performance. We also show that decomposition-based features can be integrated into an automated approach for predicting algorithm performance and selecting the most accurate one on blind instances. In particular, our study reveals that such a landscape-aware approach is substantially better than the single best solver computed over the three considered Moea/d variants. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85107355853
"Pavelski L.M., Kessaci M.-É., Delgado M.","55575774600;57195234359;7202169816;","Flowshop NEH-Based Heuristic Recommendation",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12692 LNCS",,,"136","151",,,"10.1007/978-3-030-72904-2_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107347003&doi=10.1007%2f978-3-030-72904-2_9&partnerID=40&md5=f9c7de0a8ac9662ec91ac45477a6957c","Flowshop problems (FSPs) have many variants and a broad set of heuristics proposed to solve them. Choosing the best heuristic and its parameters for a given FSP instance can be very challenging for practitioners. Per-instance Algorithm Configuration (PIAC) approaches aim at recommending the best algorithm configuration for a particular instance problem. This paper presents a PIAC methodology for building models to automatically configure the Nawaz, Encore, and Ham (NEH) algorithm which proved to be a good choice in most FSP variants (especially when they are used to provide initial solutions). We use irace to build the performance dataset (problem features ↔ algorithm configuration), while training Decision Tree and Random Forest models to recommend NEH configurations on unseen problems of the test set. Results show that the recommended heuristics have good performance, especially those by random forest models considering parameter dependencies. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85107347003
"Xu X., Li J., Zhou M., Yu X.","57202783917;55900097200;7403506743;57200266227;","Precedence-Constrained Colored Traveling Salesman Problem: An Augmented Variable Neighborhood Search Approach",2021,"IEEE Transactions on Cybernetics",,,,"","",,6,"10.1109/TCYB.2021.3070143","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107223116&doi=10.1109%2fTCYB.2021.3070143&partnerID=40&md5=8b8e550534c385aa19b729e0ed2839aa","A traveling salesman problem (CTSP) as a generalization of the well-known multiple traveling salesman problem utilizes colors to distinguish the accessibility of individual cities to salesmen. This work formulates a precedence-constrained CTSP (PCTSP) over hypergraphs with asymmetric city distances. It is capable of modeling the problems with operations or activities constrained to precedence relationships in many applications. Two types of precedence constraints are taken into account, i.e., 1) among individual cities and 2) among city clusters. An augmented variable neighborhood search (VNS) called POPMUSIC-based VNS (PVNS) is proposed as a main framework for solving PCTSP. It harnesses a partial optimization metaheuristic under special intensification conditions to prepare candidate sets. Moreover, a topological sort-based greedy algorithm is developed to obtain a feasible solution at the initialization phase. Next, mutation and multi-insertion of constraint-preserving exchanges are combined to produce different neighborhoods of the current solution. Two kinds of constraint-preserving k-exchange are adopted to serve as a strong local search means. Extensive experiments are conducted on 34 cases. For the sake of comparison, Lin-Kernighan heuristic, two genetic algorithms and three VNS methods are adapted to PCTSP and fine-tuned by using an automatic algorithm configurator-irace package. The experimental results show that PVNS outperforms them in terms of both search ability and convergence rate. In addition, the study of four PVNS variants each lacking an important operator reveals that all operators play significant roles in PVNS. IEEE",Article,"Article in Press","",Scopus,2-s2.0-85107223116
"Balcan M.-F., Sandholm T., Vitercik E.","8954993900;57203083791;56829527400;","Generalization in Portfolio-Based Algorithm Selection",2021,"35th AAAI Conference on Artificial Intelligence, AAAI 2021","14A",,,"12225","12232",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106983589&partnerID=40&md5=25fbf3e30dee7d4ce31cc50a4d7fb782","Portfolio-based algorithm selection has seen tremendous practical success over the past two decades. This algorithm configuration procedure works by first selecting a portfolio of diverse algorithm parameter settings, and then, on a given problem instance, using an algorithm selector to choose a parameter setting from the portfolio with strong predicted performance. Oftentimes, both the portfolio and the algorithm selector are chosen using a training set of typical problem instances from the application domain at hand. In this paper, we provide the first provable guarantees for portfolio-based algorithm selection. We analyze how large the training set should be to ensure that the resulting algorithm selector's average performance over the training set is close to its future (expected) performance. This involves analyzing three key reasons why these two quantities may diverge: 1) the learningtheoretic complexity of the algorithm selector, 2) the size of the portfolio, and 3) the learning-theoretic complexity of the algorithm's performance as a function of its parameters. We introduce an end-to-end learning-theoretic analysis of the portfolio construction and algorithm selection together. We prove that if the portfolio is large, overfitting is inevitable, even with an extremely simple algorithm selector. With experiments, we illustrate a tradeoff exposed by our theoretical analysis: as we increase the portfolio size, we can hope to include a well-suited parameter setting for every possible problem instance, but it becomes impossible to avoid overfitting. © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",Conference Paper,"Final","",Scopus,2-s2.0-85106983589
"Zöller M.-A., Nguyen T.-D., Huber M.F.","57200419565;57217776034;22835220700;","Incremental Search Space Construction for Machine Learning Pipeline Synthesis",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12695 LNCS",,,"103","115",,,"10.1007/978-3-030-74251-5_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105866484&doi=10.1007%2f978-3-030-74251-5_9&partnerID=40&md5=9a18a480868816b43a7183117a2a062b","Automated machine learning (AutoML) aims for constructing machine learning (ML) pipelines automatically. Many studies have investigated efficient methods for algorithm selection and hyperparameter optimization. However, methods for ML pipeline synthesis and optimization considering the impact of complex pipeline structures containing multiple preprocessing and classification algorithms have not been studied thoroughly. In this paper, we propose a data-centric approach based on meta-features for pipeline construction and hyperparameter optimization inspired by human behavior. By expanding the pipeline search space incrementally in combination with meta-features of intermediate data sets, we are able to prune the pipeline structure search space efficiently. Consequently, flexible and data set specific ML pipelines can be constructed. We prove the effectiveness and competitiveness of our approach on 28 data sets used in well-established AutoML benchmarks in comparison with state-of-the-art AutoML frameworks. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85105866484
"Zhang H., Buchanan G., McKay D.","57222171560;35182392700;17435193400;","Hey Alexa, What Should I Read? Comparing the Use of Social and Algorithmic Recommendations for Different Reading Genres",2021,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","12645 LNCS",,,"346","363",,1,"10.1007/978-3-030-71292-1_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104823953&doi=10.1007%2f978-3-030-71292-1_27&partnerID=40&md5=82e4fdca91d7bdb6d648b223a37be8c8","Users often seek reading recommendations for what to read, across a variety of topics of interest and genres. While there has been extensive research on the development of recommender algorithms, our understanding of social factors relating to reading recommendation in the digital era is poor. We have no holistic view of how readers interact with diverse resources, social and digital, to obtain reading recommendations. Users can consult computer-generated summaries and human-created reviews. How much or how often the typical user relies on one or other source, or what variations there are by genre of intended reading, are both open questions. To narrow these research gaps, we conducted a diary study to capture a comprehensive picture of readers’ use of algorithm- and social-sourced information to inform their future reading choices. Based on a qualitative analysis of these diaries, we produced a survey to investigate in-depth readers’ recommendation preferences across fictional reading, factual reading, academic resources, and news and articles. We show that users rely on different sources of recommendation information in different ways across different genres, and that modern social media plays an increasing role alongside established mass media, especially for fiction. © 2021, Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85104823953
"Czako Z., Sebestyen G., Hangan A.","57203068304;24178273200;24465133000;","Artificial Intelligence Algorithms Selection and Tuning for Anomaly Detection",2021,"Studies in Computational Intelligence","893",,,"73","88",,,"10.1007/978-3-030-64731-5_4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104774301&doi=10.1007%2f978-3-030-64731-5_4&partnerID=40&md5=78f1477c594956bbd5b367cd4d7ef78e","Artificial intelligence (AI) algorithms have recently become a topic of interest, among researchers and commercial product developers. A large number of AI algorithms aiming different kind of real-life problems have qualitatively different results, based on the nature of data, the nature of the problems and based on the context in which they are used. Selecting the most appropriate algorithm to solve a particular problem is not an easy task. In our research, we focus on developing methods and instruments for the selection and tuning of AI algorithms, to solve the problem of anomaly detection in datasets. The goal of our research is to create a platform, which can be used for data analysis. With this platform, the user could be able to quickly train, test and evaluate several artificial intelligence algorithms and also they will be able to find out which is the algorithm that performs best for a specific problem. Moreover, this platform will help developers to tune the parameters of the chosen algorithm in order to get better results on their problem, in a shorter amount of time. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85104774301
"Cheng J., Wang H.","57223030758;57223023918;","Adaptive algorithm recommendation and application of learning resources in english fragmented readinga",2021,"Complexity","2021",,"5592534","","",,3,"10.1155/2021/5592534","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104508990&doi=10.1155%2f2021%2f5592534&partnerID=40&md5=4548539a09f7fa8706eb9f712347f87d","This paper firstly designs a five-dimensional model of learners' characteristics (learners' English reading ability, cognitive style, learning goal, learning situation, and learning effect) and a three-dimensional model of English reading resources' characteristics (question types, topics, and difficulty of resources) in a fragmented learning environment through literature research. At the same time, to make the learning resources meet the characteristics of fragmented learning time and space, the English Level 4 reading resources are reasonably designed and segmented to adapt to the needs of learners' mobile fragmented learning. Then, combined with machine learning algorithms, an adaptive recommendation model of learning resources in English fragmented reading is constructed. The algorithm-based adaptive recommendation algorithm for English fragmented reading resources is designed. Based on the generated decision trees, the expression rules are parsed to achieve adaptive pushing of resources. The results of this study show that adaptive recommendation of learning resources in English fragmented reading can help teachers to develop future resource recommendation strategies through effective data collection to adaptively push resources that are close to learners' individual needs. The use of mobile by English learners to learn to read in a fragmented learning context enables targeted training in weak areas of English reading, thus enhancing different aspects of learners' reading skills. Copyright © 2021 Jinyu Cheng and Hong Wang.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85104508990
"Parisot O., Tamisier T.","35105734000;18042694400;","Automated machine learning for wind farms location",2021,"ICPRAM 2021 - Proceedings of the 10th International Conference on Pattern Recognition Applications and Methods",,,,"222","227",,,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103832477&partnerID=40&md5=2dbabb8cf31aed8459dff2c75bc4fd5e","Automated Machine Learning aims at preparing effective Machine Learning models with little or no data science expertise. Tedious tasks like preprocessing, algorithm selection and hyper-parameters optimization are then automatized: end-users just have to apply and deploy the model that best suits the real world problem. In this paper, we experiment Automated Machine Learning to leverage open data sources for predicting potential next wind farms location in Luxembourg, France, Belgium and Germany. © 2021 by SCITEPRESS - Science and Technology Publications, Lda. All rights reserved",Conference Paper,"Final","",Scopus,2-s2.0-85103832477
"Kazikova A., Pluhacek M., Senkerik R.","57202322367;55445026800;23975048900;","How Does the Number of Objective Function Evaluations Impact Our Understanding of Metaheuristics Behavior?",2021,"IEEE Access","9",,"9378523","44032","44048",,4,"10.1109/ACCESS.2021.3066135","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103755169&doi=10.1109%2fACCESS.2021.3066135&partnerID=40&md5=74264b3dc6e4ffa7586c869a56a58df0","Comparing various metaheuristics based on an equal number of objective function evaluations has become standard practice. Many contemporary publications use a specific number of objective function evaluations by the benchmarking sets definitions. Furthermore, many publications deal with the recurrent theme of late stagnation, which may lead to the impression that continuing the optimization process could be a waste of computational capabilities. But is it? Recently, many challenges, issues, and questions have been raised regarding fair comparisons and recommendations towards good practices for benchmarking metaheuristic algorithms. The aim of this work is not to compare the performance of several well-known algorithms but to investigate the issues that can appear in benchmarking and comparisons of metaheuristics performance (no matter what the problem is). This article studies the impact of a higher evaluation number on a selection of metaheuristic algorithms. We examine the effect of a raised evaluation budget on overall performance, mean convergence, and population diversity of selected swarm algorithms and IEEE CEC competition winners. Even though the final impact varies based on current algorithm selection, it may significantly affect the final verdict of metaheuristics comparison. This work has picked an important benchmarking issue and made extensive analysis, resulting in conclusions and possible recommendations for users working with real engineering optimization problems or researching the metaheuristics algorithms. Especially nowadays, when metaheuristic algorithms are used for increasingly complex optimization problems, and meet machine learning in AutoML frameworks, we conclude that the objective function evaluation budget should be considered another vital optimization input variable. © 2013 IEEE.",Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85103755169
"Stitini O., Kaloun S., Bencharef O.","57217042458;8893478400;55638821100;","The Recommendation of a Practical Guide for Doctoral Students Using Recommendation System Algorithms in the Education Field",2021,"Lecture Notes in Networks and Systems","183",,,"240","254",,3,"10.1007/978-3-030-66840-2_19","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102611370&doi=10.1007%2f978-3-030-66840-2_19&partnerID=40&md5=346cf3434e7d59c7a78e4f4744acf9cf","Recommendation systems provide an approach to facilitate the user’s desire. It is helpful in recommending things from various domains like e-commerce, the service industry, and social networking sites. The most researcher we found is based in the trading domain. However, there is limited information on the impact of recommender systems in other domains like education. Recently recommendation systems have proved to be efficient for the education sector as well. The online recommendation system has become a trend. Nowadays rather than going out and buying items for themselves, online recommendation provides an easier and quicker way to buy items, and transactions are also quick when it is done online. Recommended systems are powerful new technology and it helps users to find items that they want to buy. A recommendation system is broadly used to recommend the most appropriate products to end users. Thus, the objective of this study is to summarize the current knowledge that is available with regard to recommendation systems that have been employed within the education domain to support educational practices. Our results provide some findings regarding how recommendation systems can be used to support main areas in education, what approaches techniques or algorithms recommender systems use, and how they address different issues in the academic world. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Conference Paper,"Final","",Scopus,2-s2.0-85102611370
"Liu W., Zhao C.","57209101356;57199830125;","ETM: Effective Tuning Method Based on Multi-Objective and Knowledge Transfer in Image Recognition",2021,"IEEE Access","9",,"9363932","47216","47229",,,"10.1109/ACCESS.2021.3062366","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101850774&doi=10.1109%2fACCESS.2021.3062366&partnerID=40&md5=1ccff8a5815bedfba2c07dc24f1f18ad","With the widespread application of machine learning and deep learning, image recognition has been continuously developed. However, there are still huge challenges in the use of machine learning and deep learning. The tuning processes of algorithms are critical and challenging for their performance. Although there have been many previous works to improve the final accuracy of the recognition algorithms through tuning, these works cannot consider some indicators that are also very important in the actual environment (such as latency, central processing unit (cpu) utilization) in the tuning. In this paper, we propose an effective tuning method based on multi-objective and knowledge transfer, which is solved the above limitations in the image recognition. Specifically, we first use an agent to automatically tune the recognition algorithms, and combine the prediction accuracy and the running latency of each episode as a multi-objective reward signal to guide the update of the internal parameters of the agent. In this way, the agent can continuously select the better algorithm configuration to improve prediction performance. In addition, we improve the efficiency of the above tuning process by transferring knowledge. To do that, we can learn the meta parameters from other small-scale tasks to initialize the agent. In the experiments, we apply the proposed method to tune the eXtreme Gradient Boosting and random forest on 57 image recognition tasks and convolutional neural network on 2 tasks. The experimental results verify that the proposed method achieves average accuracy rankings of 1.92, 1.42 and 1.71 on three algorithms to be optimized, respectively. Especially in terms of latency performance, the proposed method performs best on all the tasks (57 data sets) on the three algorithms to be optimized. In addition, we verify the various components of the proposed method through ablation experiments. © 2021 IEEE.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85101850774
"Lu Z., Nam I.","57221996754;57222000299;","Research on the Influence of New Media Technology on Internet Short Video Content Production under Artificial Intelligence Background",2021,"Complexity","2021",,"8875700","","",,1,"10.1155/2021/8875700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100909272&doi=10.1155%2f2021%2f8875700&partnerID=40&md5=bf0f01d1262d0609a2d122a7c514ad45","With the rapid development of the Internet and smart phone technology, a large number of short videos are shared through social platforms. Therefore, video content analysis is a very important and popular work in machine learning and artificial intelligence currently. However, it is very difficult to analyze all aspects of video content originally produced by large-scale users. How to screen out bad and illegal content from short videos published by a large number of users, select high-quality videos to share with other users, and improve the quality of video on the distribution platform of the entire user is a top priority. Based on this background, this paper focuses on optimizing video auditing to provide basic features for algorithm judgment, supporting original content and increasing the distribution of new content, and strengthening manual intervention combining algorithm recommendation with manual recommendation. Four major aspects of the artificial training algorithm model discuss the optimization effect of artificial intelligence on the algorithm in order to provide some guidance for the sustainable and healthy development of mobile short video. © 2021 Zhiqin Lu and Inyong Nam.",Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85100909272
"Choudhury A.","57209162815;","Predicting cancer using supervised machine learning: Mesothelioma",2021,"Technology and Health Care","29","1",,"45","58",,6,"10.3233/THC-202237","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100314358&doi=10.3233%2fTHC-202237&partnerID=40&md5=b0d63e18f8670d1de3f81b4a9beed134","Pleural Mesothelioma (PM) is an unusual, belligerent tumor that rapidly develops into cancer in the pleura of the lungs. Pleural Mesothelioma is a common type of Mesothelioma that accounts for about 75% of all Mesothelioma diagnosed yearly in the U.S. Diagnosis of Mesothelioma takes several months and is expensive. Given the risk and constraints associated with PM diagnosis, early identification of this ailment is essential for patient health. OBJECTIVE: In this study, we use artificial intelligence algorithms recommending the best fit model for early diagnosis and prognosis of Malignant Pleural Mesothelioma (MPM). METHODS: We retrospectively retrieved patients' clinical data collected by Dicle University, Turkey and applied multilayered perceptron (MLP), voted perceptron (VP), Clojure classifier (CC), kernel logistic regression (KLR), stochastic gradient decent (SGD), adaptive boosting (AdaBoost), Hoeffding tree (VFDT), and primal estimated sub-gradient solver for support vector machine (s-Pegasos). We evaluated the models, compared and tested them using paired t-test (corrected) at 0.05 significance based on their respective classification accuracy, f-measure, precision, recall, root mean squared error, receivers' characteristic curve (ROC), and precision-recall curve (PRC). RESULTS: In phase 1, SGD, AdaBoost.M1, KLR, MLP, VFDT generate optimal results with the highest possible performance measures. In phase 2, AdaBoost, with a classification accuracy of 71.29%, outperformed all other algorithms. C-reactive protein, platelet count, duration of symptoms, gender, and pleural protein were found to be the most relevant predictors that can prognosticate Mesothelioma. CONCLUSION: This study confirms that data obtained from biopsy and imaging tests are strong predictors of Mesothelioma but are associated with a high cost; however, they can identify Mesothelioma with optimal accuracy. © 2021 - IOS Press. All rights reserved.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85100314358
"Muñoz M.A., Kirley M.","55499624900;6602345033;","Sampling effects on algorithm selection for continuous black-box optimization",2021,"Algorithms","14","1","19","","",,1,"10.3390/a14010019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099785768&doi=10.3390%2fa14010019&partnerID=40&md5=b08258d794680dad9485730123138e71","In this paper, we investigate how systemic errors due to random sampling impact on automated algorithm selection for bound-constrained, single-objective, continuous black-box optimization. We construct a machine learning-based algorithm selector, which uses exploratory landscape analysis features as inputs. We test the accuracy of the recommendations experimentally using resampling techniques and the hold-one-instance-out and hold-one-problem-out validation methods. The results demonstrate that the selector remains accurate even with sampling noise, although not without trade-offs. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Article,"Final","All Open Access, Green",Scopus,2-s2.0-85099785768
"Marques J.A.L., Gois F.N.B., Xavier-Neto J., Fong S.J.","57196897667;57220176589;6602992492;7102256353;","Artificial Intelligence Prediction for the COVID-19 Data Based on LSTM Neural Networks and H2O AutoML",2021,"SpringerBriefs in Applied Sciences and Technology",,,,"69","87",,,"10.1007/978-3-030-61913-8_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097175872&doi=10.1007%2f978-3-030-61913-8_5&partnerID=40&md5=f634b9cbb490b7279db005c9a592971f","The use of computational intelligence techniques is being considered for a vast number of applications not only because of its increasing popularity but also because the results achieve good performance and are promising to keep improving. In this chapter, we present the basic theoretical aspects and assumptions of the LSTM model and H20 AutoML framework. It is evaluated on the prediction of the COVID-19 epidemiological data series for five different countries (China, United States, Brazil, Italy, and Singapore), each of them with specific curves, which are results of policies and decisions during the pandemic spread. The discussion about the results is performed with the focus on three evaluation criteria: R2 Score, MAE, and MSE. Higher R2 Score was obtained when the sample time series was smoothly increasing or decreasing. The results obtained by the AutoML framework achieved a higher R2 Score and lower MAE and MSE when compared with LSTM and also with other techniques proposed in the book, such as ARIMA and Kalman predictor. The application of machine learning algorithm selector might be a promising candidate for a good predictor for epidemic time series. © 2021, The Author(s), under exclusive license to Springer Nature Switzerland AG.",Book Chapter,"Final","",Scopus,2-s2.0-85097175872
"Gopagoni D.R., Lakshmi P.V., Siripurapu P.","57215593665;37077395200;57219424668;","Predicting the Sales Conversion Rate of Car Insurance Promotional Calls",2021,"Advances in Intelligent Systems and Computing","1187",,,"321","329",,1,"10.1007/978-981-15-6014-9_37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092638058&doi=10.1007%2f978-981-15-6014-9_37&partnerID=40&md5=8f7bcf4791db572fb54eaf6a1f6cee4e","Telemarketing is one of the important promotional activities which brought a revolution to business marketing. It allows us to reach target customers quickly and gauge customer’s interest in the product. The ultimate success of telemarketing is concomitant to the conversion of calls to sales. Understanding the patterns in the generated data will help to increase the success rate. A diverse set of machine learning algorithms have been explored to examine useful information from the telemarketing data like identify the important factors for better sales conversion rate. Multiple classification algorithms are applied to understand the relationship between the calls and customer data points to predict the success rate. Anticipating the business happened is YES or NO absolute endpoint. The current study additionally features the significance of selecting the right machine learning algorithm to unleash the important information in the given dataset. A combination of different classification algorithms resulted in improved model accuracy. © 2021, Springer Nature Singapore Pte Ltd.",Conference Paper,"Final","",Scopus,2-s2.0-85092638058
"Marzec M.","55902424000;","Monitoring temperature-related hazards using mobile devices and a thermal camera",2021,"Advances in Intelligent Systems and Computing","1186",,,"369","383",,,"10.1007/978-3-030-49666-1_29","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091000197&doi=10.1007%2f978-3-030-49666-1_29&partnerID=40&md5=557b52a4a42a09352adca6210413db0a","The paper proposes an algorithm that allows for automatic detection of thermal hazards using a smartphone-type device and a mobile thermal camera. The algorithm works in two stages. First, it analyses the environment of a disabled person for areas with a high or very high temperature. If they are detected, it signals by voice messages the appearance of such an object in the camera field of view. The second stage is the detection of a situation when a visually impaired person brings their hand closer to a hot object. In this situation, the application automatically detects the hand and the hot object and signals by voice messages a higher hazard level. In order to classify the level of thermal hazards, image thresholding methods were used, which enable to detect objects with temperatures above 40, 55, 70 $$^\circ $$C. For hand detection in a situation threatening burns, an approach based on sequential thermal image analysis and machine learning was used. Tests were carried out for various algorithm configurations. The accuracy and precision values were as follows: Acc $$=0.89$$, Prec $$=0.91$$. The proposed method allows for quick and completely automatic warning of hazards and can be used for popular Android mobile devices. © The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Switzerland AG 2021.",Book Chapter,"Final","",Scopus,2-s2.0-85091000197
"Balcıoğlu H.E., Seçkin A.Ç.","55751867400;57103461800;","Comparison of machine learning methods and finite element analysis on the fracture behavior of polymer composites",2021,"Archive of Applied Mechanics","91","1",,"223","239",,6,"10.1007/s00419-020-01765-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090459636&doi=10.1007%2fs00419-020-01765-5&partnerID=40&md5=5a0647e7a56cdb631f732592775cf42e","In recent years, it became possible to use different methods for the analysis of mechanical systems with the help of computers to learn like humans and by increasing their interaction with the world by observing autonomously. One of these mechanical analyzes is the fracture mechanics in which the behavior of the laminated composites having a crack is examined. In this study, experimental methods, finite element analysis (FEA) and machine learning algorithms (MLA) were used to analyze the fracture behavior of polymer composites in Mode I, Mode I/II and Mode II loading situations. For the experimental study, the fracture behaviors of the laminated composites reinforced with pure glass, pure carbon and glass/carbon hybrid knitted fabrics were tested with the help of Arcan test apparatus. In the finite element method, the linear elastic fracture behavior at the crack tip was analyzed by using the J-integral method. In the field of MLA, there is no single learning algorithm that provides good learning on all real-world problem data. Therefore, algorithm selection is done experimentally so various machine algorithms were used in the study. The analysis result showed that the finite element analysis and machine learning results were in good agreement with experimental measurements. This study is particularly important for the comparison of machine learning techniques with FEA in regression applications. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Article,"Final","",Scopus,2-s2.0-85090459636
"Bertsimas D., Orfanoudaki A., Wiberg H.","7005282772;57216889389;57193490175;","Interpretable clustering: an optimization approach",2021,"Machine Learning","110","1",,"89","138",,11,"10.1007/s10994-020-05896-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089447323&doi=10.1007%2fs10994-020-05896-2&partnerID=40&md5=5b83c1f36f419a7d26629732e47d4e69","State-of-the-art clustering algorithms provide little insight into the rationale for cluster membership, limiting their interpretability. In complex real-world applications, the latter poses a barrier to machine learning adoption when experts are asked to provide detailed explanations of their algorithms’ recommendations. We present a new unsupervised learning method that leverages Mixed Integer Optimization techniques to generate interpretable tree-based clustering models. Utilizing a flexible optimization-driven framework, our algorithm approximates the globally optimal solution leading to high quality partitions of the feature space. We propose a novel method which can optimize for various clustering internal validation metrics and naturally determines the optimal number of clusters. It successfully addresses the challenge of mixed numerical and categorical data and achieves comparable or superior performance to other clustering methods on both synthetic and real-world datasets while offering significantly higher interpretability. © 2020, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85089447323
"Szymkowski M., Milewski P., Saeed K.","57191262204;57218449610;35183998800;","Fingerprint and Keystroke Dynamics Fusion in Multimodal Biometrics System",2021,"Advances in Intelligent Systems and Computing","1178",,,"67","82",,1,"10.1007/978-981-15-5747-7_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089243588&doi=10.1007%2f978-981-15-5747-7_5&partnerID=40&md5=a1551f4fa1a067e12b39505925bb09d7","Recently biometrics traits are the most popular way by which we can secure our data and devices. In the case of them, we do not need any specific passwords or cards, simply measurable human traits become a key. However, it was proven that sometimes one trait is not enough to assure high accuracy level of biometrics system. In the proposed algorithm, we concentrate on two measurable traits—fingerprint and keystroke dynamics. The first of them is commonly used in diversified approaches. The second one is less used due to its low distinguishability. Methods to extract feature vector from fingerprint and keystroke dynamics along with the way to use it in multimodal biometrics system are presented in this paper. We divided experiments into three main parts: accuracy of fingerprint-based method, precision of keystroke dynamics-based algorithm, and at last the concept of multimodal system. During the experiments we have used diversified approaches, like k-nearest neighbors, k-means, Naive Bayes classifier, and decision trees. All experiments were realized on the sample sets collected by the authors. Performed analysis has shown that it is clearly possible to recognize human identity on the basis of keystroke dynamics with satisfactory accuracy level. Moreover, it showed that it is a huge impact of machine learning algorithm selection on the quality of classification and recognition. © 2021, Springer Nature Singapore Pte Ltd.",Conference Paper,"Final","",Scopus,2-s2.0-85089243588
"Garg H.S., Jain V., Chaudhary G.","57218281129;57209522515;57189576936;","The Curious Case of Modified Merge Sort",2021,"Advances in Intelligent Systems and Computing","1164",,,"481","487",,1,"10.1007/978-981-15-4992-2_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088590546&doi=10.1007%2f978-981-15-4992-2_45&partnerID=40&md5=cc3f92df4a5079b4b6fe0ba9de451a76","We propose an O(N logN) sorting method by Modified Merge Sort Method, which shows huge potential to sort big data sets, and even better potential to sort big data sets with a smaller range. Six sorting algorithms: Selection Sort, Insertion sort, Merge sort, Quick sort, Bubble Sort and Modified Merge Sort were implemented in C++ programming languages and tested for the random sequence input of length 100, 1000, 4000, 9000 on random numbers and the IPL Dataset. © 2021, The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd.",Conference Paper,"Final","",Scopus,2-s2.0-85088590546
